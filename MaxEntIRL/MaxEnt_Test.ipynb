{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch Version: 0.5.0a0+ab6afc2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "# Torch\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "print(\"Torch Version:\", torch.__version__)\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "# Planning\n",
    "sys.path.append(\"../Planning/\")\n",
    "import Planners as Planners\n",
    "\n",
    "# Control\n",
    "sys.path.append(\"../Acting/\")\n",
    "import TrajectorySamplers as TrajectorySamplers\n",
    "\n",
    "# Model\n",
    "sys.path.append(\"../Learning/\")\n",
    "import Models as Models\n",
    "\n",
    "# Simple RL\n",
    "from simple_rl.tasks import NavigationWorldMDP\n",
    "from simple_rl.tasks.navigation.NavigationWorldMDP import *\n",
    "from simple_rl.tasks.navigation.NavigationStateClass import NavigationWorldState\n",
    "from simple_rl.mdp.StateClass import State\n",
    "\n",
    "# IRL\n",
    "sys.path.append(\"../MLIRL\")\n",
    "from MLIRL import MLIRL\n",
    "\n",
    "# MaxEnt\n",
    "sys.path.append(\"../MaxEntIRL\")\n",
    "from MaxEntIRL import *\n",
    "\n",
    "# Eval\n",
    "sys.path.append(\"../utils/\")\n",
    "from Evaluation import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.style as style\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "# style.use('classic')\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib as mpl\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "mpl.rcParams['axes.titlesize'] = 18\n",
    "mpl.rcParams['axes.labelsize'] = 14\n",
    "mpl.rcParams['axes.grid'] = False\n",
    "mpl.rcParams['image.interpolation'] = 'nearest'\n",
    "mpl.rcParams['figure.figsize'] = 15, 55\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class IntState(State):\n",
    "    ''' Class for Grid World States '''\n",
    "\n",
    "    def __init__(self, x):\n",
    "        State.__init__(self, data=x)\n",
    "        self.x = round(x, 10)\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(self.data)\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"s: (\" + str(self.x) + \")\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return isinstance(other, IntState) and self.x == other.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TEST: I/P: trajectory that toggles beween two states. O/P: SVF should approximate uniform distribution.\n",
    "n_traj = 100\n",
    "traj_len = 10000\n",
    "S = [(0,0),(1,1)]\n",
    "trajectory_list = [[(S[np.random.randint(0, len(S))],0) for j in range(traj_len)] for i in range(n_traj)]\n",
    "assert np.linalg.norm(compute_svf(trajectory_list, S)-[0.5,0.5]) < 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TEST: I/P: trajectory that stays in one state. O/P: SVF should have all mass on that state.\n",
    "n_traj = 100\n",
    "traj_len = 10000\n",
    "S = [(0,0),(1,1)]\n",
    "trajectory_list = [[(S[0],0) for j in range(traj_len)] for i in range(n_traj)]\n",
    "assert np.linalg.norm(compute_svf(trajectory_list, S)-[1.,0]) < 1e-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TEST: I/P: trajectory that stays in another state. O/P: SVF should have all mass on that state.\n",
    "n_traj = 100\n",
    "traj_len = 10000\n",
    "S = [(0,0),(1,1)]\n",
    "trajectory_list = [[(S[1],0) for j in range(traj_len)] for i in range(n_traj)]\n",
    "assert np.linalg.norm(compute_svf(trajectory_list, S)-[0,1.]) < 1e-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TEST: I/P: trajectory that toggles between 3 states. O/P: SVF should approximate uniform distribution.\n",
    "n_traj = 100\n",
    "traj_len = 1000\n",
    "S = [(0,0),(1,1),(2,2)] #list(np.random.randint(0,2,(1000,2)))\n",
    "trajectory_list = [[(S[np.random.randint(0, len(S))],0) for j in range(traj_len)] for i in range(n_traj)]\n",
    "assert np.linalg.norm(compute_svf(trajectory_list, S)-[0.33,0.33,0.33]) < 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TEST: I/P: trajectory that toggles between 2 of the states. O/P: SVF should approximate uniform distribution \n",
    "# on those 2 states and 0 mass on unvisited state.\n",
    "n_traj = 100\n",
    "traj_len = 1000\n",
    "S = [(0,0),(1,1),(2,2)] #list(np.random.randint(0,2,(1000,2)))\n",
    "trajectory_list = [[(S[np.random.choice([0,2])],0) for j in range(traj_len)] for i in range(n_traj)]\n",
    "assert np.linalg.norm(compute_svf(trajectory_list, S)-[0.5,0,0.5]) < 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected SVF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TEST\n",
    "I/P: 2 state MDP with Policy that stays in state 2.\n",
    "O/P: Expected SVF with all mass on state 2.\n",
    "\"\"\"\n",
    "S = [IntState(s) for s in [1,2]] # S1, S2\n",
    "A = [\"stay\", \"move\"]\n",
    "\n",
    "T_dict = {S[0]: {\"stay\": [(S[0], 1.)], \"move\": [(S[1], 1.)]},\n",
    "             S[1]:{\"stay\": [(S[1], 1.)], \"move\": [(S[0], 1.)]}}\n",
    "\n",
    "def T(s,a): return T_dict[s][a]\n",
    "trajectory_list = [list(zip(np.random.choice([S[1]], 10), np.random.choice([\"stay\", \"move\"], 10))) \n",
    "                   for i in range(100)] # data doesn't matter for expected SVF, it's only used for determining N.\n",
    "# Policy: stay in S2\n",
    "Pi = np.array(\n",
    "      [[0,1], # Pi[S1]\n",
    "      [1,0]] # Pi[S2]\n",
    ")\n",
    "assert np.linalg.norm(compute_expected_svf(trajectory_list, S, A, Pi, T) - [0,1]) < 1e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TEST\n",
    "I/P: 2 state MDP with Policy that toggles between 2 states.\n",
    "O/P: Expected SVF should approxiamte uniform distribution.\n",
    "\"\"\"\n",
    "S = [IntState(s) for s in [1,2]] # S1, S2\n",
    "A = [\"stay\", \"move\"]\n",
    "\n",
    "T_dict = {S[0]: {\"stay\": [(S[0], 1.)], \"move\": [(S[1], 1.)]},\n",
    "             S[1]:{\"stay\": [(S[1], 1.)], \"move\": [(S[0], 1.)]}}\n",
    "def T(s,a): return T_dict[s][a]\n",
    "trajectory_list = [list(zip(np.random.choice([S[0],S[1]], 10), np.random.choice([\"stay\", \"move\"], 10)))\n",
    "                   for i in range(100)] # data doesn't matter for expected SVF, it's only used for determining N.\n",
    "# Policy: keep jumping between S1 and S2\n",
    "Pi = np.array(\n",
    "      [[0,1], # Pi[S1]\n",
    "      [0,1]] # Pi[S2]\n",
    ")\n",
    "assert np.linalg.norm(compute_expected_svf(trajectory_list, S, A, Pi, T) - [0.5,0.5]) < 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MaxEnt IRL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MDP\n",
    "S = [IntState(s) for s in [1,2,3,4]] # S1, S2\n",
    "A = [\"stay\", \"move\"]\n",
    "\n",
    "T_dict = {S[0]: \n",
    "              {\"stay\": [(S[0], 1.0)], \n",
    "               \"move\": [(S[1], 1.0)]},\n",
    "          S[1]:\n",
    "              {\"stay\": [(S[1], 1.0)], \n",
    "               \"move\": [(S[3], 1.0)]},\n",
    "          S[2]:\n",
    "              {\"stay\": [(S[2], 1.0)], \n",
    "               \"move\": [(S[3], 1.0)]},\n",
    "          S[3]:\n",
    "              {\"stay\": [(S[3], 1.0)],\n",
    "               \"move\": [(S[3], 1.0)]},\n",
    "         }\n",
    "def T(s,a): return T_dict[s][a]\n",
    "trajectory_list = [[(S[0], \"move\"), (S[1], \"move\"), (S[3], \"stay\")], \n",
    "                   [(S[0], \"move\"), (S[2], \"move\"), (S[3], \"stay\")]\n",
    "                  ]\n",
    "# Params\n",
    "gamma = 0.99\n",
    "\n",
    "# Features\n",
    "phi_dict = {S[0]: [0,0,0,1],\n",
    "            S[1]: [0,0,1,0],\n",
    "            S[2]: [0,1,0,0],\n",
    "            S[3]: [1,0,0,0]}\n",
    "phi = lambda s: torch.FloatTensor(phi_dict[s])\n",
    "phi_dim = len(phi(S[0]))\n",
    "\n",
    "# Reward Model\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.normal_(m.weight, mean=-1, std=0.01)\n",
    "R_model = Models.LinearRewardModel_ReLU(phi_dim, 0)\n",
    "R_model.apply(weights_init)\n",
    "\n",
    "# Optimizer\n",
    "lr, weight_decay = 0.1, 1e-2\n",
    "optimizer_fn=lambda params, lr, weight_decay: optim.Adam(params, lr=lr, weight_decay=weight_decay)\n",
    "R_optimizer = optimizer_fn(R_model.parameters(), lr, weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaxEntIRL params \n",
      "-----\n",
      "\t Domains: 2, sizes: [4, 4],\n",
      "\t Action dim: 2, \n",
      "\t Feature dim: 4,\n",
      "\t Iterations: 100, \n",
      "\t Max likelihood: 0.99,\n",
      "\t VI iterations: 100, \n",
      "\t VI convergence eps: 0.001,\n",
      "\t Gamma (discount factor): 0.99,\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass didn't converge by 100.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.93902075 0.06097929]\n",
      " [0.9660942  0.03390587]\n",
      " [0.9660942  0.03390587]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [6.146894 3.384167 3.384167 0.      ], \n",
      "\tQ: [[6.0839763 3.349673 ]\n",
      " [3.349673  0.       ]\n",
      " [3.349673  0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [9.4026023e-01 5.9050620e-02 0.0000000e+00 6.8918540e-04] \n",
      "\tDiff: tensor([-0.6069,  0.1076,  0.1667,  0.3326]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[0., 0., 0., 0.]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass didn't converge by 100.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.93902075 0.06097929]\n",
      " [0.9660942  0.03390587]\n",
      " [0.9660942  0.03390587]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [6.146894 3.384167 3.384167 0.      ], \n",
      "\tQ: [[6.0839763 3.349673 ]\n",
      " [3.349673  0.       ]\n",
      " [3.349673  0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [9.4026023e-01 5.9050620e-02 0.0000000e+00 6.8918540e-04] \n",
      "\tDiff: tensor([-0.6069,  0.1076,  0.1667,  0.3326]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[0., 0., 0., 0.]])\n",
      "\n",
      ">>> Iter: 0000 (0.028s): loss = 00.303463, likelihood = 0.0000\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass didn't converge by 100.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.93902075 0.06097929]\n",
      " [0.9660942  0.03390587]\n",
      " [0.9660942  0.03390587]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [6.146894 3.384167 3.384167 0.      ], \n",
      "\tQ: [[6.0839763 3.349673 ]\n",
      " [3.349673  0.       ]\n",
      " [3.349673  0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [9.4026023e-01 5.9050620e-02 0.0000000e+00 6.8918540e-04] \n",
      "\tDiff: tensor([-0.6069,  0.1076,  0.1667,  0.3326]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[0., 0., 0., 0.]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass didn't converge by 100.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.93902075 0.06097929]\n",
      " [0.9660942  0.03390587]\n",
      " [0.9660942  0.03390587]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [6.146894 3.384167 3.384167 0.      ], \n",
      "\tQ: [[6.0839763 3.349673 ]\n",
      " [3.349673  0.       ]\n",
      " [3.349673  0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [9.4026023e-01 5.9050620e-02 0.0000000e+00 6.8918540e-04] \n",
      "\tDiff: tensor([-0.6069,  0.1076,  0.1667,  0.3326]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[0., 0., 0., 0.]])\n",
      "\n",
      ">>> Iter: 0001 (0.028s): loss = 00.303463, likelihood = 0.0000\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass didn't converge by 100.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.93902075 0.06097929]\n",
      " [0.9660942  0.03390587]\n",
      " [0.9660942  0.03390587]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [6.146894 3.384167 3.384167 0.      ], \n",
      "\tQ: [[6.0839763 3.349673 ]\n",
      " [3.349673  0.       ]\n",
      " [3.349673  0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [9.4026023e-01 5.9050620e-02 0.0000000e+00 6.8918540e-04] \n",
      "\tDiff: tensor([-0.6069,  0.1076,  0.1667,  0.3326]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[0., 0., 0., 0.]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass didn't converge by 100.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.93902075 0.06097929]\n",
      " [0.9660942  0.03390587]\n",
      " [0.9660942  0.03390587]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [6.146894 3.384167 3.384167 0.      ], \n",
      "\tQ: [[6.0839763 3.349673 ]\n",
      " [3.349673  0.       ]\n",
      " [3.349673  0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [9.4026023e-01 5.9050620e-02 0.0000000e+00 6.8918540e-04] \n",
      "\tDiff: tensor([-0.6069,  0.1076,  0.1667,  0.3326]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[0., 0., 0., 0.]])\n",
      "\n",
      ">>> Iter: 0002 (0.024s): loss = 00.303463, likelihood = 0.0000\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass didn't converge by 100.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.93902075 0.06097929]\n",
      " [0.9660942  0.03390587]\n",
      " [0.9660942  0.03390587]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [6.146894 3.384167 3.384167 0.      ], \n",
      "\tQ: [[6.0839763 3.349673 ]\n",
      " [3.349673  0.       ]\n",
      " [3.349673  0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [9.4026023e-01 5.9050620e-02 0.0000000e+00 6.8918540e-04] \n",
      "\tDiff: tensor([-0.6069,  0.1076,  0.1667,  0.3326]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[0., 0., 0., 0.]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass didn't converge by 100.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.93902075 0.06097929]\n",
      " [0.9660942  0.03390587]\n",
      " [0.9660942  0.03390587]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [6.146894 3.384167 3.384167 0.      ], \n",
      "\tQ: [[6.0839763 3.349673 ]\n",
      " [3.349673  0.       ]\n",
      " [3.349673  0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [9.4026023e-01 5.9050620e-02 0.0000000e+00 6.8918540e-04] \n",
      "\tDiff: tensor([-0.6069,  0.1076,  0.1667,  0.3326]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[0., 0., 0., 0.]])\n",
      "\n",
      ">>> Iter: 0003 (0.023s): loss = 00.303463, likelihood = 0.0000\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass didn't converge by 100.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.93902075 0.06097929]\n",
      " [0.9660942  0.03390587]\n",
      " [0.9660942  0.03390587]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [6.146894 3.384167 3.384167 0.      ], \n",
      "\tQ: [[6.0839763 3.349673 ]\n",
      " [3.349673  0.       ]\n",
      " [3.349673  0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [9.4026023e-01 5.9050620e-02 0.0000000e+00 6.8918540e-04] \n",
      "\tDiff: tensor([-0.6069,  0.1076,  0.1667,  0.3326]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[0., 0., 0., 0.]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass didn't converge by 100.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.93902075 0.06097929]\n",
      " [0.9660942  0.03390587]\n",
      " [0.9660942  0.03390587]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [6.146894 3.384167 3.384167 0.      ], \n",
      "\tQ: [[6.0839763 3.349673 ]\n",
      " [3.349673  0.       ]\n",
      " [3.349673  0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [9.4026023e-01 5.9050620e-02 0.0000000e+00 6.8918540e-04] \n",
      "\tDiff: tensor([-0.6069,  0.1076,  0.1667,  0.3326]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[0., 0., 0., 0.]])\n",
      "\n",
      ">>> Iter: 0004 (0.025s): loss = 00.303463, likelihood = 0.0000\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass didn't converge by 100.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.93902075 0.06097929]\n",
      " [0.9660942  0.03390587]\n",
      " [0.9660942  0.03390587]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [6.146894 3.384167 3.384167 0.      ], \n",
      "\tQ: [[6.0839763 3.349673 ]\n",
      " [3.349673  0.       ]\n",
      " [3.349673  0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [9.4026023e-01 5.9050620e-02 0.0000000e+00 6.8918540e-04] \n",
      "\tDiff: tensor([-0.6069,  0.1076,  0.1667,  0.3326]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[0., 0., 0., 0.]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass didn't converge by 100.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.93902075 0.06097929]\n",
      " [0.9660942  0.03390587]\n",
      " [0.9660942  0.03390587]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [6.146894 3.384167 3.384167 0.      ], \n",
      "\tQ: [[6.0839763 3.349673 ]\n",
      " [3.349673  0.       ]\n",
      " [3.349673  0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [9.4026023e-01 5.9050620e-02 0.0000000e+00 6.8918540e-04] \n",
      "\tDiff: tensor([-0.6069,  0.1076,  0.1667,  0.3326]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[0., 0., 0., 0.]])\n",
      "\n",
      ">>> Iter: 0005 (0.026s): loss = 00.303463, likelihood = 0.0000\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass didn't converge by 100.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.93902075 0.06097929]\n",
      " [0.9660942  0.03390587]\n",
      " [0.9660942  0.03390587]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [6.146894 3.384167 3.384167 0.      ], \n",
      "\tQ: [[6.0839763 3.349673 ]\n",
      " [3.349673  0.       ]\n",
      " [3.349673  0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [9.4026023e-01 5.9050620e-02 0.0000000e+00 6.8918540e-04] \n",
      "\tDiff: tensor([-0.6069,  0.1076,  0.1667,  0.3326]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[0., 0., 0., 0.]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass didn't converge by 100.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.93902075 0.06097929]\n",
      " [0.9660942  0.03390587]\n",
      " [0.9660942  0.03390587]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [6.146894 3.384167 3.384167 0.      ], \n",
      "\tQ: [[6.0839763 3.349673 ]\n",
      " [3.349673  0.       ]\n",
      " [3.349673  0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [9.4026023e-01 5.9050620e-02 0.0000000e+00 6.8918540e-04] \n",
      "\tDiff: tensor([-0.6069,  0.1076,  0.1667,  0.3326]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[0., 0., 0., 0.]])\n",
      "\n",
      ">>> Iter: 0006 (0.026s): loss = 00.303463, likelihood = 0.0000\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass didn't converge by 100.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.93902075 0.06097929]\n",
      " [0.9660942  0.03390587]\n",
      " [0.9660942  0.03390587]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [6.146894 3.384167 3.384167 0.      ], \n",
      "\tQ: [[6.0839763 3.349673 ]\n",
      " [3.349673  0.       ]\n",
      " [3.349673  0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [9.4026023e-01 5.9050620e-02 0.0000000e+00 6.8918540e-04] \n",
      "\tDiff: tensor([-0.6069,  0.1076,  0.1667,  0.3326]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[0., 0., 0., 0.]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass didn't converge by 100.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.93902075 0.06097929]\n",
      " [0.9660942  0.03390587]\n",
      " [0.9660942  0.03390587]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [6.146894 3.384167 3.384167 0.      ], \n",
      "\tQ: [[6.0839763 3.349673 ]\n",
      " [3.349673  0.       ]\n",
      " [3.349673  0.       ]\n",
      " [0.        0.       ]]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [9.4026023e-01 5.9050620e-02 0.0000000e+00 6.8918540e-04] \n",
      "\tDiff: tensor([-0.6069,  0.1076,  0.1667,  0.3326]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[0., 0., 0., 0.]])\n",
      "\n",
      ">>> Iter: 0007 (0.024s): loss = 00.303463, likelihood = 0.0000\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass didn't converge by 100.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.93902075 0.06097929]\n",
      " [0.9660942  0.03390587]\n",
      " [0.9660942  0.03390587]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [6.146894 3.384167 3.384167 0.      ], \n",
      "\tQ: [[6.0839763 3.349673 ]\n",
      " [3.349673  0.       ]\n",
      " [3.349673  0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [9.4026023e-01 5.9050620e-02 0.0000000e+00 6.8918540e-04] \n",
      "\tDiff: tensor([-0.6069,  0.1076,  0.1667,  0.3326]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[0., 0., 0., 0.]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass didn't converge by 100.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.93902075 0.06097929]\n",
      " [0.9660942  0.03390587]\n",
      " [0.9660942  0.03390587]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [6.146894 3.384167 3.384167 0.      ], \n",
      "\tQ: [[6.0839763 3.349673 ]\n",
      " [3.349673  0.       ]\n",
      " [3.349673  0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [9.4026023e-01 5.9050620e-02 0.0000000e+00 6.8918540e-04] \n",
      "\tDiff: tensor([-0.6069,  0.1076,  0.1667,  0.3326]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[0., 0., 0., 0.]])\n",
      "\n",
      ">>> Iter: 0008 (0.026s): loss = 00.303463, likelihood = 0.0000\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass didn't converge by 100.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.93902075 0.06097929]\n",
      " [0.9660942  0.03390587]\n",
      " [0.9660942  0.03390587]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [6.146894 3.384167 3.384167 0.      ], \n",
      "\tQ: [[6.0839763 3.349673 ]\n",
      " [3.349673  0.       ]\n",
      " [3.349673  0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [9.4026023e-01 5.9050620e-02 0.0000000e+00 6.8918540e-04] \n",
      "\tDiff: tensor([-0.6069,  0.1076,  0.1667,  0.3326]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[0., 0., 0., 0.]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass didn't converge by 100.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.93902075 0.06097929]\n",
      " [0.9660942  0.03390587]\n",
      " [0.9660942  0.03390587]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [6.146894 3.384167 3.384167 0.      ], \n",
      "\tQ: [[6.0839763 3.349673 ]\n",
      " [3.349673  0.       ]\n",
      " [3.349673  0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [9.4026023e-01 5.9050620e-02 0.0000000e+00 6.8918540e-04] \n",
      "\tDiff: tensor([-0.6069,  0.1076,  0.1667,  0.3326]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[0., 0., 0., 0.]])\n",
      "\n",
      ">>> Iter: 0009 (0.024s): loss = 00.303463, likelihood = 0.0000\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass didn't converge by 100.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.93902075 0.06097929]\n",
      " [0.9660942  0.03390587]\n",
      " [0.9660942  0.03390587]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [6.146894 3.384167 3.384167 0.      ], \n",
      "\tQ: [[6.0839763 3.349673 ]\n",
      " [3.349673  0.       ]\n",
      " [3.349673  0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [9.4026023e-01 5.9050620e-02 0.0000000e+00 6.8918540e-04] \n",
      "\tDiff: tensor([-0.6069,  0.1076,  0.1667,  0.3326]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[0., 0., 0., 0.]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass didn't converge by 100.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.93902075 0.06097929]\n",
      " [0.9660942  0.03390587]\n",
      " [0.9660942  0.03390587]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [6.146894 3.384167 3.384167 0.      ], \n",
      "\tQ: [[6.0839763 3.349673 ]\n",
      " [3.349673  0.       ]\n",
      " [3.349673  0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [9.4026023e-01 5.9050620e-02 0.0000000e+00 6.8918540e-04] \n",
      "\tDiff: tensor([-0.6069,  0.1076,  0.1667,  0.3326]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[0., 0., 0., 0.]])\n",
      "\n",
      ">>> Iter: 0010 (0.029s): loss = 00.303463, likelihood = 0.0000\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(-0.0039, grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass didn't converge by 100.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.93902075 0.06097929]\n",
      " [0.9660942  0.03390587]\n",
      " [0.9660942  0.03390587]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [6.146894 3.384167 3.384167 0.      ], \n",
      "\tQ: [[6.0839763 3.349673 ]\n",
      " [3.349673  0.       ]\n",
      " [3.349673  0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [9.4026023e-01 5.9050620e-02 0.0000000e+00 6.8918540e-04] \n",
      "\tDiff: tensor([-0.6069,  0.1076,  0.1667,  0.3326]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[0.3326, 0.0000, 0.0000, 0.0000]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(-0.0039, grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass didn't converge by 100.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.93902075 0.06097929]\n",
      " [0.9660942  0.03390587]\n",
      " [0.9660942  0.03390587]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [6.146894 3.384167 3.384167 0.      ], \n",
      "\tQ: [[6.0839763 3.349673 ]\n",
      " [3.349673  0.       ]\n",
      " [3.349673  0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [9.4026023e-01 5.9050620e-02 0.0000000e+00 6.8918540e-04] \n",
      "\tDiff: tensor([-0.6069,  0.1076,  0.1667,  0.3326]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[0.6653, 0.0000, 0.0000, 0.0000]])\n",
      "\n",
      ">>> Iter: 0011 (0.029s): loss = 00.303463, likelihood = 0.0000\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-0.0526, grad_fn=<SelectBackward>), tensor(-0.0576, grad_fn=<SelectBackward>), tensor(-0.0570, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ .. ] Backward pass converged @ 66.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.90392804 0.09607198]\n",
      " [0.920698   0.0793019 ]\n",
      " [0.9211922  0.07880783]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [4.741915  2.4768715 2.4837334 0.       ], \n",
      "\tQ: [[ 4.6409097   2.3992577 ]\n",
      " [ 2.3942482  -0.05762178]\n",
      " [ 2.4016469  -0.05700947]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.90700465 0.09045581 0.         0.00253956] \n",
      "\tDiff: tensor([-0.5737,  0.0762,  0.1667,  0.3308]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.1667,  0.0762, -0.5737]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-0.0526, grad_fn=<SelectBackward>), tensor(-0.0576, grad_fn=<SelectBackward>), tensor(-0.0570, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ .. ] Backward pass converged @ 66.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.90392804 0.09607198]\n",
      " [0.920698   0.0793019 ]\n",
      " [0.9211922  0.07880783]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [4.741915  2.4768715 2.4837334 0.       ], \n",
      "\tQ: [[ 4.6409097   2.3992577 ]\n",
      " [ 2.3942482  -0.05762178]\n",
      " [ 2.4016469  -0.05700947]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.90700465 0.09045581 0.         0.00253956] \n",
      "\tDiff: tensor([-0.5737,  0.0762,  0.1667,  0.3308]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.3333,  0.1524, -1.1473]])\n",
      "\n",
      ">>> Iter: 0012 (0.021s): loss = 00.286836, likelihood = 0.0001\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-0.1018, grad_fn=<SelectBackward>), tensor(-0.0172, grad_fn=<SelectBackward>), tensor(-0.0123, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ .. ] Backward pass converged @ 81.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.85946894 0.14053112]\n",
      " [0.9529001  0.04709985]\n",
      " [0.95658654 0.04341337]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [4.867748  3.0382483 3.124665  0.       ], \n",
      "\tQ: [[ 4.716307    2.9054215 ]\n",
      " [ 2.990003   -0.01723712]\n",
      " [ 3.080281   -0.01232284]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.8660519  0.13174179 0.         0.00220633] \n",
      "\tDiff: tensor([-0.5327,  0.0349,  0.1667,  0.3311]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.1667,  0.0349, -0.5327]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-0.1018, grad_fn=<SelectBackward>), tensor(-0.0172, grad_fn=<SelectBackward>), tensor(-0.0123, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ .. ] Backward pass converged @ 81.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.85946894 0.14053112]\n",
      " [0.9529001  0.04709985]\n",
      " [0.95658654 0.04341337]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [4.867748  3.0382483 3.124665  0.       ], \n",
      "\tQ: [[ 4.716307    2.9054215 ]\n",
      " [ 2.990003   -0.01723712]\n",
      " [ 3.080281   -0.01232284]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.8660519  0.13174179 0.         0.00220633] \n",
      "\tDiff: tensor([-0.5327,  0.0349,  0.1667,  0.3311]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.3333,  0.0698, -1.0654]])\n",
      "\n",
      ">>> Iter: 0013 (0.022s): loss = 00.266359, likelihood = 0.0000\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-0.1673, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 95.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.80537915 0.19462097]\n",
      " [0.96597147 0.03402844]\n",
      " [0.96597147 0.03402844]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [4.815339  3.3805585 3.3805585 0.       ], \n",
      "\tQ: [[4.598897  3.1786377]\n",
      " [3.3459375 0.       ]\n",
      " [3.3459375 0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.8180049  0.17978764 0.         0.00220755] \n",
      "\tDiff: tensor([-0.4847, -0.0131,  0.1667,  0.3311]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.4847]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-0.1673, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 95.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.80537915 0.19462097]\n",
      " [0.96597147 0.03402844]\n",
      " [0.96597147 0.03402844]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [4.815339  3.3805585 3.3805585 0.       ], \n",
      "\tQ: [[4.598897  3.1786377]\n",
      " [3.3459375 0.       ]\n",
      " [3.3459375 0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.8180049  0.17978764 0.         0.00220755] \n",
      "\tDiff: tensor([-0.4847, -0.0131,  0.1667,  0.3311]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.9693]])\n",
      "\n",
      ">>> Iter: 0014 (0.025s): loss = 00.248896, likelihood = 0.0000\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-0.2433, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 94.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.7489436  0.25105628]\n",
      " [0.9659436  0.03405649]\n",
      " [0.9659436  0.03405649]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [4.4838977 3.3797348 3.3797348 0.       ], \n",
      "\tQ: [[4.194806  3.1018195]\n",
      " [3.345085  0.       ]\n",
      " [3.345085  0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.7699534  0.2271965  0.         0.00285003] \n",
      "\tDiff: tensor([-0.4366, -0.0605,  0.1667,  0.3305]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.4366]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-0.2433, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Backward Pass  [ ... ] Backward pass converged @ 94.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.7489436  0.25105628]\n",
      " [0.9659436  0.03405649]\n",
      " [0.9659436  0.03405649]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [4.4838977 3.3797348 3.3797348 0.       ], \n",
      "\tQ: [[4.194806  3.1018195]\n",
      " [3.345085  0.       ]\n",
      " [3.345085  0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.7699534  0.2271965  0.         0.00285003] \n",
      "\tDiff: tensor([-0.4366, -0.0605,  0.1667,  0.3305]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.8732]])\n",
      "\n",
      ">>> Iter: 0015 (0.029s): loss = 00.248575, likelihood = 0.0001\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-0.3264, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 94.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.6912044  0.30879542]\n",
      " [0.9659436  0.03405649]\n",
      " [0.9659436  0.03405649]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [4.1937375 3.3797348 3.3797348 0.       ], \n",
      "\tQ: [[3.8244178 3.0186613]\n",
      " [3.345085  0.       ]\n",
      " [3.345085  0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.7229893  0.27350506 0.         0.0035055 ] \n",
      "\tDiff: tensor([-0.3897, -0.1068,  0.1667,  0.3298]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.3897]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-0.3264, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 94.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.6912044  0.30879542]\n",
      " [0.9659436  0.03405649]\n",
      " [0.9659436  0.03405649]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [4.1937375 3.3797348 3.3797348 0.       ], \n",
      "\tQ: [[3.8244178 3.0186613]\n",
      " [3.345085  0.       ]\n",
      " [3.345085  0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.7229893  0.27350506 0.         0.0035055 ] \n",
      "\tDiff: tensor([-0.3897, -0.1068,  0.1667,  0.3298]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.7793]])\n",
      "\n",
      ">>> Iter: 0016 (0.024s): loss = 00.248247, likelihood = 0.0001\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-0.4145, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 93.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.6345521  0.36544797]\n",
      " [0.96591413 0.03408583]\n",
      " [0.96591413 0.03408583]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [3.9363353 3.3788736 3.3788736 0.       ], \n",
      "\tQ: [[3.4814994 2.929704 ]\n",
      " [3.3441932 0.       ]\n",
      " [3.3441932 0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.6790695 0.3167784 0.        0.0041522] \n",
      "\tDiff: tensor([-0.3457, -0.1501,  0.1667,  0.3292]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.3457]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-0.4145, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 93.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.6345521  0.36544797]\n",
      " [0.96591413 0.03408583]\n",
      " [0.96591413 0.03408583]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [3.9363353 3.3788736 3.3788736 0.       ], \n",
      "\tQ: [[3.4814994 2.929704 ]\n",
      " [3.3441932 0.       ]\n",
      " [3.3441932 0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.6790695 0.3167784 0.        0.0041522] \n",
      "\tDiff: tensor([-0.3457, -0.1501,  0.1667,  0.3292]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.6915]])\n",
      "\n",
      ">>> Iter: 0017 (0.025s): loss = 00.247924, likelihood = 0.0002\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-0.5057, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 93.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.58055496 0.41944504]\n",
      " [0.96591413 0.03408583]\n",
      " [0.96591413 0.03408583]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [3.7072885 3.3788736 3.3788736 0.       ], \n",
      "\tQ: [[3.1635177 2.8384657]\n",
      " [3.3441932 0.       ]\n",
      " [3.3441932 0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.6391997  0.3560346  0.         0.00476571] \n",
      "\tDiff: tensor([-0.3059, -0.1894,  0.1667,  0.3286]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.3059]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-0.5057, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 93.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.58055496 0.41944504]\n",
      " [0.96591413 0.03408583]\n",
      " [0.96591413 0.03408583]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [3.7072885 3.3788736 3.3788736 0.       ], \n",
      "\tQ: [[3.1635177 2.8384657]\n",
      " [3.3441932 0.       ]\n",
      " [3.3441932 0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.6391997  0.3560346  0.         0.00476571] \n",
      "\tDiff: tensor([-0.3059, -0.1894,  0.1667,  0.3286]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.6117]])\n",
      "\n",
      ">>> Iter: 0018 (0.025s): loss = 00.247617, likelihood = 0.0002\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-0.5988, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 93.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.53007865 0.46992138]\n",
      " [0.96591413 0.03408583]\n",
      " [0.96591413 0.03408583]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [3.5006206 3.3788736 3.3788736 0.       ], \n",
      "\tQ: [[2.8658907 2.7454307]\n",
      " [3.3441932 0.       ]\n",
      " [3.3441932 0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.60368735 0.39097345 0.         0.00533922] \n",
      "\tDiff: tensor([-0.2704, -0.2243,  0.1667,  0.3280]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.2704]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-0.5988, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 93.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.53007865 0.46992138]\n",
      " [0.96591413 0.03408583]\n",
      " [0.96591413 0.03408583]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [3.5006206 3.3788736 3.3788736 0.       ], \n",
      "\tQ: [[2.8658907 2.7454307]\n",
      " [3.3441932 0.       ]\n",
      " [3.3441932 0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.60368735 0.39097345 0.         0.00533922] \n",
      "\tDiff: tensor([-0.2704, -0.2243,  0.1667,  0.3280]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.5407]])\n",
      "\n",
      ">>> Iter: 0019 (0.028s): loss = 00.247330, likelihood = 0.0003\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-0.6925, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 93.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.48356932 0.51643074]\n",
      " [0.96591413 0.03408583]\n",
      " [0.96591413 0.03408583]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [3.3125265 3.3788736 3.3788736 0.       ], \n",
      "\tQ: [[2.5859659 2.6517124]\n",
      " [3.3441932 0.       ]\n",
      " [3.3441932 0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.57246953 0.42166284 0.         0.00586766] \n",
      "\tDiff: tensor([-0.2391, -0.2550,  0.1667,  0.3275]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.2391]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-0.6925, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 93.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.48356932 0.51643074]\n",
      " [0.96591413 0.03408583]\n",
      " [0.96591413 0.03408583]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [3.3125265 3.3788736 3.3788736 0.       ], \n",
      "\tQ: [[2.5859659 2.6517124]\n",
      " [3.3441932 0.       ]\n",
      " [3.3441932 0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.57246953 0.42166284 0.         0.00586766] \n",
      "\tDiff: tensor([-0.2391, -0.2550,  0.1667,  0.3275]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.4783]])\n",
      "\n",
      ">>> Iter: 0020 (0.023s): loss = 00.247066, likelihood = 0.0003\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-0.7860, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 93.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.44116893 0.5588311 ]\n",
      " [0.96591413 0.03408583]\n",
      " [0.96591413 0.03408583]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [3.1401246 3.3788736 3.3788736 0.       ], \n",
      "\tQ: [[2.3217971 2.5582166]\n",
      " [3.3441932 0.       ]\n",
      " [3.3441932 0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.54526633 0.44838428 0.         0.00634941] \n",
      "\tDiff: tensor([-0.2119, -0.2817,  0.1667,  0.3270]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.2119]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-0.7860, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 93.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.44116893 0.5588311 ]\n",
      " [0.96591413 0.03408583]\n",
      " [0.96591413 0.03408583]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [3.1401246 3.3788736 3.3788736 0.       ], \n",
      "\tQ: [[2.3217971 2.5582166]\n",
      " [3.3441932 0.       ]\n",
      " [3.3441932 0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.54526633 0.44838428 0.         0.00634941] \n",
      "\tDiff: tensor([-0.2119, -0.2817,  0.1667,  0.3270]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.4239]])\n",
      "\n",
      ">>> Iter: 0021 (0.024s): loss = 00.246825, likelihood = 0.0004\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-0.8785, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.40280327 0.59719676]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [2.980254  3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[2.070947  2.4647453]\n",
      " [3.3432608 0.       ]\n",
      " [3.3432608 0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.5216846  0.471524   0.         0.00679143] \n",
      "\tDiff: tensor([-0.1884, -0.3049,  0.1667,  0.3265]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.1884]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-0.8785, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.40280327 0.59719676]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [2.980254  3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[2.070947  2.4647453]\n",
      " [3.3432608 0.       ]\n",
      " [3.3432608 0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.5216846  0.471524   0.         0.00679143] \n",
      "\tDiff: tensor([-0.1884, -0.3049,  0.1667,  0.3265]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.3767]])\n",
      "\n",
      ">>> Iter: 0022 (0.024s): loss = 00.246604, likelihood = 0.0004\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-0.9695, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.36831206 0.6316879 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [2.8331127 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[1.834288  2.3737528]\n",
      " [3.3432608 0.       ]\n",
      " [3.3432608 0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.501322   0.49149433 0.         0.00718367] \n",
      "\tDiff: tensor([-0.1680, -0.3248,  0.1667,  0.3261]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.1680]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-0.9695, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.36831206 0.6316879 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [2.8331127 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[1.834288  2.3737528]\n",
      " [3.3432608 0.       ]\n",
      " [3.3432608 0.       ]\n",
      " [0.        0.       ]]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.501322   0.49149433 0.         0.00718367] \n",
      "\tDiff: tensor([-0.1680, -0.3248,  0.1667,  0.3261]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.3360]])\n",
      "\n",
      ">>> Iter: 0023 (0.029s): loss = 00.246408, likelihood = 0.0005\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-1.0585, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.33741733 0.6625827 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [2.696383  3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[1.6099483 2.284773 ]\n",
      " [3.3432608 0.       ]\n",
      " [3.3432608 0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.48375595 0.50870913 0.         0.00753501] \n",
      "\tDiff: tensor([-0.1504, -0.3420,  0.1667,  0.3258]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.1504]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-1.0585, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.33741733 0.6625827 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [2.696383  3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[1.6099483 2.284773 ]\n",
      " [3.3432608 0.       ]\n",
      " [3.3432608 0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.48375595 0.50870913 0.         0.00753501] \n",
      "\tDiff: tensor([-0.1504, -0.3420,  0.1667,  0.3258]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.3008]])\n",
      "\n",
      ">>> Iter: 0024 (0.024s): loss = 00.246233, likelihood = 0.0005\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-1.1451, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.30981955 0.6901805 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [2.5689685 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[1.3972033 2.1981664]\n",
      " [3.3432608 0.       ]\n",
      " [3.3432608 0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.46860257 0.5235486  0.         0.00784886] \n",
      "\tDiff: tensor([-0.1353, -0.3569,  0.1667,  0.3255]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.1353]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-1.1451, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.30981955 0.6901805 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [2.5689685 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[1.3972033 2.1981664]\n",
      " [3.3432608 0.       ]\n",
      " [3.3432608 0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.46860257 0.5235486  0.         0.00784886] \n",
      "\tDiff: tensor([-0.1353, -0.3569,  0.1667,  0.3255]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.2705]])\n",
      "\n",
      ">>> Iter: 0025 (0.026s): loss = 00.246076, likelihood = 0.0006\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-1.2291, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.28520834 0.71479166]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [2.4499686 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[1.1954333 2.1142044]\n",
      " [3.3432608 0.       ]\n",
      " [3.3432608 0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.45551738 0.5363539  0.         0.00812874] \n",
      "\tDiff: tensor([-0.1222, -0.3697,  0.1667,  0.3252]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.1222]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-1.2291, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.28520834 0.71479166]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [2.4499686 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[1.1954333 2.1142044]\n",
      " [3.3432608 0.       ]\n",
      " [3.3432608 0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.45551738 0.5363539  0.         0.00812874] \n",
      "\tDiff: tensor([-0.1222, -0.3697,  0.1667,  0.3252]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.2444]])\n",
      "\n",
      ">>> Iter: 0026 (0.025s): loss = 00.245936, likelihood = 0.0006\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-1.3102, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.26327893 0.73672116]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [2.3386295 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[1.0040883 2.0330837]\n",
      " [3.3432608 0.       ]\n",
      " [3.3432608 0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.44419825 0.54742366 0.         0.00837813] \n",
      "\tDiff: tensor([-0.1109, -0.3808,  0.1667,  0.3250]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.1109]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-1.3102, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.26327893 0.73672116]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [2.3386295 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[1.0040883 2.0330837]\n",
      " [3.3432608 0.       ]\n",
      " [3.3432608 0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.44419825 0.54742366 0.         0.00837813] \n",
      "\tDiff: tensor([-0.1109, -0.3808,  0.1667,  0.3250]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.2217]])\n",
      "\n",
      ">>> Iter: 0027 (0.028s): loss = 00.245811, likelihood = 0.0006\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-1.3883, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.24374242 0.75625753]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [2.2343104 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[0.8226671 1.9549371]\n",
      " [3.3432608 0.       ]\n",
      " [3.3432608 0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.43438426 0.5570154  0.         0.0086003 ] \n",
      "\tDiff: tensor([-0.1011, -0.3903,  0.1667,  0.3247]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.1011]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-1.3883, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.24374242 0.75625753]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [2.2343104 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[0.8226671 1.9549371]\n",
      " [3.3432608 0.       ]\n",
      " [3.3432608 0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.43438426 0.5570154  0.         0.0086003 ] \n",
      "\tDiff: tensor([-0.1011, -0.3903,  0.1667,  0.3247]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.2021]])\n",
      "\n",
      ">>> Iter: 0028 (0.023s): loss = 00.245700, likelihood = 0.0007\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-1.4634, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.22633135 0.7736686 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [2.1364574 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[0.6507023 1.8798457]\n",
      " [3.3432608 0.       ]\n",
      " [3.3432608 0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.42585242 0.5653493  0.         0.0087983 ] \n",
      "\tDiff: tensor([-0.0925, -0.3987,  0.1667,  0.3245]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0925]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-1.4634, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.22633135 0.7736686 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [2.1364574 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[0.6507023 1.8798457]\n",
      " [3.3432608 0.       ]\n",
      " [3.3432608 0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.42585242 0.5653493  0.         0.0087983 ] \n",
      "\tDiff: tensor([-0.0925, -0.3987,  0.1667,  0.3245]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.1850]])\n",
      "\n",
      ">>> Iter: 0029 (0.024s): loss = 00.245601, likelihood = 0.0007\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-1.5354, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.21080247 0.7891977 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [2.0445857 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[0.487752  1.8078473]\n",
      " [3.3432608 0.       ]\n",
      " [3.3432608 0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.41841337 0.57261187 0.         0.0089749 ] \n",
      "\tDiff: tensor([-0.0851, -0.4059,  0.1667,  0.3244]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0851]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-1.5354, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.21080247 0.7891977 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [2.0445857 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[0.487752  1.8078473]\n",
      " [3.3432608 0.       ]\n",
      " [3.3432608 0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.41841337 0.57261187 0.         0.0089749 ] \n",
      "\tDiff: tensor([-0.0851, -0.4059,  0.1667,  0.3244]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.1702]])\n",
      "\n",
      ">>> Iter: 0030 (0.024s): loss = 00.245513, likelihood = 0.0007\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-1.6043, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.19693662 0.80306333]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.9582658 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[0.33339253 1.738944  ]\n",
      " [3.3432608  0.        ]\n",
      " [3.3432608  0.        ]\n",
      " [0.         0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.41190687 0.5789605  0.         0.00913258] \n",
      "\tDiff: tensor([-0.0786, -0.4123,  0.1667,  0.3242]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0786]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-1.6043, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.19693662 0.80306333]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.9582658 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[0.33339253 1.738944  ]\n",
      " [3.3432608  0.        ]\n",
      " [3.3432608  0.        ]\n",
      " [0.         0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.41190687 0.5789605  0.         0.00913258] \n",
      "\tDiff: tensor([-0.0786, -0.4123,  0.1667,  0.3242]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.1571]])\n",
      "\n",
      ">>> Iter: 0031 (0.025s): loss = 00.245434, likelihood = 0.0008\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-1.6702, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.18453893 0.81546104]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.8771119 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[0.1872171 1.6731102]\n",
      " [3.3432608 0.       ]\n",
      " [3.3432608 0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.40619788 0.58452857 0.         0.00927357] \n",
      "\tDiff: tensor([-0.0729, -0.4179,  0.1667,  0.3241]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0729]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-1.6702, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.18453893 0.81546104]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.8771119 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[0.1872171 1.6731102]\n",
      " [3.3432608 0.       ]\n",
      " [3.3432608 0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.40619788 0.58452857 0.         0.00927357] \n",
      "\tDiff: tensor([-0.0729, -0.4179,  0.1667,  0.3241]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.1457]])\n",
      "\n",
      ">>> Iter: 0032 (0.026s): loss = 00.245363, likelihood = 0.0008\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-1.7330, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.17343646 0.8265635 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.8007758 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[0.04883187 1.6102972 ]\n",
      " [3.3432608  0.        ]\n",
      " [3.3432608  0.        ]\n",
      " [0.         0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.4011722  0.5894279  0.         0.00939983] \n",
      "\tDiff: tensor([-0.0678, -0.4228,  0.1667,  0.3239]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0678]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-1.7330, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.17343646 0.8265635 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.8007758 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[0.04883187 1.6102972 ]\n",
      " [3.3432608  0.        ]\n",
      " [3.3432608  0.        ]\n",
      " [0.         0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.4011722  0.5894279  0.         0.00939983] \n",
      "\tDiff: tensor([-0.0678, -0.4228,  0.1667,  0.3239]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.1357]])\n",
      "\n",
      ">>> Iter: 0033 (0.024s): loss = 00.245300, likelihood = 0.0008\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-1.7928, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.16347688 0.8365231 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.7289397 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-0.08214398  1.5504385 ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.39673388 0.59375304 0.         0.00951309] \n",
      "\tDiff: tensor([-0.0634, -0.4271,  0.1667,  0.3238]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0634]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-1.7928, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.16347688 0.8365231 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.7289397 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-0.08214398  1.5504385 ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.39673388 0.59375304 0.         0.00951309] \n",
      "\tDiff: tensor([-0.0634, -0.4271,  0.1667,  0.3238]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.1268]])\n",
      "\n",
      ">>> Iter: 0034 (0.026s): loss = 00.245243, likelihood = 0.0008\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-1.8498, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.15452622 0.8454737 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.6613123 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-0.20607908  1.4934541 ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.39280152 0.59758353 0.         0.00961488] \n",
      "\tDiff: tensor([-0.0595, -0.4309,  0.1667,  0.3237]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0595]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-1.8498, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.15452622 0.8454737 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.6613123 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-0.20607908  1.4934541 ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.39280152 0.59758353 0.         0.00961488] \n",
      "\tDiff: tensor([-0.0595, -0.4309,  0.1667,  0.3237]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.1189]])\n",
      "\n",
      ">>> Iter: 0035 (0.025s): loss = 00.245193, likelihood = 0.0008\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-1.9040, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.14646706 0.85353297]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.5976245 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-0.3233302  1.4392534]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.38930655 0.60098696 0.         0.00970653] \n",
      "\tDiff: tensor([-0.0560, -0.4343,  0.1667,  0.3236]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0560]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-1.9040, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.14646706 0.85353297]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.5976245 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-0.3233302  1.4392534]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.38930655 0.60098696 0.         0.00970653] \n",
      "\tDiff: tensor([-0.0560, -0.4343,  0.1667,  0.3236]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.1119]])\n",
      "\n",
      ">>> Iter: 0036 (0.027s): loss = 00.245147, likelihood = 0.0008\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-1.9555, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.13919641 0.86080354]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.5376276 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-0.4342418  1.3877386]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.38619068 0.60402006 0.         0.00978921] \n",
      "\tDiff: tensor([-0.0529, -0.4374,  0.1667,  0.3235]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0529]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-1.9555, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.13919641 0.86080354]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.5376276 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-0.4342418  1.3877386]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.38619068 0.60402006 0.         0.00978921] \n",
      "\tDiff: tensor([-0.0529, -0.4374,  0.1667,  0.3235]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.1057]])\n",
      "\n",
      ">>> Iter: 0037 (0.029s): loss = 00.245105, likelihood = 0.0009\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.0045, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.13262422 0.8673758 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.4810896 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-0.539146   1.3388066]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3834045  0.6067316  0.         0.00986395] \n",
      "\tDiff: tensor([-0.0501, -0.4401,  0.1667,  0.3235]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0501]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.0045, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.13262422 0.8673758 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.4810896 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-0.539146   1.3388066]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3834045  0.6067316  0.         0.00986395] \n",
      "\tDiff: tensor([-0.0501, -0.4401,  0.1667,  0.3235]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.1001]])\n",
      "\n",
      ">>> Iter: 0038 (0.027s): loss = 00.245068, likelihood = 0.0009\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.0509, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.12667157 0.87332845]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.427795  3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-0.6383625  1.2923515]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.38090575 0.6091626  0.         0.00993165] \n",
      "\tDiff: tensor([-0.0476, -0.4425,  0.1667,  0.3234]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0476]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.0509, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.12667157 0.87332845]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.427795  3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-0.6383625  1.2923515]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.38090575 0.6091626  0.         0.00993165] \n",
      "\tDiff: tensor([-0.0476, -0.4425,  0.1667,  0.3234]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0951]])\n",
      "\n",
      ">>> Iter: 0039 (0.025s): loss = 00.245034, likelihood = 0.0009\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.0950, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.12126938 0.87873065]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.3775421 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-0.7321988  1.2482653]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.37865853 0.6113484  0.         0.00999308] \n",
      "\tDiff: tensor([-0.0453, -0.4447,  0.1667,  0.3233]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0453]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.0950, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.12126938 0.87873065]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.3775421 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-0.7321988  1.2482653]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.37865853 0.6113484  0.         0.00999308] \n",
      "\tDiff: tensor([-0.0453, -0.4447,  0.1667,  0.3233]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0907]])\n",
      "\n",
      ">>> Iter: 0040 (0.030s): loss = 00.245003, likelihood = 0.0009\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.1368, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.11635705 0.883643  ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.3301424 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-0.8209494  1.2064402]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.376632   0.6133191  0.         0.01004895] \n",
      "\tDiff: tensor([-0.0433, -0.4467,  0.1667,  0.3233]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0433]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.1368, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.11635705 0.883643  ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.3301424 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-0.8209494  1.2064402]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.376632   0.6133191  0.         0.01004895] \n",
      "\tDiff: tensor([-0.0433, -0.4467,  0.1667,  0.3233]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0866]])\n",
      "\n",
      ">>> Iter: 0041 (0.025s): loss = 00.244976, likelihood = 0.0009\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.1765, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.11188145 0.8881185 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.2854191 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-0.90489644  1.166769  ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.37479964 0.6151005  0.         0.01009984] \n",
      "\tDiff: tensor([-0.0415, -0.4484,  0.1667,  0.3232]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0415]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.1765, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.11188145 0.8881185 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.2854191 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-0.90489644  1.166769  ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.37479964 0.6151005  0.         0.01009984] \n",
      "\tDiff: tensor([-0.0415, -0.4484,  0.1667,  0.3232]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0829]])\n",
      "\n",
      ">>> Iter: 0042 (0.024s): loss = 00.244950, likelihood = 0.0009\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.2141, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.10779584 0.89220417]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.2432064 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-0.98430973  1.1291461 ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.37313858 0.61671513 0.         0.01014631] \n",
      "\tDiff: tensor([-0.0398, -0.4500,  0.1667,  0.3232]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0398]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.2141, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.10779584 0.89220417]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.2432064 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-0.98430973  1.1291461 ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.37313858 0.61671513 0.         0.01014631] \n",
      "\tDiff: tensor([-0.0398, -0.4500,  0.1667,  0.3232]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0796]])\n",
      "\n",
      ">>> Iter: 0043 (0.025s): loss = 00.244927, likelihood = 0.0009\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.2498, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.10405918 0.8959409 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.2033488 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-1.0594467  1.093468 ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.37162915 0.6181821  0.         0.0101888 ] \n",
      "\tDiff: tensor([-0.0383, -0.4515,  0.1667,  0.3231]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0383]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.2498, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.10405918 0.8959409 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.2033488 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-1.0594467  1.093468 ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.37162915 0.6181821  0.         0.0101888 ] \n",
      "\tDiff: tensor([-0.0383, -0.4515,  0.1667,  0.3231]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0766]])\n",
      "\n",
      ">>> Iter: 0044 (0.024s): loss = 00.244906, likelihood = 0.0009\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.2836, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.10063525 0.89936477]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.1657008 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-1.1305519  1.0596342]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.37025425 0.61951804 0.         0.01022774] \n",
      "\tDiff: tensor([-0.0369, -0.4529,  0.1667,  0.3231]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0369]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.2836, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.10063525 0.89936477]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.1657008 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-1.1305519  1.0596342]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.37025425 0.61951804 0.         0.01022774] \n",
      "\tDiff: tensor([-0.0369, -0.4529,  0.1667,  0.3231]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0738]])\n",
      "\n",
      ">>> Iter: 0045 (0.026s): loss = 00.244886, likelihood = 0.0009\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.3157, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.09749213 0.9025079 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.1301252 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-1.1978585  1.0275474]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.36899891 0.6207376  0.         0.01026348] \n",
      "\tDiff: tensor([-0.0357, -0.4541,  0.1667,  0.3231]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0357]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.3157, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.09749213 0.9025079 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.1301252 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-1.1978585  1.0275474]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.36899891 0.6207376  0.         0.01026348] \n",
      "\tDiff: tensor([-0.0357, -0.4541,  0.1667,  0.3231]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0713]])\n",
      "\n",
      ">>> Iter: 0046 (0.026s): loss = 00.244868, likelihood = 0.0009\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.3461, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.09460158 0.90539837]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.0964937 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-1.2615874   0.99711347]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3678504  0.6218533  0.         0.01029635] \n",
      "\tDiff: tensor([-0.0345, -0.4552,  0.1667,  0.3230]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0345]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.3461, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.09460158 0.90539837]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.0964937 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-1.2615874   0.99711347]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3678504  0.6218533  0.         0.01029635] \n",
      "\tDiff: tensor([-0.0345, -0.4552,  0.1667,  0.3230]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0690]])\n",
      "\n",
      ">>> Iter: 0047 (0.029s): loss = 00.244852, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.3750, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.0919386  0.9080614 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.0646855 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-1.3219488   0.96824217]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3667971  0.6228762  0.         0.01032664] \n",
      "\tDiff: tensor([-0.0335, -0.4562,  0.1667,  0.3230]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0335]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.3750, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.0919386  0.9080614 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.0646855 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-1.3219488   0.96824217]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3667971  0.6228762  0.         0.01032664] \n",
      "\tDiff: tensor([-0.0335, -0.4562,  0.1667,  0.3230]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0669]])\n",
      "\n",
      ">>> Iter: 0048 (0.025s): loss = 00.244837, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.4024, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.08948104 0.91051894]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.0345875 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-1.379141   0.9408469]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.36582926 0.6238161  0.         0.01035459] \n",
      "\tDiff: tensor([-0.0325, -0.4571,  0.1667,  0.3230]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0325]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.4024, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.08948104 0.91051894]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.0345875 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-1.379141   0.9408469]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.36582926 0.6238161  0.         0.01035459] \n",
      "\tDiff: tensor([-0.0325, -0.4571,  0.1667,  0.3230]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0650]])\n",
      "\n",
      ">>> Iter: 0049 (0.027s): loss = 00.244823, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.4284, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.08720919 0.91279083]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.0060933 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-1.4333522   0.91484475]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.36493823 0.6246814  0.         0.01038042] \n",
      "\tDiff: tensor([-0.0316, -0.4580,  0.1667,  0.3230]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0316]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.4284, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.08720919 0.91279083]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.0060933 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-1.4333522   0.91484475]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.36493823 0.6246814  0.         0.01038042] \n",
      "\tDiff: tensor([-0.0316, -0.4580,  0.1667,  0.3230]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0632]])\n",
      "\n",
      ">>> Iter: 0050 (0.026s): loss = 00.244810, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.4531, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.0851055  0.9148946 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.9791032 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-1.4847604   0.89015675]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.36411616 0.6254796  0.         0.01040435] \n",
      "\tDiff: tensor([-0.0308, -0.4588,  0.1667,  0.3229]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0308]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.4531, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.0851055  0.9148946 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.9791032 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-1.4847604   0.89015675]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.36411616 0.6254796  0.         0.01040435] \n",
      "\tDiff: tensor([-0.0308, -0.4588,  0.1667,  0.3229]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0616]])\n",
      "\n",
      ">>> Iter: 0051 (0.024s): loss = 00.244798, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.4766, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.08315435 0.9168457 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.95352393 3.3779728  3.3779728  0.        ], \n",
      "\tQ: [[-1.5335327  0.8667078]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.36335632 0.6262172  0.         0.01042654] \n",
      "\tDiff: tensor([-0.0300, -0.4596,  0.1667,  0.3229]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0300]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.4766, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.08315435 0.9168457 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.95352393 3.3779728  3.3779728  0.        ], \n",
      "\tQ: [[-1.5335327  0.8667078]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.36335632 0.6262172  0.         0.01042654] \n",
      "\tDiff: tensor([-0.0300, -0.4596,  0.1667,  0.3229]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0600]])\n",
      "\n",
      ">>> Iter: 0052 (0.024s): loss = 00.244787, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.4988, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.0813418  0.9186582 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.92926776 3.3779728  3.3779728  0.        ], \n",
      "\tQ: [[-1.5798273   0.84442663]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.36265275 0.62690014 0.         0.01044715] \n",
      "\tDiff: tensor([-0.0293, -0.4602,  0.1667,  0.3229]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0293]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.4988, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.0813418  0.9186582 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.92926776 3.3779728  3.3779728  0.        ], \n",
      "\tQ: [[-1.5798273   0.84442663]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.36265275 0.62690014 0.         0.01044715] \n",
      "\tDiff: tensor([-0.0293, -0.4602,  0.1667,  0.3229]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0586]])\n",
      "\n",
      ">>> Iter: 0053 (0.024s): loss = 00.244776, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.5200, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.07965536 0.9203447 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.9062525 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-1.6237935  0.8232455]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.36200014 0.6275336  0.         0.01046633] \n",
      "\tDiff: tensor([-0.0287, -0.4609,  0.1667,  0.3229]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0287]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.5200, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.07965536 0.9203447 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.9062525 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-1.6237935  0.8232455]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.36200014 0.6275336  0.         0.01046633] \n",
      "\tDiff: tensor([-0.0287, -0.4609,  0.1667,  0.3229]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0573]])\n",
      "\n",
      ">>> Iter: 0054 (0.022s): loss = 00.244767, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.5402, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.07808381 0.9219162 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.8844013 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-1.6655713   0.80310035]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3613936  0.62812215 0.         0.0104842 ] \n",
      "\tDiff: tensor([-0.0281, -0.4615,  0.1667,  0.3228]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0281]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.5402, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.07808381 0.9219162 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.8844013 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-1.6655713   0.80310035]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3613936  0.62812215 0.         0.0104842 ] \n",
      "\tDiff: tensor([-0.0281, -0.4615,  0.1667,  0.3228]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0561]])\n",
      "\n",
      ">>> Iter: 0055 (0.024s): loss = 00.244758, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.5593, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.07661712 0.92338294]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.8636418 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-1.7052929   0.78393054]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.36082911 0.62867004 0.         0.01050088] \n",
      "\tDiff: tensor([-0.0275, -0.4620,  0.1667,  0.3228]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0275]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.5593, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.07661712 0.92338294]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.8636418 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-1.7052929   0.78393054]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.36082911 0.62867004 0.         0.01050088] \n",
      "\tDiff: tensor([-0.0275, -0.4620,  0.1667,  0.3228]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0550]])\n",
      "\n",
      ">>> Iter: 0056 (0.024s): loss = 00.244750, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.5776, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.07524625 0.9247537 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.8439067 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-1.7430824  0.7656789]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.36030272 0.6291807  0.         0.01051647] \n",
      "\tDiff: tensor([-0.0270, -0.4625,  0.1667,  0.3228]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0270]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.5776, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.07524625 0.9247537 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.8439067 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-1.7430824  0.7656789]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.36030272 0.6291807  0.         0.01051647] \n",
      "\tDiff: tensor([-0.0270, -0.4625,  0.1667,  0.3228]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0539]])\n",
      "\n",
      ">>> Iter: 0057 (0.026s): loss = 00.244742, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.5950, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.07396311 0.9260369 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.8251324 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-1.7790564   0.74829125]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35981122 0.62965775 0.         0.01053106] \n",
      "\tDiff: tensor([-0.0265, -0.4630,  0.1667,  0.3228]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0265]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.5950, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.07396311 0.9260369 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.8251324 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-1.7790564   0.74829125]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35981122 0.62965775 0.         0.01053106] \n",
      "\tDiff: tensor([-0.0265, -0.4630,  0.1667,  0.3228]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0530]])\n",
      "\n",
      ">>> Iter: 0058 (0.027s): loss = 00.244734, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.6115, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.07276032 0.92723966]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.8072601 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-1.8133243  0.7317169]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3593515  0.63010377 0.         0.01054474] \n",
      "\tDiff: tensor([-0.0260, -0.4634,  0.1667,  0.3228]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0260]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.6115, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.07276032 0.92723966]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.8072601 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-1.8133243  0.7317169]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3593515  0.63010377 0.         0.01054474] \n",
      "\tDiff: tensor([-0.0260, -0.4634,  0.1667,  0.3228]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0520]])\n",
      "\n",
      ">>> Iter: 0059 (0.024s): loss = 00.244728, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.6274, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.07163132 0.9283688 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.79023427 3.3779728  3.3779728  0.        ], \n",
      "\tQ: [[-1.8459886   0.71590805]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35892078 0.6305217  0.         0.01055758] \n",
      "\tDiff: tensor([-0.0256, -0.4639,  0.1667,  0.3228]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0256]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.6274, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.07163132 0.9283688 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.79023427 3.3779728  3.3779728  0.        ], \n",
      "\tQ: [[-1.8459886   0.71590805]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35892078 0.6305217  0.         0.01055758] \n",
      "\tDiff: tensor([-0.0256, -0.4639,  0.1667,  0.3228]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0512]])\n",
      "\n",
      ">>> Iter: 0060 (0.023s): loss = 00.244721, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.6424, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.07057006 0.9294298 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.77400345 3.3779728  3.3779728  0.        ], \n",
      "\tQ: [[-1.8771459  0.7008195]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35851672 0.63091356 0.         0.01056964] \n",
      "\tDiff: tensor([-0.0252, -0.4642,  0.1667,  0.3228]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0252]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.6424, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.07057006 0.9294298 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.77400345 3.3779728  3.3779728  0.        ], \n",
      "\tQ: [[-1.8771459  0.7008195]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35851672 0.63091356 0.         0.01056964] \n",
      "\tDiff: tensor([-0.0252, -0.4642,  0.1667,  0.3228]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0504]])\n",
      "\n",
      ">>> Iter: 0061 (0.026s): loss = 00.244715, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.6569, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.06957117 0.9304288 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.75851846 3.3779728  3.3779728  0.        ], \n",
      "\tQ: [[-1.9068866   0.68640876]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3581371  0.63128185 0.         0.01058101] \n",
      "\tDiff: tensor([-0.0248, -0.4646,  0.1667,  0.3228]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0248]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.6569, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.06957117 0.9304288 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.75851846 3.3779728  3.3779728  0.        ], \n",
      "\tQ: [[-1.9068866   0.68640876]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3581371  0.63128185 0.         0.01058101] \n",
      "\tDiff: tensor([-0.0248, -0.4646,  0.1667,  0.3228]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0496]])\n",
      "\n",
      ">>> Iter: 0062 (0.024s): loss = 00.244709, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.6706, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.06862973 0.9313703 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.7437346 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-1.935295   0.6726363]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35777995 0.63162845 0.         0.01059171] \n",
      "\tDiff: tensor([-0.0244, -0.4650,  0.1667,  0.3227]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0244]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.6706, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.06862973 0.9313703 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.7437346 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-1.935295   0.6726363]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35777995 0.63162845 0.         0.01059171] \n",
      "\tDiff: tensor([-0.0244, -0.4650,  0.1667,  0.3227]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0489]])\n",
      "\n",
      ">>> Iter: 0063 (0.024s): loss = 00.244704, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.6838, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.06774122 0.9322588 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.7296092 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-1.9624512   0.65946436]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35744333 0.63195485 0.         0.01060182] \n",
      "\tDiff: tensor([-0.0241, -0.4653,  0.1667,  0.3227]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0241]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.6838, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.06774122 0.9322588 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.7296092 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-1.9624512   0.65946436]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35744333 0.63195485 0.         0.01060182] \n",
      "\tDiff: tensor([-0.0241, -0.4653,  0.1667,  0.3227]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0482]])\n",
      "\n",
      ">>> Iter: 0064 (0.025s): loss = 00.244699, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.6964, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.06690166 0.9330983 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.7161029 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-1.9884285  0.6468582]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35712585 0.63226277 0.         0.01061136] \n",
      "\tDiff: tensor([-0.0238, -0.4656,  0.1667,  0.3227]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0238]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.6964, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.06690166 0.9330983 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.7161029 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-1.9884285  0.6468582]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35712585 0.63226277 0.         0.01061136] \n",
      "\tDiff: tensor([-0.0238, -0.4656,  0.1667,  0.3227]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0476]])\n",
      "\n",
      ">>> Iter: 0065 (0.024s): loss = 00.244694, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.7085, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.06610736 0.93389267]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.7031787 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-2.0132966   0.63478494]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35682586 0.63255376 0.         0.0106204 ] \n",
      "\tDiff: tensor([-0.0235, -0.4659,  0.1667,  0.3227]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0235]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.7085, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.06610736 0.93389267]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.7031787 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-2.0132966   0.63478494]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35682586 0.63255376 0.         0.0106204 ] \n",
      "\tDiff: tensor([-0.0235, -0.4659,  0.1667,  0.3227]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0470]])\n",
      "\n",
      ">>> Iter: 0066 (0.025s): loss = 00.244690, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.7200, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.0653549  0.9346451 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.6908019 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-2.037121   0.6232135]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35654208 0.632829   0.         0.01062895] \n",
      "\tDiff: tensor([-0.0232, -0.4662,  0.1667,  0.3227]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0232]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.7200, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.0653549  0.9346451 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.6908019 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-2.037121   0.6232135]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35654208 0.632829   0.         0.01062895] \n",
      "\tDiff: tensor([-0.0232, -0.4662,  0.1667,  0.3227]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0464]])\n",
      "\n",
      ">>> Iter: 0067 (0.024s): loss = 00.244686, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.7311, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.06464124 0.93535876]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.67894053 3.3779728  3.3779728  0.        ], \n",
      "\tQ: [[-2.059962   0.6121154]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35627326 0.6330897  0.         0.01063707] \n",
      "\tDiff: tensor([-0.0229, -0.4664,  0.1667,  0.3227]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0229]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.7311, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.06464124 0.93535876]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.67894053 3.3779728  3.3779728  0.        ], \n",
      "\tQ: [[-2.059962   0.6121154]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35627326 0.6330897  0.         0.01063707] \n",
      "\tDiff: tensor([-0.0229, -0.4664,  0.1667,  0.3227]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0459]])\n",
      "\n",
      ">>> Iter: 0068 (0.023s): loss = 00.244681, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.7418, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.06396363 0.93603635]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.6675643 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-2.0818765  0.6014633]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35601833 0.6333369  0.         0.01064477] \n",
      "\tDiff: tensor([-0.0227, -0.4667,  0.1667,  0.3227]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0227]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.7418, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.06396363 0.93603635]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.6675643 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-2.0818765  0.6014633]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35601833 0.6333369  0.         0.01064477] \n",
      "\tDiff: tensor([-0.0227, -0.4667,  0.1667,  0.3227]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0454]])\n",
      "\n",
      ">>> Iter: 0069 (0.024s): loss = 00.244678, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.7520, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.06331944 0.93668056]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.6566448 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-2.1029181  0.5912318]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35577628 0.6335716  0.         0.0106521 ] \n",
      "\tDiff: tensor([-0.0224, -0.4669,  0.1667,  0.3227]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0224]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.7520, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.06331944 0.93668056]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.6566448 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-2.1029181  0.5912318]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35577628 0.6335716  0.         0.0106521 ] \n",
      "\tDiff: tensor([-0.0224, -0.4669,  0.1667,  0.3227]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0449]])\n",
      "\n",
      ">>> Iter: 0070 (0.023s): loss = 00.244674, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.7619, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.06270637 0.9372937 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.64615613 3.3779728  3.3779728  0.        ], \n",
      "\tQ: [[-2.1231363   0.58139753]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35554615 0.63379484 0.         0.01065907] \n",
      "\tDiff: tensor([-0.0222, -0.4671,  0.1667,  0.3227]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0222]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.7619, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.06270637 0.9372937 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.64615613 3.3779728  3.3779728  0.        ], \n",
      "\tQ: [[-2.1231363   0.58139753]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35554615 0.63379484 0.         0.01065907] \n",
      "\tDiff: tensor([-0.0222, -0.4671,  0.1667,  0.3227]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0444]])\n",
      "\n",
      ">>> Iter: 0071 (0.025s): loss = 00.244670, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.7713, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.06212224 0.9378777 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.63607377 3.3779728  3.3779728  0.        ], \n",
      "\tQ: [[-2.1425774   0.57193804]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35532713 0.63400716 0.         0.01066572] \n",
      "\tDiff: tensor([-0.0220, -0.4673,  0.1667,  0.3227]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0220]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.7713, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.06212224 0.9378777 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.63607377 3.3779728  3.3779728  0.        ], \n",
      "\tQ: [[-2.1425774   0.57193804]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35532713 0.63400716 0.         0.01066572] \n",
      "\tDiff: tensor([-0.0220, -0.4673,  0.1667,  0.3227]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0440]])\n",
      "\n",
      ">>> Iter: 0072 (0.024s): loss = 00.244667, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.7804, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.06156515 0.93843484]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.6263744 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-2.161285   0.5628326]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35511848 0.63420945 0.         0.01067205] \n",
      "\tDiff: tensor([-0.0218, -0.4675,  0.1667,  0.3227]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0218]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.7804, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.06156515 0.93843484]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.6263744 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-2.161285   0.5628326]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35511848 0.63420945 0.         0.01067205] \n",
      "\tDiff: tensor([-0.0218, -0.4675,  0.1667,  0.3227]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0436]])\n",
      "\n",
      ">>> Iter: 0073 (0.025s): loss = 00.244664, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.7892, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.06103321 0.9389668 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.6170368 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-2.1793003   0.55406165]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35491943 0.6344025  0.         0.0106781 ] \n",
      "\tDiff: tensor([-0.0216, -0.4677,  0.1667,  0.3227]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0216]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.7892, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.06103321 0.9389668 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.6170368 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-2.1793003   0.55406165]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35491943 0.6344025  0.         0.0106781 ] \n",
      "\tDiff: tensor([-0.0216, -0.4677,  0.1667,  0.3227]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0432]])\n",
      "\n",
      ">>> Iter: 0074 (0.024s): loss = 00.244661, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.7977, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.06052483 0.9394751 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.6080413 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-2.1966603  0.5456073]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35472938 0.63458675 0.         0.01068388] \n",
      "\tDiff: tensor([-0.0214, -0.4679,  0.1667,  0.3226]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0214]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.7977, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.06052483 0.9394751 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.6080413 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-2.1966603  0.5456073]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35472938 0.63458675 0.         0.01068388] \n",
      "\tDiff: tensor([-0.0214, -0.4679,  0.1667,  0.3226]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0428]])\n",
      "\n",
      ">>> Iter: 0075 (0.025s): loss = 00.244658, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.8058, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.06003848 0.9399615 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.5993688 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-2.2134008   0.53745246]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3545477  0.6347629  0.         0.01068941] \n",
      "\tDiff: tensor([-0.0212, -0.4681,  0.1667,  0.3226]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0212]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.8058, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.06003848 0.9399615 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.5993688 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-2.2134008   0.53745246]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3545477  0.6347629  0.         0.01068941] \n",
      "\tDiff: tensor([-0.0212, -0.4681,  0.1667,  0.3226]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0424]])\n",
      "\n",
      ">>> Iter: 0076 (0.023s): loss = 00.244655, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.8137, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05957275 0.94042724]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.59100205 3.3779728  3.3779728  0.        ], \n",
      "\tQ: [[-2.2295551   0.52958107]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35437384 0.6349314  0.         0.01069471] \n",
      "\tDiff: tensor([-0.0210, -0.4683,  0.1667,  0.3226]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0210]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.8137, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05957275 0.94042724]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.59100205 3.3779728  3.3779728  0.        ], \n",
      "\tQ: [[-2.2295551   0.52958107]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35437384 0.6349314  0.         0.01069471] \n",
      "\tDiff: tensor([-0.0210, -0.4683,  0.1667,  0.3226]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0421]])\n",
      "\n",
      ">>> Iter: 0077 (0.022s): loss = 00.244653, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.8213, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05912631 0.94087374]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.5829247 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-2.2451544  0.5219784]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3542074  0.63509285 0.         0.01069979] \n",
      "\tDiff: tensor([-0.0209, -0.4684,  0.1667,  0.3226]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0209]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.8213, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05912631 0.94087374]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.5829247 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-2.2451544  0.5219784]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3542074  0.63509285 0.         0.01069979] \n",
      "\tDiff: tensor([-0.0209, -0.4684,  0.1667,  0.3226]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0417]])\n",
      "\n",
      ">>> Iter: 0078 (0.024s): loss = 00.244650, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.8286, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05869803 0.941302  ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.5751216 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-2.2602274  0.5146303]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35404786 0.6352475  0.         0.01070466] \n",
      "\tDiff: tensor([-0.0207, -0.4686,  0.1667,  0.3226]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0207]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.8286, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05869803 0.941302  ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.5751216 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-2.2602274  0.5146303]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35404786 0.6352475  0.         0.01070466] \n",
      "\tDiff: tensor([-0.0207, -0.4686,  0.1667,  0.3226]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0414]])\n",
      "\n",
      ">>> Iter: 0079 (0.025s): loss = 00.244648, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.8357, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05828677 0.9417132 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.5675785 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-2.2748015  0.507524 ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3538947  0.63539594 0.         0.01070933] \n",
      "\tDiff: tensor([-0.0206, -0.4687,  0.1667,  0.3226]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0206]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.8357, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05828677 0.9417132 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.5675785 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-2.2748015  0.507524 ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3538947  0.63539594 0.         0.01070933] \n",
      "\tDiff: tensor([-0.0206, -0.4687,  0.1667,  0.3226]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0411]])\n",
      "\n",
      ">>> Iter: 0080 (0.023s): loss = 00.244645, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.8426, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05789155 0.94210845]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.5602822 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-2.2889016  0.5006473]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3537477  0.6355385  0.         0.01071383] \n",
      "\tDiff: tensor([-0.0204, -0.4689,  0.1667,  0.3226]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0204]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.8426, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05789155 0.94210845]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.5602822 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-2.2889016  0.5006473]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3537477  0.6355385  0.         0.01071383] \n",
      "\tDiff: tensor([-0.0204, -0.4689,  0.1667,  0.3226]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0408]])\n",
      "\n",
      ">>> Iter: 0081 (0.024s): loss = 00.244643, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.8493, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05751142 0.9424886 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.5532202 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-2.3025515   0.49398875]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35360634 0.63567555 0.         0.01071815] \n",
      "\tDiff: tensor([-0.0203, -0.4690,  0.1667,  0.3226]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0203]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.8493, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05751142 0.9424886 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.5532202 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-2.3025515   0.49398875]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35360634 0.63567555 0.         0.01071815] \n",
      "\tDiff: tensor([-0.0203, -0.4690,  0.1667,  0.3226]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0405]])\n",
      "\n",
      ">>> Iter: 0082 (0.026s): loss = 00.244641, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.8557, grad_fn=<SelectBackward>), tensor(-0.0006, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 91.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05715212 0.9428479 ]\n",
      " [0.96545684 0.03454308]\n",
      " [0.96585125 0.03414869]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.5335217 3.364989  3.377031  0.       ], \n",
      "\tQ: [[-2.3285172e+00  4.7467139e-01]\n",
      " [ 3.3298352e+00 -5.5917120e-04]\n",
      " [ 3.3422856e+00  0.0000000e+00]\n",
      " [ 0.0000000e+00  0.0000000e+00]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35347283 0.6356709  0.         0.01085629] \n",
      "\tDiff: tensor([-0.0201, -0.4690,  0.1667,  0.3225]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000, -0.4690, -0.0201]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.8557, grad_fn=<SelectBackward>), tensor(-0.0006, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 91.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05715212 0.9428479 ]\n",
      " [0.96545684 0.03454308]\n",
      " [0.96585125 0.03414869]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.5335217 3.364989  3.377031  0.       ], \n",
      "\tQ: [[-2.3285172e+00  4.7467139e-01]\n",
      " [ 3.3298352e+00 -5.5917120e-04]\n",
      " [ 3.3422856e+00  0.0000000e+00]\n",
      " [ 0.0000000e+00  0.0000000e+00]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35347283 0.6356709  0.         0.01085629] \n",
      "\tDiff: tensor([-0.0201, -0.4690,  0.1667,  0.3225]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000, -0.9380, -0.0403]])\n",
      "\n",
      ">>> Iter: 0083 (0.024s): loss = 00.244572, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.8620, grad_fn=<SelectBackward>), tensor(-0.0894, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 91.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05753459 0.94246536]\n",
      " [0.89489174 0.10510818]\n",
      " [0.96585125 0.03414869]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [-0.6610077  2.1633484  3.377031   0.       ], \n",
      "\tQ: [[-3.5163765  -0.7202638 ]\n",
      " [ 2.052296   -0.08941675]\n",
      " [ 3.3422856   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35361493 0.61336476 0.         0.03302027] \n",
      "\tDiff: tensor([-0.0203, -0.4467,  0.1667,  0.3003]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000, -0.4467, -0.0203]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.8620, grad_fn=<SelectBackward>), tensor(-0.0894, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 91.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05753459 0.94246536]\n",
      " [0.89489174 0.10510818]\n",
      " [0.96585125 0.03414869]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [-0.6610077  2.1633484  3.377031   0.       ], \n",
      "\tQ: [[-3.5163765  -0.7202638 ]\n",
      " [ 2.052296   -0.08941675]\n",
      " [ 3.3422856   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35361493 0.61336476 0.         0.03302027] \n",
      "\tDiff: tensor([-0.0203, -0.4467,  0.1667,  0.3003]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000, -0.8934, -0.0406]])\n",
      "\n",
      ">>> Iter: 0084 (0.023s): loss = 00.233490, likelihood = 0.0032\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.8681, grad_fn=<SelectBackward>), tensor(-0.2099, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 91.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05762483 0.9423752 ]\n",
      " [0.799435   0.20056506]\n",
      " [0.96585125 0.03414869]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [-1.4259429  1.396734   3.377031   0.       ], \n",
      "\tQ: [[-4.2797446  -1.4852947 ]\n",
      " [ 1.172884   -0.20988263]\n",
      " [ 3.3422856   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35364845 0.58334905 0.         0.06300251] \n",
      "\tDiff: tensor([-0.0203, -0.4167,  0.1667,  0.2703]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000, -0.4167, -0.0203]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.8681, grad_fn=<SelectBackward>), tensor(-0.2099, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 91.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05762483 0.9423752 ]\n",
      " [0.799435   0.20056506]\n",
      " [0.96585125 0.03414869]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [-1.4259429  1.396734   3.377031   0.       ], \n",
      "\tQ: [[-4.2797446  -1.4852947 ]\n",
      " [ 1.172884   -0.20988263]\n",
      " [ 3.3422856   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35364845 0.58334905 0.         0.06300251] \n",
      "\tDiff: tensor([-0.0203, -0.4167,  0.1667,  0.2703]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000, -0.8334, -0.0406]])\n",
      "\n",
      ">>> Iter: 0085 (0.022s): loss = 00.218499, likelihood = 0.0061\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.8740, grad_fn=<SelectBackward>), tensor(-0.3507, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 91.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05759958 0.94240046]\n",
      " [0.69826794 0.3017321 ]\n",
      " [0.96585125 0.03414869]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [-1.9756074  0.8475387  3.377031   0.       ], \n",
      "\tQ: [[-4.8298473 -2.0349324]\n",
      " [ 0.4883863 -0.350677 ]\n",
      " [ 3.3422856  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3536391  0.5515768  0.         0.09478415] \n",
      "\tDiff: tensor([-0.0203, -0.3849,  0.1667,  0.2385]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000, -0.3849, -0.0203]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.8740, grad_fn=<SelectBackward>), tensor(-0.3507, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 91.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05759958 0.94240046]\n",
      " [0.69826794 0.3017321 ]\n",
      " [0.96585125 0.03414869]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [-1.9756074  0.8475387  3.377031   0.       ], \n",
      "\tQ: [[-4.8298473 -2.0349324]\n",
      " [ 0.4883863 -0.350677 ]\n",
      " [ 3.3422856  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3536391  0.5515768  0.         0.09478415] \n",
      "\tDiff: tensor([-0.0203, -0.3849,  0.1667,  0.2385]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000, -0.7698, -0.0406]])\n",
      "\n",
      ">>> Iter: 0086 (0.022s): loss = 00.202608, likelihood = 0.0092\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.8798, grad_fn=<SelectBackward>), tensor(-0.5055, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 91.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05751706 0.94248295]\n",
      " [0.60074955 0.39925048]\n",
      " [0.96585125 0.03414869]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [-2.4119668   0.41271624  3.377031    0.        ], \n",
      "\tQ: [[-5.2676406  -2.4712043 ]\n",
      " [-0.09686093 -0.50545   ]\n",
      " [ 3.3422856   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3536084  0.52096266 0.         0.12542893] \n",
      "\tDiff: tensor([-0.0203, -0.3543,  0.1667,  0.2079]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000, -0.3543, -0.0203]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.8798, grad_fn=<SelectBackward>), tensor(-0.5055, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 91.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05751706 0.94248295]\n",
      " [0.60074955 0.39925048]\n",
      " [0.96585125 0.03414869]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [-2.4119668   0.41271624  3.377031    0.        ], \n",
      "\tQ: [[-5.2676406  -2.4712043 ]\n",
      " [-0.09686093 -0.50545   ]\n",
      " [ 3.3422856   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3536084  0.52096266 0.         0.12542893] \n",
      "\tDiff: tensor([-0.0203, -0.3543,  0.1667,  0.2079]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000, -0.7086, -0.0406]])\n",
      "\n",
      ">>> Iter: 0087 (0.024s): loss = 00.187286, likelihood = 0.0121\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.8855, grad_fn=<SelectBackward>), tensor(-0.6699, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 91.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05740276 0.9425973 ]\n",
      " [0.5115327  0.48846728]\n",
      " [0.96585125 0.03414869]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [-2.7802098   0.04660518  3.377031    0.        ], \n",
      "\tQ: [[-5.6378727  -2.839326  ]\n",
      " [-0.6237385  -0.66987765]\n",
      " [ 3.3422856   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35356593 0.49295816 0.         0.15347598] \n",
      "\tDiff: tensor([-0.0202, -0.3263,  0.1667,  0.1799]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000, -0.3263, -0.0202]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.8855, grad_fn=<SelectBackward>), tensor(-0.6699, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 91.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05740276 0.9425973 ]\n",
      " [0.5115327  0.48846728]\n",
      " [0.96585125 0.03414869]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [-2.7802098   0.04660518  3.377031    0.        ], \n",
      "\tQ: [[-5.6378727  -2.839326  ]\n",
      " [-0.6237385  -0.66987765]\n",
      " [ 3.3422856   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35356593 0.49295816 0.         0.15347598] \n",
      "\tDiff: tensor([-0.0202, -0.3263,  0.1667,  0.1799]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000, -0.6526, -0.0405]])\n",
      "\n",
      ">>> Iter: 0088 (0.024s): loss = 00.173262, likelihood = 0.0148\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.8910, grad_fn=<SelectBackward>), tensor(-0.8408, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 91.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.0572696  0.94273037]\n",
      " [0.432562   0.56743807]\n",
      " [0.96585125 0.03414869]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [-3.1034508  -0.27414736  3.377031    0.        ], \n",
      "\tQ: [[-5.963436  -3.1624258]\n",
      " [-1.112177  -0.8407711]\n",
      " [ 3.3422856  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35351646 0.46816984 0.         0.17831369] \n",
      "\tDiff: tensor([-0.0202, -0.3015,  0.1667,  0.1550]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000, -0.3015, -0.0202]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.8910, grad_fn=<SelectBackward>), tensor(-0.8408, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 91.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.0572696  0.94273037]\n",
      " [0.432562   0.56743807]\n",
      " [0.96585125 0.03414869]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [-3.1034508  -0.27414736  3.377031    0.        ], \n",
      "\tQ: [[-5.963436  -3.1624258]\n",
      " [-1.112177  -0.8407711]\n",
      " [ 3.3422856  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35351646 0.46816984 0.         0.17831369] \n",
      "\tDiff: tensor([-0.0202, -0.3015,  0.1667,  0.1550]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000, -0.6030, -0.0404]])\n",
      "\n",
      ">>> Iter: 0089 (0.026s): loss = 00.160843, likelihood = 0.0172\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.8965, grad_fn=<SelectBackward>), tensor(-1.0157, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 91.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05712483 0.9428753 ]\n",
      " [0.36419475 0.6358053 ]\n",
      " [0.96585125 0.03414869]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [-3.3948474 -0.5628318  3.377031   0.       ], \n",
      "\tQ: [[-6.257364  -3.4536686]\n",
      " [-1.5728983 -1.0156947]\n",
      " [ 3.3422856  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3534627  0.44670907 0.         0.19982839] \n",
      "\tDiff: tensor([-0.0201, -0.2800,  0.1667,  0.1335]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000, -0.2800, -0.0201]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.8965, grad_fn=<SelectBackward>), tensor(-1.0157, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 91.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05712483 0.9428753 ]\n",
      " [0.36419475 0.6358053 ]\n",
      " [0.96585125 0.03414869]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [-3.3948474 -0.5628318  3.377031   0.       ], \n",
      "\tQ: [[-6.257364  -3.4536686]\n",
      " [-1.5728983 -1.0156947]\n",
      " [ 3.3422856  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3534627  0.44670907 0.         0.19982839] \n",
      "\tDiff: tensor([-0.0201, -0.2800,  0.1667,  0.1335]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000, -0.5601, -0.0403]])\n",
      "\n",
      ">>> Iter: 0090 (0.025s): loss = 00.150086, likelihood = 0.0193\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.9018, grad_fn=<SelectBackward>), tensor(-1.1928, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 91.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05697272 0.9430273 ]\n",
      " [0.3059025  0.69409746]\n",
      " [0.96585125 0.03414869]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [-3.6624935 -0.8276221  3.377031   0.       ], \n",
      "\tQ: [[-6.527676  -3.7211535]\n",
      " [-2.012111  -1.192765 ]\n",
      " [ 3.3422856  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35340622 0.42840955 0.         0.21818429] \n",
      "\tDiff: tensor([-0.0201, -0.2617,  0.1667,  0.1151]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000, -0.2617, -0.0201]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.9018, grad_fn=<SelectBackward>), tensor(-1.1928, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 91.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05697272 0.9430273 ]\n",
      " [0.3059025  0.69409746]\n",
      " [0.96585125 0.03414869]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [-3.6624935 -0.8276221  3.377031   0.       ], \n",
      "\tQ: [[-6.527676  -3.7211535]\n",
      " [-2.012111  -1.192765 ]\n",
      " [ 3.3422856  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35340622 0.42840955 0.         0.21818429] \n",
      "\tDiff: tensor([-0.0201, -0.2617,  0.1667,  0.1151]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000, -0.5235, -0.0401]])\n",
      "\n",
      ">>> Iter: 0091 (0.025s): loss = 00.140908, likelihood = 0.0211\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.9071, grad_fn=<SelectBackward>), tensor(-1.3705, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 91.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05681605 0.94318384]\n",
      " [0.25671563 0.74328434]\n",
      " [0.96585125 0.03414869]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [-3.9116685 -1.073848   3.377031   0.       ], \n",
      "\tQ: [[-6.779605  -3.9701626]\n",
      " [-2.4336343 -1.3705246]\n",
      " [ 3.3422856  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35334805 0.4129673  0.         0.2336846 ] \n",
      "\tDiff: tensor([-0.0200, -0.2463,  0.1667,  0.0996]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000, -0.2463, -0.0200]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.9071, grad_fn=<SelectBackward>), tensor(-1.3705, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 91.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05681605 0.94318384]\n",
      " [0.25671563 0.74328434]\n",
      " [0.96585125 0.03414869]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [-3.9116685 -1.073848   3.377031   0.       ], \n",
      "\tQ: [[-6.779605  -3.9701626]\n",
      " [-2.4336343 -1.3705246]\n",
      " [ 3.3422856  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35334805 0.4129673  0.         0.2336846 ] \n",
      "\tDiff: tensor([-0.0200, -0.2463,  0.1667,  0.0996]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000, -0.4926, -0.0400]])\n",
      "\n",
      ">>> Iter: 0092 (0.024s): loss = 00.133158, likelihood = 0.0226\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.9122, grad_fn=<SelectBackward>), tensor(-1.5479, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 91.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05665661 0.94334346]\n",
      " [0.21549788 0.78450215]\n",
      " [0.96585125 0.03414869]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [-4.1459794 -1.3051497  3.377031   0.       ], \n",
      "\tQ: [[-7.016726  -4.204304 ]\n",
      " [-2.839954  -1.5478556]\n",
      " [ 3.3422856  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35328886 0.4000262  0.         0.24668498] \n",
      "\tDiff: tensor([-0.0200, -0.2334,  0.1667,  0.0866]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000, -0.2334, -0.0200]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.9122, grad_fn=<SelectBackward>), tensor(-1.5479, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 91.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05665661 0.94334346]\n",
      " [0.21549788 0.78450215]\n",
      " [0.96585125 0.03414869]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [-4.1459794 -1.3051497  3.377031   0.       ], \n",
      "\tQ: [[-7.016726  -4.204304 ]\n",
      " [-2.839954  -1.5478556]\n",
      " [ 3.3422856  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35328886 0.4000262  0.         0.24668498] \n",
      "\tDiff: tensor([-0.0200, -0.2334,  0.1667,  0.0866]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000, -0.4667, -0.0399]])\n",
      "\n",
      ">>> Iter: 0093 (0.025s): loss = 00.126658, likelihood = 0.0238\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.9173, grad_fn=<SelectBackward>), tensor(-1.7239, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 91.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05649564 0.9435045 ]\n",
      " [0.18110642 0.8188936 ]\n",
      " [0.96585125 0.03414869]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [-4.3679867 -1.5241104  3.377031   0.       ], \n",
      "\tQ: [[-7.2415786 -4.426141 ]\n",
      " [-3.232781  -1.7239115]\n",
      " [ 3.3422856  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35322914 0.38922772 0.         0.25754327] \n",
      "\tDiff: tensor([-0.0199, -0.2226,  0.1667,  0.0758]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000, -0.2226, -0.0199]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.9173, grad_fn=<SelectBackward>), tensor(-1.7239, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 91.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05649564 0.9435045 ]\n",
      " [0.18110642 0.8188936 ]\n",
      " [0.96585125 0.03414869]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [-4.3679867 -1.5241104  3.377031   0.       ], \n",
      "\tQ: [[-7.2415786 -4.426141 ]\n",
      " [-3.232781  -1.7239115]\n",
      " [ 3.3422856  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35322914 0.38922772 0.         0.25754327] \n",
      "\tDiff: tensor([-0.0199, -0.2226,  0.1667,  0.0758]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000, -0.4451, -0.0398]])\n",
      "\n",
      ">>> Iter: 0094 (0.024s): loss = 00.121228, likelihood = 0.0249\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.9223, grad_fn=<SelectBackward>), tensor(-1.8981, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 91.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05633396 0.94366604]\n",
      " [0.1524775  0.84752256]\n",
      " [0.96585125 0.03414869]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [-4.579571  -1.7326267  3.377031   0.       ], \n",
      "\tQ: [[-7.4560285 -4.6375537]\n",
      " [-3.613365  -1.8980645]\n",
      " [ 3.3422856  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35316917 0.3802381  0.         0.26659277] \n",
      "\tDiff: tensor([-0.0198, -0.2136,  0.1667,  0.0667]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000, -0.2136, -0.0198]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.9223, grad_fn=<SelectBackward>), tensor(-1.8981, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 91.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05633396 0.94366604]\n",
      " [0.1524775  0.84752256]\n",
      " [0.96585125 0.03414869]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [-4.579571  -1.7326267  3.377031   0.       ], \n",
      "\tQ: [[-7.4560285 -4.6375537]\n",
      " [-3.613365  -1.8980645]\n",
      " [ 3.3422856  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35316917 0.3802381  0.         0.26659277] \n",
      "\tDiff: tensor([-0.0198, -0.2136,  0.1667,  0.0667]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000, -0.4271, -0.0397]])\n",
      "\n",
      ">>> Iter: 0095 (0.023s): loss = 00.116704, likelihood = 0.0258\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.9272, grad_fn=<SelectBackward>), tensor(-2.0699, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 91.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05617221 0.9438276 ]\n",
      " [0.12866534 0.87133473]\n",
      " [0.96585125 0.03414869]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [-4.7821546 -1.9321328  3.377031   0.       ], \n",
      "\tQ: [[-7.6614876 -4.8399663]\n",
      " [-3.9826734 -2.069862 ]\n",
      " [ 3.3422856  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3531092  0.3727608  0.         0.27412993] \n",
      "\tDiff: tensor([-0.0198, -0.2061,  0.1667,  0.0592]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000, -0.2061, -0.0198]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.9272, grad_fn=<SelectBackward>), tensor(-2.0699, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 91.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05617221 0.9438276 ]\n",
      " [0.12866534 0.87133473]\n",
      " [0.96585125 0.03414869]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [-4.7821546 -1.9321328  3.377031   0.       ], \n",
      "\tQ: [[-7.6614876 -4.8399663]\n",
      " [-3.9826734 -2.069862 ]\n",
      " [ 3.3422856  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3531092  0.3727608  0.         0.27412993] \n",
      "\tDiff: tensor([-0.0198, -0.2061,  0.1667,  0.0592]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000, -0.4122, -0.0396]])\n",
      "\n",
      ">>> Iter: 0096 (0.024s): loss = 00.112935, likelihood = 0.0265\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.9320, grad_fn=<SelectBackward>), tensor(-2.2390, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 91.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.0560108  0.9439892 ]\n",
      " [0.10885344 0.89114666]\n",
      " [0.96585125 0.03414869]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [-4.9768453 -2.123744   3.377031   0.       ], \n",
      "\tQ: [[-7.859056  -5.034486 ]\n",
      " [-4.341497  -2.2389903]\n",
      " [ 3.3422856  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35304937 0.36653975 0.         0.28041095] \n",
      "\tDiff: tensor([-0.0197, -0.1999,  0.1667,  0.0529]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000, -0.1999, -0.0197]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.9320, grad_fn=<SelectBackward>), tensor(-2.2390, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 91.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.0560108  0.9439892 ]\n",
      " [0.10885344 0.89114666]\n",
      " [0.96585125 0.03414869]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [-4.9768453 -2.123744   3.377031   0.       ], \n",
      "\tQ: [[-7.859056  -5.034486 ]\n",
      " [-4.341497  -2.2389903]\n",
      " [ 3.3422856  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35304937 0.36653975 0.         0.28041095] \n",
      "\tDiff: tensor([-0.0197, -0.1999,  0.1667,  0.0529]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000, -0.3997, -0.0394]])\n",
      "\n",
      ">>> Iter: 0097 (0.023s): loss = 00.109795, likelihood = 0.0271\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.9367, grad_fn=<SelectBackward>), tensor(-2.4052, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 91.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05585011 0.9441499 ]\n",
      " [0.09235062 0.9076494 ]\n",
      " [0.96585125 0.03414869]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [-5.1645246 -2.3083494  3.377031   0.       ], \n",
      "\tQ: [[-8.049608  -5.221995 ]\n",
      " [-4.690512  -2.4052465]\n",
      " [ 3.3422856  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3529898  0.3613579  0.         0.28565237] \n",
      "\tDiff: tensor([-0.0197, -0.1947,  0.1667,  0.0477]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000, -0.1947, -0.0197]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.9367, grad_fn=<SelectBackward>), tensor(-2.4052, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 91.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05585011 0.9441499 ]\n",
      " [0.09235062 0.9076494 ]\n",
      " [0.96585125 0.03414869]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [-5.1645246 -2.3083494  3.377031   0.       ], \n",
      "\tQ: [[-8.049608  -5.221995 ]\n",
      " [-4.690512  -2.4052465]\n",
      " [ 3.3422856  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3529898  0.3613579  0.         0.28565237] \n",
      "\tDiff: tensor([-0.0197, -0.1947,  0.1667,  0.0477]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000, -0.3894, -0.0393]])\n",
      "\n",
      ">>> Iter: 0098 (0.027s): loss = 00.107174, likelihood = 0.0276\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.9414, grad_fn=<SelectBackward>), tensor(-2.5685, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 91.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05569034 0.9443098 ]\n",
      " [0.07857933 0.92142063]\n",
      " [0.96585125 0.03414869]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [-5.3459144 -2.4866745  3.377031   0.       ], \n",
      "\tQ: [[-8.233863  -5.4032154]\n",
      " [-5.030321  -2.5685132]\n",
      " [ 3.3422856  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35293058 0.35703397 0.         0.29003552] \n",
      "\tDiff: tensor([-0.0196, -0.1904,  0.1667,  0.0433]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000, -0.1904, -0.0196]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.9414, grad_fn=<SelectBackward>), tensor(-2.5685, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 91.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05569034 0.9443098 ]\n",
      " [0.07857933 0.92142063]\n",
      " [0.96585125 0.03414869]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [-5.3459144 -2.4866745  3.377031   0.       ], \n",
      "\tQ: [[-8.233863  -5.4032154]\n",
      " [-5.030321  -2.5685132]\n",
      " [ 3.3422856  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35293058 0.35703397 0.         0.29003552] \n",
      "\tDiff: tensor([-0.0196, -0.1904,  0.1667,  0.0433]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000, -0.3807, -0.0392]])\n",
      "\n",
      ">>> Iter: 0099 (0.027s): loss = 00.104982, likelihood = 0.0281\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "maxent_loss_history, learned_policies, _, _ = MaxEntIRL(trajectory_list, lambda _: S, lambda _: T, A, phi, R_model, R_optimizer, gamma, \n",
    "                                                  verbose=True, debug=True, n_iters=100, boltzmann_temp=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,0,'Iterations')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(maxent_loss_history)\n",
    "plt.title(\"MaxEnt\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Iterations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.06, 0.94],\n",
       "       [0.08, 0.92],\n",
       "       [0.97, 0.03],\n",
       "       [0.5 , 0.5 ]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learned_policies[-1].round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BUG: Incorrect gradient specification\n",
    "\n",
    "Phew.. The syntax of back-propagating MaxEnt gradient used was wrong. Instead of computing (dL / dR)*(dR / dtheta) for each state and adding them, the code was computing dL / dR over whole state space and the sum of such jacobian was being applied repeatedly for all states.\n",
    "\n",
    "That is, desired operation was\n",
    "\n",
    "$\\frac{dL(s)}{d\\theta} = \\frac{dL(s)}{dR(s)} \\frac{dR(s)}{d\\theta} $\n",
    "\n",
    "$\\frac{dL}{d\\theta} = \\sum_s \\frac{dL(s)}{d\\theta}$\n",
    "\n",
    "\n",
    "However, the implementation was doing\n",
    "\n",
    "$\\frac{dL(s)}{d\\theta} = [\\frac{dL(s_0)}{dR(s_0)}, \\frac{dL(s_1)}{dR(s_1)}, ..., \\frac{dL(s_n)}{dR(s_n)}] * \\frac{dR(s)}{d\\theta} $\n",
    "$\\frac{dL}{d\\theta} = \\sum_s \\frac{dL(s)}{d\\theta}$\n",
    "\n",
    "The vector $[\\frac{dL(s_0)}{dR(s_0)}, \\frac{dL(s_1)}{dR(s_1)}, ..., \\frac{dL(s_n)}{dR(s_n)}]$ automatically was being summed by PyTorch because it was one to many relationship. So gradients of reward over the whole states were equal, that led to no failure in parameter update."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scratchpad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "R_model = Models.LinearRewardModel_ReLU(phi_dim, 0)\n",
    "R_model.apply(weights_init)\n",
    "R = [R_model(phi(s)).type(torch.float32)[0] for s in S] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "R[0].backward(gradient=torch.FloatTensor([-5.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_model.w.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "R_model = Models.LinearRewardModel_ReLU(phi_dim, 0)\n",
    "R_model.apply(weights_init)\n",
    "R = [R_model(phi(s)).type(torch.float32)[0] for s in S] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "R[0].backward(gradient=torch.FloatTensor([-5., -5., -5.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_model.w.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "R_model = Models.LinearRewardModel_ReLU(phi_dim, 0)\n",
    "R_model.apply(weights_init)\n",
    "R = [R_model(phi(s)).type(torch.float32)[0] for s in S] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for r in R:\n",
    "    r.backward(gradient=torch.FloatTensor([-5.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_model.w.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "R_model = Models.LinearRewardModel_ReLU(phi_dim, 0)\n",
    "R_model.apply(weights_init)\n",
    "R = [R_model(phi(s)).type(torch.float32)[0] for s in S] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for r in R:\n",
    "    r.backward(gradient=torch.FloatTensor([-1., -2, -3, -4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_model.w.weight.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "R_model = Models.LinearRewardModel_ReLU(phi_dim, 0)\n",
    "R_model.apply(weights_init)\n",
    "\n",
    "# Optimizer\n",
    "lr, weight_decay = 0.1, 1e-2\n",
    "optimizer_fn=lambda params, lr, weight_decay: optim.Adam(params, lr=lr, weight_decay=weight_decay)\n",
    "R_optimizer = optimizer_fn(R_model.parameters(), lr, weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaxEntIRL params \n",
      "-----\n",
      "\t Domains: 2, sizes: [4, 4],\n",
      "\t Action dim: 2, \n",
      "\t Feature dim: 4,\n",
      "\t Iterations: 100, \n",
      "\t Max likelihood: 0.99,\n",
      "\t VI iterations: 100, \n",
      "\t VI convergence eps: 0.001,\n",
      "\t Gamma (discount factor): 0.99,\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass didn't converge by 100.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.93902075 0.06097929]\n",
      " [0.9660942  0.03390587]\n",
      " [0.9660942  0.03390587]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [6.146894 3.384167 3.384167 0.      ], \n",
      "\tQ: [[6.0839763 3.349673 ]\n",
      " [3.349673  0.       ]\n",
      " [3.349673  0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [9.4026023e-01 5.9050620e-02 0.0000000e+00 6.8918540e-04] \n",
      "\tDiff: tensor([-0.6069,  0.1076,  0.1667,  0.3326]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[0., 0., 0., 0.]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass didn't converge by 100.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.93902075 0.06097929]\n",
      " [0.9660942  0.03390587]\n",
      " [0.9660942  0.03390587]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [6.146894 3.384167 3.384167 0.      ], \n",
      "\tQ: [[6.0839763 3.349673 ]\n",
      " [3.349673  0.       ]\n",
      " [3.349673  0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [9.4026023e-01 5.9050620e-02 0.0000000e+00 6.8918540e-04] \n",
      "\tDiff: tensor([-0.6069,  0.1076,  0.1667,  0.3326]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[0., 0., 0., 0.]])\n",
      "\n",
      ">>> Iter: 0000 (0.026s): loss = 00.303463, likelihood = 0.0000\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass didn't converge by 100.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.93902075 0.06097929]\n",
      " [0.9660942  0.03390587]\n",
      " [0.9660942  0.03390587]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [6.146894 3.384167 3.384167 0.      ], \n",
      "\tQ: [[6.0839763 3.349673 ]\n",
      " [3.349673  0.       ]\n",
      " [3.349673  0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [9.4026023e-01 5.9050620e-02 0.0000000e+00 6.8918540e-04] \n",
      "\tDiff: tensor([-0.6069,  0.1076,  0.1667,  0.3326]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[0., 0., 0., 0.]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass didn't converge by 100.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.93902075 0.06097929]\n",
      " [0.9660942  0.03390587]\n",
      " [0.9660942  0.03390587]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [6.146894 3.384167 3.384167 0.      ], \n",
      "\tQ: [[6.0839763 3.349673 ]\n",
      " [3.349673  0.       ]\n",
      " [3.349673  0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [9.4026023e-01 5.9050620e-02 0.0000000e+00 6.8918540e-04] \n",
      "\tDiff: tensor([-0.6069,  0.1076,  0.1667,  0.3326]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[0., 0., 0., 0.]])\n",
      "\n",
      ">>> Iter: 0001 (0.025s): loss = 00.303463, likelihood = 0.0000\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass didn't converge by 100.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.93902075 0.06097929]\n",
      " [0.9660942  0.03390587]\n",
      " [0.9660942  0.03390587]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [6.146894 3.384167 3.384167 0.      ], \n",
      "\tQ: [[6.0839763 3.349673 ]\n",
      " [3.349673  0.       ]\n",
      " [3.349673  0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [9.4026023e-01 5.9050620e-02 0.0000000e+00 6.8918540e-04] \n",
      "\tDiff: tensor([-0.6069,  0.1076,  0.1667,  0.3326]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[0., 0., 0., 0.]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass didn't converge by 100.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.93902075 0.06097929]\n",
      " [0.9660942  0.03390587]\n",
      " [0.9660942  0.03390587]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [6.146894 3.384167 3.384167 0.      ], \n",
      "\tQ: [[6.0839763 3.349673 ]\n",
      " [3.349673  0.       ]\n",
      " [3.349673  0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [9.4026023e-01 5.9050620e-02 0.0000000e+00 6.8918540e-04] \n",
      "\tDiff: tensor([-0.6069,  0.1076,  0.1667,  0.3326]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[0., 0., 0., 0.]])\n",
      "\n",
      ">>> Iter: 0002 (0.025s): loss = 00.303463, likelihood = 0.0000\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass didn't converge by 100.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.93902075 0.06097929]\n",
      " [0.9660942  0.03390587]\n",
      " [0.9660942  0.03390587]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [6.146894 3.384167 3.384167 0.      ], \n",
      "\tQ: [[6.0839763 3.349673 ]\n",
      " [3.349673  0.       ]\n",
      " [3.349673  0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [9.4026023e-01 5.9050620e-02 0.0000000e+00 6.8918540e-04] \n",
      "\tDiff: tensor([-0.6069,  0.1076,  0.1667,  0.3326]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[0., 0., 0., 0.]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass didn't converge by 100.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.93902075 0.06097929]\n",
      " [0.9660942  0.03390587]\n",
      " [0.9660942  0.03390587]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [6.146894 3.384167 3.384167 0.      ], \n",
      "\tQ: [[6.0839763 3.349673 ]\n",
      " [3.349673  0.       ]\n",
      " [3.349673  0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [9.4026023e-01 5.9050620e-02 0.0000000e+00 6.8918540e-04] \n",
      "\tDiff: tensor([-0.6069,  0.1076,  0.1667,  0.3326]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[0., 0., 0., 0.]])\n",
      "\n",
      ">>> Iter: 0003 (0.023s): loss = 00.303463, likelihood = 0.0000\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass didn't converge by 100.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.93902075 0.06097929]\n",
      " [0.9660942  0.03390587]\n",
      " [0.9660942  0.03390587]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [6.146894 3.384167 3.384167 0.      ], \n",
      "\tQ: [[6.0839763 3.349673 ]\n",
      " [3.349673  0.       ]\n",
      " [3.349673  0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [9.4026023e-01 5.9050620e-02 0.0000000e+00 6.8918540e-04] \n",
      "\tDiff: tensor([-0.6069,  0.1076,  0.1667,  0.3326]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[0., 0., 0., 0.]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass didn't converge by 100.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.93902075 0.06097929]\n",
      " [0.9660942  0.03390587]\n",
      " [0.9660942  0.03390587]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [6.146894 3.384167 3.384167 0.      ], \n",
      "\tQ: [[6.0839763 3.349673 ]\n",
      " [3.349673  0.       ]\n",
      " [3.349673  0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [9.4026023e-01 5.9050620e-02 0.0000000e+00 6.8918540e-04] \n",
      "\tDiff: tensor([-0.6069,  0.1076,  0.1667,  0.3326]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[0., 0., 0., 0.]])\n",
      "\n",
      ">>> Iter: 0004 (0.024s): loss = 00.303463, likelihood = 0.0000\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass didn't converge by 100.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.93902075 0.06097929]\n",
      " [0.9660942  0.03390587]\n",
      " [0.9660942  0.03390587]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [6.146894 3.384167 3.384167 0.      ], \n",
      "\tQ: [[6.0839763 3.349673 ]\n",
      " [3.349673  0.       ]\n",
      " [3.349673  0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [9.4026023e-01 5.9050620e-02 0.0000000e+00 6.8918540e-04] \n",
      "\tDiff: tensor([-0.6069,  0.1076,  0.1667,  0.3326]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[0., 0., 0., 0.]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass didn't converge by 100.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.93902075 0.06097929]\n",
      " [0.9660942  0.03390587]\n",
      " [0.9660942  0.03390587]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [6.146894 3.384167 3.384167 0.      ], \n",
      "\tQ: [[6.0839763 3.349673 ]\n",
      " [3.349673  0.       ]\n",
      " [3.349673  0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [9.4026023e-01 5.9050620e-02 0.0000000e+00 6.8918540e-04] \n",
      "\tDiff: tensor([-0.6069,  0.1076,  0.1667,  0.3326]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[0., 0., 0., 0.]])\n",
      "\n",
      ">>> Iter: 0005 (0.023s): loss = 00.303463, likelihood = 0.0000\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass didn't converge by 100.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.93902075 0.06097929]\n",
      " [0.9660942  0.03390587]\n",
      " [0.9660942  0.03390587]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [6.146894 3.384167 3.384167 0.      ], \n",
      "\tQ: [[6.0839763 3.349673 ]\n",
      " [3.349673  0.       ]\n",
      " [3.349673  0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [9.4026023e-01 5.9050620e-02 0.0000000e+00 6.8918540e-04] \n",
      "\tDiff: tensor([-0.6069,  0.1076,  0.1667,  0.3326]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[0., 0., 0., 0.]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass didn't converge by 100.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.93902075 0.06097929]\n",
      " [0.9660942  0.03390587]\n",
      " [0.9660942  0.03390587]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [6.146894 3.384167 3.384167 0.      ], \n",
      "\tQ: [[6.0839763 3.349673 ]\n",
      " [3.349673  0.       ]\n",
      " [3.349673  0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [9.4026023e-01 5.9050620e-02 0.0000000e+00 6.8918540e-04] \n",
      "\tDiff: tensor([-0.6069,  0.1076,  0.1667,  0.3326]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[0., 0., 0., 0.]])\n",
      "\n",
      ">>> Iter: 0006 (0.024s): loss = 00.303463, likelihood = 0.0000\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass didn't converge by 100.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.93902075 0.06097929]\n",
      " [0.9660942  0.03390587]\n",
      " [0.9660942  0.03390587]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [6.146894 3.384167 3.384167 0.      ], \n",
      "\tQ: [[6.0839763 3.349673 ]\n",
      " [3.349673  0.       ]\n",
      " [3.349673  0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [9.4026023e-01 5.9050620e-02 0.0000000e+00 6.8918540e-04] \n",
      "\tDiff: tensor([-0.6069,  0.1076,  0.1667,  0.3326]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[0., 0., 0., 0.]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass didn't converge by 100.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.93902075 0.06097929]\n",
      " [0.9660942  0.03390587]\n",
      " [0.9660942  0.03390587]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [6.146894 3.384167 3.384167 0.      ], \n",
      "\tQ: [[6.0839763 3.349673 ]\n",
      " [3.349673  0.       ]\n",
      " [3.349673  0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [9.4026023e-01 5.9050620e-02 0.0000000e+00 6.8918540e-04] \n",
      "\tDiff: tensor([-0.6069,  0.1076,  0.1667,  0.3326]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[0., 0., 0., 0.]])\n",
      "\n",
      ">>> Iter: 0007 (0.025s): loss = 00.303463, likelihood = 0.0000\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass didn't converge by 100.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.93902075 0.06097929]\n",
      " [0.9660942  0.03390587]\n",
      " [0.9660942  0.03390587]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [6.146894 3.384167 3.384167 0.      ], \n",
      "\tQ: [[6.0839763 3.349673 ]\n",
      " [3.349673  0.       ]\n",
      " [3.349673  0.       ]\n",
      " [0.        0.       ]]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [9.4026023e-01 5.9050620e-02 0.0000000e+00 6.8918540e-04] \n",
      "\tDiff: tensor([-0.6069,  0.1076,  0.1667,  0.3326]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[0., 0., 0., 0.]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass didn't converge by 100.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.93902075 0.06097929]\n",
      " [0.9660942  0.03390587]\n",
      " [0.9660942  0.03390587]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [6.146894 3.384167 3.384167 0.      ], \n",
      "\tQ: [[6.0839763 3.349673 ]\n",
      " [3.349673  0.       ]\n",
      " [3.349673  0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [9.4026023e-01 5.9050620e-02 0.0000000e+00 6.8918540e-04] \n",
      "\tDiff: tensor([-0.6069,  0.1076,  0.1667,  0.3326]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[0., 0., 0., 0.]])\n",
      "\n",
      ">>> Iter: 0008 (0.028s): loss = 00.303463, likelihood = 0.0000\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass didn't converge by 100.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.93902075 0.06097929]\n",
      " [0.9660942  0.03390587]\n",
      " [0.9660942  0.03390587]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [6.146894 3.384167 3.384167 0.      ], \n",
      "\tQ: [[6.0839763 3.349673 ]\n",
      " [3.349673  0.       ]\n",
      " [3.349673  0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [9.4026023e-01 5.9050620e-02 0.0000000e+00 6.8918540e-04] \n",
      "\tDiff: tensor([-0.6069,  0.1076,  0.1667,  0.3326]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[0., 0., 0., 0.]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass didn't converge by 100.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.93902075 0.06097929]\n",
      " [0.9660942  0.03390587]\n",
      " [0.9660942  0.03390587]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [6.146894 3.384167 3.384167 0.      ], \n",
      "\tQ: [[6.0839763 3.349673 ]\n",
      " [3.349673  0.       ]\n",
      " [3.349673  0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [9.4026023e-01 5.9050620e-02 0.0000000e+00 6.8918540e-04] \n",
      "\tDiff: tensor([-0.6069,  0.1076,  0.1667,  0.3326]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[0., 0., 0., 0.]])\n",
      "\n",
      ">>> Iter: 0009 (0.025s): loss = 00.303463, likelihood = 0.0000\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass didn't converge by 100.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.93902075 0.06097929]\n",
      " [0.9660942  0.03390587]\n",
      " [0.9660942  0.03390587]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [6.146894 3.384167 3.384167 0.      ], \n",
      "\tQ: [[6.0839763 3.349673 ]\n",
      " [3.349673  0.       ]\n",
      " [3.349673  0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [9.4026023e-01 5.9050620e-02 0.0000000e+00 6.8918540e-04] \n",
      "\tDiff: tensor([-0.6069,  0.1076,  0.1667,  0.3326]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[0., 0., 0., 0.]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass didn't converge by 100.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.93902075 0.06097929]\n",
      " [0.9660942  0.03390587]\n",
      " [0.9660942  0.03390587]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [6.146894 3.384167 3.384167 0.      ], \n",
      "\tQ: [[6.0839763 3.349673 ]\n",
      " [3.349673  0.       ]\n",
      " [3.349673  0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [9.4026023e-01 5.9050620e-02 0.0000000e+00 6.8918540e-04] \n",
      "\tDiff: tensor([-0.6069,  0.1076,  0.1667,  0.3326]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[0., 0., 0., 0.]])\n",
      "\n",
      ">>> Iter: 0010 (0.028s): loss = 00.303463, likelihood = 0.0000\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-0.0067, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(-0.0015, grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass didn't converge by 100.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.93366075 0.06633915]\n",
      " [0.9660942  0.03390587]\n",
      " [0.9660942  0.03390587]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [6.0558996 3.384167  3.384167  0.       ], \n",
      "\tQ: [[5.9872575 3.3429246]\n",
      " [3.349673  0.       ]\n",
      " [3.349673  0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [9.3512774e-01 6.4122424e-02 0.0000000e+00 7.4976234e-04] \n",
      "\tDiff: tensor([-0.6018,  0.1025,  0.1667,  0.3326]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.3326,  0.0000,  0.0000, -0.6018]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-0.0067, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(-0.0015, grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass didn't converge by 100.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.93366075 0.06633915]\n",
      " [0.9660942  0.03390587]\n",
      " [0.9660942  0.03390587]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [6.0558996 3.384167  3.384167  0.       ], \n",
      "\tQ: [[5.9872575 3.3429246]\n",
      " [3.349673  0.       ]\n",
      " [3.349673  0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [9.3512774e-01 6.4122424e-02 0.0000000e+00 7.4976234e-04] \n",
      "\tDiff: tensor([-0.6018,  0.1025,  0.1667,  0.3326]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.6652,  0.0000,  0.0000, -1.2036]])\n",
      "\n",
      ">>> Iter: 0011 (0.029s): loss = 00.300897, likelihood = 0.0000\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-0.0559, grad_fn=<SelectBackward>), tensor(-0.0395, grad_fn=<SelectBackward>), tensor(-0.0620, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ .. ] Backward pass converged @ 72.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.8994504  0.10054965]\n",
      " [0.9353337  0.06466623]\n",
      " [0.91719466 0.0828055 ]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [4.9128695 2.6990047 2.4292219 0.       ], \n",
      "\tQ: [[ 4.806898    2.6157658 ]\n",
      " [ 2.6321528  -0.03951149]\n",
      " [ 2.3427863  -0.06203879]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.9028205  0.09501219 0.         0.00216739] \n",
      "\tDiff: tensor([-0.5695,  0.0717,  0.1667,  0.3312]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.1667,  0.0717, -0.5695]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-0.0559, grad_fn=<SelectBackward>), tensor(-0.0395, grad_fn=<SelectBackward>), tensor(-0.0620, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ .. ] Backward pass converged @ 72.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.8994504  0.10054965]\n",
      " [0.9353337  0.06466623]\n",
      " [0.91719466 0.0828055 ]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [4.9128695 2.6990047 2.4292219 0.       ], \n",
      "\tQ: [[ 4.806898    2.6157658 ]\n",
      " [ 2.6321528  -0.03951149]\n",
      " [ 2.3427863  -0.06203879]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.9028205  0.09501219 0.         0.00216739] \n",
      "\tDiff: tensor([-0.5695,  0.0717,  0.1667,  0.3312]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.3333,  0.1433, -1.1390]])\n",
      "\n",
      ">>> Iter: 0012 (0.025s): loss = 00.284744, likelihood = 0.0001\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-0.1212, grad_fn=<SelectBackward>), tensor(-0.0002, grad_fn=<SelectBackward>), tensor(-0.0173, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 96.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.8412874  0.15871277]\n",
      " [0.96588117 0.03411887]\n",
      " [0.95314735 0.04685258]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [5.0626388 3.3777397 3.043459  0.       ], \n",
      "\tQ: [[ 4.8898168e+00  3.2219796e+00]\n",
      " [ 3.3430252e+00 -1.6505644e-04]\n",
      " [ 2.9954731e+00 -1.7290462e-02]\n",
      " [ 0.0000000e+00  0.0000000e+00]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.84968394 0.14851117 0.         0.00180503] \n",
      "\tDiff: tensor([-0.5164,  0.0182,  0.1667,  0.3315]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.1667,  0.0182, -0.5164]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-0.1212, grad_fn=<SelectBackward>), tensor(-0.0002, grad_fn=<SelectBackward>), tensor(-0.0173, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 96.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.8412874  0.15871277]\n",
      " [0.96588117 0.03411887]\n",
      " [0.95314735 0.04685258]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [5.0626388 3.3777397 3.043459  0.       ], \n",
      "\tQ: [[ 4.8898168e+00  3.2219796e+00]\n",
      " [ 3.3430252e+00 -1.6505644e-04]\n",
      " [ 2.9954731e+00 -1.7290462e-02]\n",
      " [ 0.0000000e+00  0.0000000e+00]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.84968394 0.14851117 0.         0.00180503] \n",
      "\tDiff: tensor([-0.5164,  0.0182,  0.1667,  0.3315]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.3333,  0.0363, -1.0327]])\n",
      "\n",
      ">>> Iter: 0013 (0.026s): loss = 00.258175, likelihood = 0.0000\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-0.1969, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 95.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.7830178  0.21698219]\n",
      " [0.96597147 0.03402844]\n",
      " [0.96597147 0.03402844]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [4.6770153 3.3805585 3.3805585 0.       ], \n",
      "\tQ: [[4.4324155 3.1490753]\n",
      " [3.3459375 0.       ]\n",
      " [3.3459375 0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.7987116  0.19882722 0.         0.00246119] \n",
      "\tDiff: tensor([-0.4654, -0.0322,  0.1667,  0.3309]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.4654]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-0.1969, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 95.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.7830178  0.21698219]\n",
      " [0.96597147 0.03402844]\n",
      " [0.96597147 0.03402844]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [4.6770153 3.3805585 3.3805585 0.       ], \n",
      "\tQ: [[4.4324155 3.1490753]\n",
      " [3.3459375 0.       ]\n",
      " [3.3459375 0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.7987116  0.19882722 0.         0.00246119] \n",
      "\tDiff: tensor([-0.4654, -0.0322,  0.1667,  0.3309]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.9308]])\n",
      "\n",
      ">>> Iter: 0014 (0.025s): loss = 00.248769, likelihood = 0.0001\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-0.2796, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 94.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.72321725 0.2767825 ]\n",
      " [0.9659436  0.03405649]\n",
      " [0.9659436  0.03405649]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [4.3500357 3.3797348 3.3797348 0.       ], \n",
      "\tQ: [[4.02599   3.0655124]\n",
      " [3.345085  0.       ]\n",
      " [3.345085  0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.7487535  0.24810421 0.         0.00314208] \n",
      "\tDiff: tensor([-0.4154, -0.0814,  0.1667,  0.3302]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.4154]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-0.2796, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 94.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.72321725 0.2767825 ]\n",
      " [0.9659436  0.03405649]\n",
      " [0.9659436  0.03405649]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [4.3500357 3.3797348 3.3797348 0.       ], \n",
      "\tQ: [[4.02599   3.0655124]\n",
      " [3.345085  0.       ]\n",
      " [3.345085  0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.7487535  0.24810421 0.         0.00314208] \n",
      "\tDiff: tensor([-0.4154, -0.0814,  0.1667,  0.3302]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.8308]])\n",
      "\n",
      ">>> Iter: 0015 (0.026s): loss = 00.248429, likelihood = 0.0001\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-0.3671, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Backward Pass  [ ... ] Backward pass converged @ 94.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.664496   0.33550385]\n",
      " [0.9659436  0.03405649]\n",
      " [0.9659436  0.03405649]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [4.070131  3.3797348 3.3797348 0.       ], \n",
      "\tQ: [[3.6614044 2.978009 ]\n",
      " [3.345085  0.       ]\n",
      " [3.345085  0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.702017   0.2941742  0.         0.00380869] \n",
      "\tDiff: tensor([-0.3687, -0.1275,  0.1667,  0.3295]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.3687]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-0.3671, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 94.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.664496   0.33550385]\n",
      " [0.9659436  0.03405649]\n",
      " [0.9659436  0.03405649]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [4.070131  3.3797348 3.3797348 0.       ], \n",
      "\tQ: [[3.6614044 2.978009 ]\n",
      " [3.345085  0.       ]\n",
      " [3.345085  0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.702017   0.2941742  0.         0.00380869] \n",
      "\tDiff: tensor([-0.3687, -0.1275,  0.1667,  0.3295]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.7374]])\n",
      "\n",
      ">>> Iter: 0016 (0.026s): loss = 00.248096, likelihood = 0.0001\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-0.4577, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 93.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.6084271  0.39157277]\n",
      " [0.96591413 0.03408583]\n",
      " [0.96591413 0.03408583]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [3.824117  3.3788736 3.3788736 0.       ], \n",
      "\tQ: [[3.3272388 2.886533 ]\n",
      " [3.3441932 0.       ]\n",
      " [3.3441932 0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.6595369  0.33601394 0.         0.00444903] \n",
      "\tDiff: tensor([-0.3262, -0.1693,  0.1667,  0.3289]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.3262]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-0.4577, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 93.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.6084271  0.39157277]\n",
      " [0.96591413 0.03408583]\n",
      " [0.96591413 0.03408583]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [3.824117  3.3788736 3.3788736 0.       ], \n",
      "\tQ: [[3.3272388 2.886533 ]\n",
      " [3.3441932 0.       ]\n",
      " [3.3441932 0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.6595369  0.33601394 0.         0.00444903] \n",
      "\tDiff: tensor([-0.3262, -0.1693,  0.1667,  0.3289]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.6524]])\n",
      "\n",
      ">>> Iter: 0017 (0.025s): loss = 00.247775, likelihood = 0.0002\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-0.5500, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 93.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.5559935  0.44400638]\n",
      " [0.96591413 0.03408583]\n",
      " [0.96591413 0.03408583]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [3.6061382 3.3788736 3.3788736 0.       ], \n",
      "\tQ: [[3.0191395 2.7942219]\n",
      " [3.3441932 0.       ]\n",
      " [3.3441932 0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.62170744 0.3732477  0.         0.00504478] \n",
      "\tDiff: tensor([-0.2884, -0.2066,  0.1667,  0.3283]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.2884]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-0.5500, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 93.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.5559935  0.44400638]\n",
      " [0.96591413 0.03408583]\n",
      " [0.96591413 0.03408583]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [3.6061382 3.3788736 3.3788736 0.       ], \n",
      "\tQ: [[3.0191395 2.7942219]\n",
      " [3.3441932 0.       ]\n",
      " [3.3441932 0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.62170744 0.3732477  0.         0.00504478] \n",
      "\tDiff: tensor([-0.2884, -0.2066,  0.1667,  0.3283]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.5767]])\n",
      "\n",
      ">>> Iter: 0018 (0.026s): loss = 00.247478, likelihood = 0.0002\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-0.6429, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 93.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.5076446  0.4923555 ]\n",
      " [0.96591413 0.03408583]\n",
      " [0.96591413 0.03408583]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [3.4098299 3.3788736 3.3788736 0.       ], \n",
      "\tQ: [[2.731856  2.7012756]\n",
      " [3.3441932 0.       ]\n",
      " [3.3441932 0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.58844924 0.40595675 0.         0.00559412] \n",
      "\tDiff: tensor([-0.2551, -0.2393,  0.1667,  0.3277]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.2551]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-0.6429, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 93.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.5076446  0.4923555 ]\n",
      " [0.96591413 0.03408583]\n",
      " [0.96591413 0.03408583]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [3.4098299 3.3788736 3.3788736 0.       ], \n",
      "\tQ: [[2.731856  2.7012756]\n",
      " [3.3441932 0.       ]\n",
      " [3.3441932 0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.58844924 0.40595675 0.         0.00559412] \n",
      "\tDiff: tensor([-0.2551, -0.2393,  0.1667,  0.3277]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.5102]])\n",
      "\n",
      ">>> Iter: 0019 (0.025s): loss = 00.247203, likelihood = 0.0003\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-0.7356, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 93.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.46353287 0.5364671 ]\n",
      " [0.96591413 0.03408583]\n",
      " [0.96591413 0.03408583]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [3.2313309 3.3788736 3.3788736 0.       ], \n",
      "\tQ: [[2.462453  2.6085808]\n",
      " [3.3441932 0.       ]\n",
      " [3.3441932 0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.5594652  0.43443942 0.         0.00609531] \n",
      "\tDiff: tensor([-0.2261, -0.2678,  0.1667,  0.3272]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.2261]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-0.7356, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 93.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.46353287 0.5364671 ]\n",
      " [0.96591413 0.03408583]\n",
      " [0.96591413 0.03408583]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [3.2313309 3.3788736 3.3788736 0.       ], \n",
      "\tQ: [[2.462453  2.6085808]\n",
      " [3.3441932 0.       ]\n",
      " [3.3441932 0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.5594652  0.43443942 0.         0.00609531] \n",
      "\tDiff: tensor([-0.2261, -0.2678,  0.1667,  0.3272]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.4523]])\n",
      "\n",
      ">>> Iter: 0020 (0.025s): loss = 00.246952, likelihood = 0.0003\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-0.8273, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 93.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.42360052 0.5763996 ]\n",
      " [0.96591413 0.03408583]\n",
      " [0.96591413 0.03408583]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [3.0678082 3.3788736 3.3788736 0.       ], \n",
      "\tQ: [[2.2088437 2.516854 ]\n",
      " [3.3441932 0.       ]\n",
      " [3.3441932 0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.534346   0.4591051  0.         0.00654902] \n",
      "\tDiff: tensor([-0.2010, -0.2924,  0.1667,  0.3268]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.2010]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-0.8273, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 93.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.42360052 0.5763996 ]\n",
      " [0.96591413 0.03408583]\n",
      " [0.96591413 0.03408583]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [3.0678082 3.3788736 3.3788736 0.       ], \n",
      "\tQ: [[2.2088437 2.516854 ]\n",
      " [3.3441932 0.       ]\n",
      " [3.3441932 0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.534346   0.4591051  0.         0.00654902] \n",
      "\tDiff: tensor([-0.2010, -0.2924,  0.1667,  0.3268]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.4020]])\n",
      "\n",
      ">>> Iter: 0021 (0.027s): loss = 00.246726, likelihood = 0.0004\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-0.9175, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.38764226 0.6123577 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [2.9161766 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[1.9685042 2.4257379]\n",
      " [3.3432608 0.       ]\n",
      " [3.3432608 0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.51263624 0.48039985 0.         0.00696384] \n",
      "\tDiff: tensor([-0.1793, -0.3137,  0.1667,  0.3264]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.1793]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-0.9175, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.38764226 0.6123577 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [2.9161766 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[1.9685042 2.4257379]\n",
      " [3.3432608 0.       ]\n",
      " [3.3432608 0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.51263624 0.48039985 0.         0.00696384] \n",
      "\tDiff: tensor([-0.1793, -0.3137,  0.1667,  0.3264]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.3586]])\n",
      "\n",
      ">>> Iter: 0022 (0.028s): loss = 00.246518, likelihood = 0.0004\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-1.0057, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.35541743 0.6445826 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [2.7767022 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[1.7422398 2.33755  ]\n",
      " [3.3432608 0.       ]\n",
      " [3.3432608 0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.49391302 0.49875674 0.         0.00733031] \n",
      "\tDiff: tensor([-0.1606, -0.3321,  0.1667,  0.3260]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.1606]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-1.0057, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.35541743 0.6445826 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [2.7767022 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[1.7422398 2.33755  ]\n",
      " [3.3432608 0.       ]\n",
      " [3.3432608 0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.49391302 0.49875674 0.         0.00733031] \n",
      "\tDiff: tensor([-0.1606, -0.3321,  0.1667,  0.3260]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.3212]])\n",
      "\n",
      ">>> Iter: 0023 (0.028s): loss = 00.246335, likelihood = 0.0005\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-1.0916, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.32660446 0.67339563]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [2.647131  3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[1.5281255 2.2517087]\n",
      " [3.3432608 0.       ]\n",
      " [3.3432608 0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.47775832 0.51458377 0.         0.00765798] \n",
      "\tDiff: tensor([-0.1444, -0.3479,  0.1667,  0.3257]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.1444]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-1.0916, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.32660446 0.67339563]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [2.647131  3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[1.5281255 2.2517087]\n",
      " [3.3432608 0.       ]\n",
      " [3.3432608 0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.47775832 0.51458377 0.         0.00765798] \n",
      "\tDiff: tensor([-0.1444, -0.3479,  0.1667,  0.3257]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.2888]])\n",
      "\n",
      ">>> Iter: 0024 (0.026s): loss = 00.246171, likelihood = 0.0005\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-1.1748, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.30088478 0.6991152 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [2.526417  3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[1.3253891 2.1684773]\n",
      " [3.3432608 0.       ]\n",
      " [3.3432608 0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.46380547 0.5282441  0.         0.00795047] \n",
      "\tDiff: tensor([-0.1305, -0.3616,  0.1667,  0.3254]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.1305]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-1.1748, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.30088478 0.6991152 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [2.526417  3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[1.3253891 2.1684773]\n",
      " [3.3432608 0.       ]\n",
      " [3.3432608 0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.46380547 0.5282441  0.         0.00795047] \n",
      "\tDiff: tensor([-0.1305, -0.3616,  0.1667,  0.3254]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.2609]])\n",
      "\n",
      ">>> Iter: 0025 (0.023s): loss = 00.246025, likelihood = 0.0006\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-1.2552, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.2779453  0.7220548 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [2.4136999 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[1.133369  2.0880456]\n",
      " [3.3432608 0.       ]\n",
      " [3.3432608 0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.45173296 0.54005575 0.         0.00821134] \n",
      "\tDiff: tensor([-0.1184, -0.3734,  0.1667,  0.3251]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.1184]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-1.2552, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.2779453  0.7220548 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [2.4136999 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[1.133369  2.0880456]\n",
      " [3.3432608 0.       ]\n",
      " [3.3432608 0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.45173296 0.54005575 0.         0.00821134] \n",
      "\tDiff: tensor([-0.1184, -0.3734,  0.1667,  0.3251]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.2368]])\n",
      "\n",
      ">>> Iter: 0026 (0.025s): loss = 00.245894, likelihood = 0.0006\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-1.3327, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.2574888  0.7425112 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [2.308259  3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[0.9514799 2.0105417]\n",
      " [3.3432608 0.       ]\n",
      " [3.3432608 0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.44126308 0.55029297 0.         0.00844397] \n",
      "\tDiff: tensor([-0.1079, -0.3836,  0.1667,  0.3249]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.1079]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-1.3327, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.2574888  0.7425112 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [2.308259  3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[0.9514799 2.0105417]\n",
      " [3.3432608 0.       ]\n",
      " [3.3432608 0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.44126308 0.55029297 0.         0.00844397] \n",
      "\tDiff: tensor([-0.1079, -0.3836,  0.1667,  0.3249]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.2159]])\n",
      "\n",
      ">>> Iter: 0027 (0.023s): loss = 00.245778, likelihood = 0.0006\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-1.4072, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.23923987 0.76076007]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [2.2094805 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[0.779192  1.9360433]\n",
      " [3.3432608 0.       ]\n",
      " [3.3432608 0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.43215856 0.55919    0.         0.0086515 ] \n",
      "\tDiff: tensor([-0.0988, -0.3925,  0.1667,  0.3247]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0988]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-1.4072, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.23923987 0.76076007]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [2.2094805 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[0.779192  1.9360433]\n",
      " [3.3432608 0.       ]\n",
      " [3.3432608 0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.43215856 0.55919    0.         0.0086515 ] \n",
      "\tDiff: tensor([-0.0988, -0.3925,  0.1667,  0.3247]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.1977]])\n",
      "\n",
      ">>> Iter: 0028 (0.023s): loss = 00.245674, likelihood = 0.0007\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-1.4787, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.2229476  0.7770525 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [2.116833  3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[0.6160145 1.8645856]\n",
      " [3.3432608 0.       ]\n",
      " [3.3432608 0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.42421773 0.56694555 0.         0.00883678] \n",
      "\tDiff: tensor([-0.0909, -0.4003,  0.1667,  0.3245]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0909]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-1.4787, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.2229476  0.7770525 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [2.116833  3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[0.6160145 1.8645856]\n",
      " [3.3432608 0.       ]\n",
      " [3.3432608 0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.42421773 0.56694555 0.         0.00883678] \n",
      "\tDiff: tensor([-0.0909, -0.4003,  0.1667,  0.3245]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.1818]])\n",
      "\n",
      ">>> Iter: 0029 (0.026s): loss = 00.245582, likelihood = 0.0007\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-1.5471, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.2083859  0.7916141 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [2.0298512 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[0.46148756 1.79617   ]\n",
      " [3.3432608  0.        ]\n",
      " [3.3432608  0.        ]\n",
      " [0.         0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.4172702  0.5737274  0.         0.00900238] \n",
      "\tDiff: tensor([-0.0839, -0.4071,  0.1667,  0.3243]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0839]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-1.5471, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.2083859  0.7916141 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [2.0298512 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[0.46148756 1.79617   ]\n",
      " [3.3432608  0.        ]\n",
      " [3.3432608  0.        ]\n",
      " [0.         0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.4172702  0.5737274  0.         0.00900238] \n",
      "\tDiff: tensor([-0.0839, -0.4071,  0.1667,  0.3243]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.1679]])\n",
      "\n",
      ">>> Iter: 0030 (0.023s): loss = 00.245499, likelihood = 0.0007\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-1.6125, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.19535325 0.8046467 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.948122  3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[0.31517628 1.73077   ]\n",
      " [3.3432608  0.        ]\n",
      " [3.3432608  0.        ]\n",
      " [0.         0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.41117206 0.57967734 0.         0.00915059] \n",
      "\tDiff: tensor([-0.0778, -0.4130,  0.1667,  0.3242]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0778]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-1.6125, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.19535325 0.8046467 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.948122  3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[0.31517628 1.73077   ]\n",
      " [3.3432608  0.        ]\n",
      " [3.3432608  0.        ]\n",
      " [0.         0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.41117206 0.57967734 0.         0.00915059] \n",
      "\tDiff: tensor([-0.0778, -0.4130,  0.1667,  0.3242]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.1557]])\n",
      "\n",
      ">>> Iter: 0031 (0.023s): loss = 00.245425, likelihood = 0.0008\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-1.6749, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.18367095 0.816329  ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.8712751 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[0.1766656 1.6683372]\n",
      " [3.3432608 0.       ]\n",
      " [3.3432608 0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.405802   0.5849145  0.         0.00928344] \n",
      "\tDiff: tensor([-0.0725, -0.4182,  0.1667,  0.3240]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0725]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-1.6749, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.18367095 0.816329  ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.8712751 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[0.1766656 1.6683372]\n",
      " [3.3432608 0.       ]\n",
      " [3.3432608 0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.405802   0.5849145  0.         0.00928344] \n",
      "\tDiff: tensor([-0.0725, -0.4182,  0.1667,  0.3240]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.1449]])\n",
      "\n",
      ">>> Iter: 0032 (0.025s): loss = 00.245358, likelihood = 0.0008\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-1.7345, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.17318116 0.8268188 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.7989757 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[0.04555874 1.608806  ]\n",
      " [3.3432608  0.        ]\n",
      " [3.3432608  0.        ]\n",
      " [0.         0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.40105763 0.58953965 0.         0.00940273] \n",
      "\tDiff: tensor([-0.0677, -0.4229,  0.1667,  0.3239]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0677]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-1.7345, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.17318116 0.8268188 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.7989757 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[0.04555874 1.608806  ]\n",
      " [3.3432608  0.        ]\n",
      " [3.3432608  0.        ]\n",
      " [0.         0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.40105763 0.58953965 0.         0.00940273] \n",
      "\tDiff: tensor([-0.0677, -0.4229,  0.1667,  0.3239]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.1354]])\n",
      "\n",
      ">>> Iter: 0033 (0.023s): loss = 00.245299, likelihood = 0.0008\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-1.7912, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.163745   0.8362549 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.7309191 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-0.07852574  1.5520973 ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3968525  0.5936374  0.         0.00951004] \n",
      "\tDiff: tensor([-0.0635, -0.4270,  0.1667,  0.3238]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0635]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-1.7912, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.163745   0.8362549 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.7309191 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-0.07852574  1.5520973 ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3968525  0.5936374  0.         0.00951004] \n",
      "\tDiff: tensor([-0.0635, -0.4270,  0.1667,  0.3238]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.1270]])\n",
      "\n",
      ">>> Iter: 0034 (0.026s): loss = 00.245245, likelihood = 0.0008\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-1.8451, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.1552407  0.8447593 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.6668258 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-0.19595267  1.4981222 ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.39311346 0.5972798  0.         0.00960676] \n",
      "\tDiff: tensor([-0.0598, -0.4306,  0.1667,  0.3237]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0598]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-1.8451, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.1552407  0.8447593 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.6668258 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-0.19595267  1.4981222 ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.39311346 0.5972798  0.         0.00960676] \n",
      "\tDiff: tensor([-0.0598, -0.4306,  0.1667,  0.3237]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.1196]])\n",
      "\n",
      ">>> Iter: 0035 (0.025s): loss = 00.245197, likelihood = 0.0008\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-1.8965, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.14756128 0.85243875]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.6064385 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-0.30707318  1.4467846 ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.38977852 0.6005274  0.         0.00969409] \n",
      "\tDiff: tensor([-0.0564, -0.4339,  0.1667,  0.3236]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0564]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-1.8965, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.14756128 0.85243875]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.6064385 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-0.30707318  1.4467846 ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.38977852 0.6005274  0.         0.00969409] \n",
      "\tDiff: tensor([-0.0564, -0.4339,  0.1667,  0.3236]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.1129]])\n",
      "\n",
      ">>> Iter: 0036 (0.025s): loss = 00.245153, likelihood = 0.0008\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-1.9453, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.14061305 0.85938704]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.5495192 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-0.41222426  1.3979833 ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.38679504 0.60343194 0.         0.00977311] \n",
      "\tDiff: tensor([-0.0535, -0.4368,  0.1667,  0.3236]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0535]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-1.9453, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.14061305 0.85938704]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.5495192 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-0.41222426  1.3979833 ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.38679504 0.60343194 0.         0.00977311] \n",
      "\tDiff: tensor([-0.0535, -0.4368,  0.1667,  0.3236]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.1069]])\n",
      "\n",
      ">>> Iter: 0037 (0.023s): loss = 00.245113, likelihood = 0.0009\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-1.9916, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.1343139  0.8656861 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.495847  3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-0.5117287  1.3516141]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.38411805 0.6060372  0.         0.00984474] \n",
      "\tDiff: tensor([-0.0508, -0.4394,  0.1667,  0.3235]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0508]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-1.9916, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.1343139  0.8656861 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.495847  3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-0.5117287  1.3516141]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.38411805 0.6060372  0.         0.00984474] \n",
      "\tDiff: tensor([-0.0508, -0.4394,  0.1667,  0.3235]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.1016]])\n",
      "\n",
      ">>> Iter: 0038 (0.024s): loss = 00.245078, likelihood = 0.0009\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.0357, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.12859187 0.87140816]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.4452162 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-0.6058956  1.3075714]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.38170925 0.608381   0.         0.00990981] \n",
      "\tDiff: tensor([-0.0484, -0.4417,  0.1667,  0.3234]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0484]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.0357, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.12859187 0.87140816]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.4452162 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-0.6058956  1.3075714]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.38170925 0.608381   0.         0.00990981] \n",
      "\tDiff: tensor([-0.0484, -0.4417,  0.1667,  0.3234]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0968]])\n",
      "\n",
      ">>> Iter: 0039 (0.025s): loss = 00.245045, likelihood = 0.0009\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.0775, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.12338375 0.87661624]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.3974354 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-0.69502044  1.2657495 ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.37953576 0.6104952  0.         0.00996904] \n",
      "\tDiff: tensor([-0.0462, -0.4438,  0.1667,  0.3234]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0462]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.0775, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.12338375 0.87661624]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.3974354 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-0.69502044  1.2657495 ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.37953576 0.6104952  0.         0.00996904] \n",
      "\tDiff: tensor([-0.0462, -0.4438,  0.1667,  0.3234]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0924]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Iter: 0040 (0.025s): loss = 00.245015, likelihood = 0.0009\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.1172, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.11863419 0.8813659 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.3523257 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-0.779385   1.2260432]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.37756944 0.6124076  0.         0.01002305] \n",
      "\tDiff: tensor([-0.0442, -0.4457,  0.1667,  0.3233]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0442]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.1172, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.11863419 0.8813659 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.3523257 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-0.779385   1.2260432]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.37756944 0.6124076  0.         0.01002305] \n",
      "\tDiff: tensor([-0.0442, -0.4457,  0.1667,  0.3233]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0885]])\n",
      "\n",
      ">>> Iter: 0041 (0.023s): loss = 00.244989, likelihood = 0.0009\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.1549, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.11429429 0.8857057 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.3097196 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-0.8592591  1.188349 ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.37578583 0.61414176 0.         0.01007241] \n",
      "\tDiff: tensor([-0.0425, -0.4475,  0.1667,  0.3233]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0425]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.1549, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.11429429 0.8857057 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.3097196 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-0.8592591  1.188349 ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.37578583 0.61414176 0.         0.01007241] \n",
      "\tDiff: tensor([-0.0425, -0.4475,  0.1667,  0.3233]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0849]])\n",
      "\n",
      ">>> Iter: 0042 (0.027s): loss = 00.244964, likelihood = 0.0009\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.1907, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.11032116 0.88967884]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.2694602 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-0.9348992  1.1525655]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.374164   0.6157184  0.         0.01011759] \n",
      "\tDiff: tensor([-0.0408, -0.4491,  0.1667,  0.3232]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0408]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.1907, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.11032116 0.88967884]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.2694602 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-0.9348992  1.1525655]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.374164   0.6157184  0.         0.01011759] \n",
      "\tDiff: tensor([-0.0408, -0.4491,  0.1667,  0.3232]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0817]])\n",
      "\n",
      ">>> Iter: 0043 (0.027s): loss = 00.244941, likelihood = 0.0009\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.2247, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.10667698 0.893323  ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.2314012 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-1.0065488  1.1185942]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.37268564 0.6171553  0.         0.01015903] \n",
      "\tDiff: tensor([-0.0394, -0.4505,  0.1667,  0.3232]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0394]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.2247, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.10667698 0.893323  ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.2314012 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-1.0065488  1.1185942]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.37268564 0.6171553  0.         0.01015903] \n",
      "\tDiff: tensor([-0.0394, -0.4505,  0.1667,  0.3232]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0787]])\n",
      "\n",
      ">>> Iter: 0044 (0.022s): loss = 00.244920, likelihood = 0.0009\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.2569, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.10332824 0.8966718 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.1954051 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-1.0744393  1.0863397]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.371335   0.6184679  0.         0.01019711] \n",
      "\tDiff: tensor([-0.0380, -0.4518,  0.1667,  0.3231]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0380]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.2569, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.10332824 0.8966718 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.1954051 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-1.0744393  1.0863397]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.371335   0.6184679  0.         0.01019711] \n",
      "\tDiff: tensor([-0.0380, -0.4518,  0.1667,  0.3231]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0760]])\n",
      "\n",
      ">>> Iter: 0045 (0.022s): loss = 00.244901, likelihood = 0.0009\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.2876, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.10024546 0.89975446]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.1613432 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-1.1387902  1.0557098]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3700982  0.6196696  0.         0.01023217] \n",
      "\tDiff: tensor([-0.0368, -0.4530,  0.1667,  0.3231]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0368]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.2876, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.10024546 0.89975446]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.1613432 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-1.1387902  1.0557098]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3700982  0.6196696  0.         0.01023217] \n",
      "\tDiff: tensor([-0.0368, -0.4530,  0.1667,  0.3231]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0735]])\n",
      "\n",
      ">>> Iter: 0046 (0.024s): loss = 00.244884, likelihood = 0.0009\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.3166, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.09740238 0.90259767]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.1290945 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-1.19981    1.0266161]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3689632  0.62077236 0.         0.0102645 ] \n",
      "\tDiff: tensor([-0.0356, -0.4541,  0.1667,  0.3231]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0356]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.3166, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.09740238 0.90259767]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.1290945 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-1.19981    1.0266161]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3689632  0.62077236 0.         0.0102645 ] \n",
      "\tDiff: tensor([-0.0356, -0.4541,  0.1667,  0.3231]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0713]])\n",
      "\n",
      ">>> Iter: 0047 (0.024s): loss = 00.244868, likelihood = 0.0009\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.3443, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.09477575 0.90522426]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.0985459 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-1.2576959   0.99897337]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.36791942 0.6217863  0.         0.01029438] \n",
      "\tDiff: tensor([-0.0346, -0.4551,  0.1667,  0.3230]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0346]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.3443, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.09477575 0.90522426]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.0985459 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-1.2576959   0.99897337]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.36791942 0.6217863  0.         0.01029438] \n",
      "\tDiff: tensor([-0.0346, -0.4551,  0.1667,  0.3230]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0692]])\n",
      "\n",
      ">>> Iter: 0048 (0.023s): loss = 00.244853, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.3706, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.09234488 0.9076551 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.0695914 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-1.3126335  0.9727006]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.36695752 0.6227205  0.         0.01032202] \n",
      "\tDiff: tensor([-0.0336, -0.4561,  0.1667,  0.3230]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0336]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.3706, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.09234488 0.9076551 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.0695914 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-1.3126335  0.9727006]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.36695752 0.6227205  0.         0.01032202] \n",
      "\tDiff: tensor([-0.0336, -0.4561,  0.1667,  0.3230]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0672]])\n",
      "\n",
      ">>> Iter: 0049 (0.025s): loss = 00.244839, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.3955, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.09009143 0.9099086 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.0421317 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-1.3647985  0.9477205]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.36606932 0.6235831  0.         0.01034765] \n",
      "\tDiff: tensor([-0.0327, -0.4569,  0.1667,  0.3230]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0327]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.3955, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.09009143 0.9099086 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.0421317 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-1.3647985  0.9477205]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.36606932 0.6235831  0.         0.01034765] \n",
      "\tDiff: tensor([-0.0327, -0.4569,  0.1667,  0.3230]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0655]])\n",
      "\n",
      ">>> Iter: 0050 (0.025s): loss = 00.244826, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.4193, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.08799895 0.9120011 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.0160736 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-1.414357   0.9239595]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3652476  0.624381   0.         0.01037144] \n",
      "\tDiff: tensor([-0.0319, -0.4577,  0.1667,  0.3230]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0319]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.4193, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.08799895 0.9120011 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.0160736 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-1.414357   0.9239595]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3652476  0.624381   0.         0.01037144] \n",
      "\tDiff: tensor([-0.0319, -0.4577,  0.1667,  0.3230]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0638]])\n",
      "\n",
      ">>> Iter: 0051 (0.026s): loss = 00.244814, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.4419, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.08605271 0.9139473 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.9913298 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-1.4614655  0.9013474]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3644859  0.62512046 0.         0.01039357] \n",
      "\tDiff: tensor([-0.0312, -0.4585,  0.1667,  0.3229]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0312]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.4419, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.08605271 0.9139473 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.9913298 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-1.4614655  0.9013474]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3644859  0.62512046 0.         0.01039357] \n",
      "\tDiff: tensor([-0.0312, -0.4585,  0.1667,  0.3229]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0623]])\n",
      "\n",
      ">>> Iter: 0052 (0.028s): loss = 00.244803, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.4634, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.08423966 0.91576034]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.96781856 3.3779728  3.3779728  0.        ], \n",
      "\tQ: [[-1.5062709   0.87981796]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.36377868 0.62580717 0.         0.01041419] \n",
      "\tDiff: tensor([-0.0304, -0.4591,  0.1667,  0.3229]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0304]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.4634, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.08423966 0.91576034]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.96781856 3.3779728  3.3779728  0.        ], \n",
      "\tQ: [[-1.5062709   0.87981796]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.36377868 0.62580717 0.         0.01041419] \n",
      "\tDiff: tensor([-0.0304, -0.4591,  0.1667,  0.3229]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0609]])\n",
      "\n",
      ">>> Iter: 0053 (0.029s): loss = 00.244793, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.4840, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.08254801 0.917452  ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.94546354 3.3779728  3.3779728  0.        ], \n",
      "\tQ: [[-1.5489118  0.8593085]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3631207  0.62644583 0.         0.01043343] \n",
      "\tDiff: tensor([-0.0298, -0.4598,  0.1667,  0.3229]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0298]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.4840, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.08254801 0.917452  ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.94546354 3.3779728  3.3779728  0.        ], \n",
      "\tQ: [[-1.5489118  0.8593085]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3631207  0.62644583 0.         0.01043343] \n",
      "\tDiff: tensor([-0.0298, -0.4598,  0.1667,  0.3229]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0596]])\n",
      "\n",
      ">>> Iter: 0054 (0.030s): loss = 00.244783, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.5035, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.08096716 0.91903275]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.92419285 3.3779728  3.3779728  0.        ], \n",
      "\tQ: [[-1.5895189   0.83975935]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3625076  0.6270409  0.         0.01045141] \n",
      "\tDiff: tensor([-0.0292, -0.4604,  0.1667,  0.3229]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0292]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.5035, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.08096716 0.91903275]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.92419285 3.3779728  3.3779728  0.        ], \n",
      "\tQ: [[-1.5895189   0.83975935]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3625076  0.6270409  0.         0.01045141] \n",
      "\tDiff: tensor([-0.0292, -0.4604,  0.1667,  0.3229]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0583]])\n",
      "\n",
      ">>> Iter: 0055 (0.032s): loss = 00.244774, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.5221, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.07948761 0.9205124 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.90393937 3.3779728  3.3779728  0.        ], \n",
      "\tQ: [[-1.6282146   0.82111454]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3619353  0.62759644 0.         0.01046823] \n",
      "\tDiff: tensor([-0.0286, -0.4609,  0.1667,  0.3229]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0286]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.5221, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.07948761 0.9205124 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.90393937 3.3779728  3.3779728  0.        ], \n",
      "\tQ: [[-1.6282146   0.82111454]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3619353  0.62759644 0.         0.01046823] \n",
      "\tDiff: tensor([-0.0286, -0.4609,  0.1667,  0.3229]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0572]])\n",
      "\n",
      ">>> Iter: 0056 (0.033s): loss = 00.244766, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.5399, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.07810087 0.92189914]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.8846406 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-1.6651137  0.8033211]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.36140022 0.62811583 0.         0.010484  ] \n",
      "\tDiff: tensor([-0.0281, -0.4614,  0.1667,  0.3228]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0281]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.5399, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.07810087 0.92189914]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.8846406 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-1.6651137  0.8033211]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.36140022 0.62811583 0.         0.010484  ] \n",
      "\tDiff: tensor([-0.0281, -0.4614,  0.1667,  0.3228]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0561]])\n",
      "\n",
      ">>> Iter: 0057 (0.034s): loss = 00.244758, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.5569, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.07679912 0.92320085]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.86623776 3.3779728  3.3779728  0.        ], \n",
      "\tQ: [[-1.7003243   0.78632927]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3608991  0.6286021  0.         0.01049881] \n",
      "\tDiff: tensor([-0.0276, -0.4619,  0.1667,  0.3228]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0276]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.5569, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.07679912 0.92320085]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.86623776 3.3779728  3.3779728  0.        ], \n",
      "\tQ: [[-1.7003243   0.78632927]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3608991  0.6286021  0.         0.01049881] \n",
      "\tDiff: tensor([-0.0276, -0.4619,  0.1667,  0.3228]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0551]])\n",
      "\n",
      ">>> Iter: 0058 (0.036s): loss = 00.244751, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.5732, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.07557543 0.9244245 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.84867567 3.3779728  3.3779728  0.        ], \n",
      "\tQ: [[-1.7339483  0.7700918]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.36042905 0.62905824 0.         0.01051272] \n",
      "\tDiff: tensor([-0.0271, -0.4624,  0.1667,  0.3228]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0271]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.5732, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.07557543 0.9244245 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.84867567 3.3779728  3.3779728  0.        ], \n",
      "\tQ: [[-1.7339483  0.7700918]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.36042905 0.62905824 0.         0.01051272] \n",
      "\tDiff: tensor([-0.0271, -0.4624,  0.1667,  0.3228]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0542]])\n",
      "\n",
      ">>> Iter: 0059 (0.037s): loss = 00.244744, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.5887, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.07442353 0.92557645]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.8319033 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-1.7660798   0.75456476]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35998747 0.62948674 0.         0.01052582] \n",
      "\tDiff: tensor([-0.0267, -0.4628,  0.1667,  0.3228]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0267]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.5887, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.07442353 0.92557645]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.8319033 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-1.7660798   0.75456476]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35998747 0.62948674 0.         0.01052582] \n",
      "\tDiff: tensor([-0.0267, -0.4628,  0.1667,  0.3228]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0533]])\n",
      "\n",
      ">>> Iter: 0060 (0.038s): loss = 00.244737, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.6036, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.07333767 0.9266623 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.81587255 3.3779728  3.3779728  0.        ], \n",
      "\tQ: [[-1.7968085  0.7397065]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35957202 0.6298898  0.         0.01053817] \n",
      "\tDiff: tensor([-0.0262, -0.4632,  0.1667,  0.3228]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0262]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.6036, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.07333767 0.9266623 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.81587255 3.3779728  3.3779728  0.        ], \n",
      "\tQ: [[-1.7968085  0.7397065]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35957202 0.6298898  0.         0.01053817] \n",
      "\tDiff: tensor([-0.0262, -0.4632,  0.1667,  0.3228]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0525]])\n",
      "\n",
      ">>> Iter: 0061 (0.039s): loss = 00.244731, likelihood = 0.0010\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.6178, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.07231267 0.92768735]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.80053866 3.3779728  3.3779728  0.        ], \n",
      "\tQ: [[-1.8262173  0.7254782]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3591806  0.6302696  0.         0.01054983] \n",
      "\tDiff: tensor([-0.0258, -0.4636,  0.1667,  0.3228]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0258]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.6178, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.07231267 0.92768735]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.80053866 3.3779728  3.3779728  0.        ], \n",
      "\tQ: [[-1.8262173  0.7254782]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3591806  0.6302696  0.         0.01054983] \n",
      "\tDiff: tensor([-0.0258, -0.4636,  0.1667,  0.3228]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0517]])\n",
      "\n",
      ">>> Iter: 0062 (0.040s): loss = 00.244725, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.6314, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.07134384 0.9286562 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.7858597 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-1.8543845  0.711843 ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3588113  0.63062793 0.         0.01056085] \n",
      "\tDiff: tensor([-0.0255, -0.4640,  0.1667,  0.3228]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0255]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.6314, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.07134384 0.9286562 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.7858597 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-1.8543845  0.711843 ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3588113  0.63062793 0.         0.01056085] \n",
      "\tDiff: tensor([-0.0255, -0.4640,  0.1667,  0.3228]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0510]])\n",
      "\n",
      ">>> Iter: 0063 (0.027s): loss = 00.244720, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.6445, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.07042689 0.929573  ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.77179664 3.3779728  3.3779728  0.        ], \n",
      "\tQ: [[-1.8813834  0.6987667]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3584623  0.63096637 0.         0.01057127] \n",
      "\tDiff: tensor([-0.0251, -0.4643,  0.1667,  0.3228]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0251]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.6445, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.07042689 0.929573  ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.77179664 3.3779728  3.3779728  0.        ], \n",
      "\tQ: [[-1.8813834  0.6987667]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3584623  0.63096637 0.         0.01057127] \n",
      "\tDiff: tensor([-0.0251, -0.4643,  0.1667,  0.3228]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0503]])\n",
      "\n",
      ">>> Iter: 0064 (0.026s): loss = 00.244714, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.6570, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.06955796 0.930442  ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.75831264 3.3779728  3.3779728  0.        ], \n",
      "\tQ: [[-1.9072821   0.68621707]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3581321  0.6312867  0.         0.01058115] \n",
      "\tDiff: tensor([-0.0248, -0.4646,  0.1667,  0.3228]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0248]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.6570, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.06955796 0.930442  ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.75831264 3.3779728  3.3779728  0.        ], \n",
      "\tQ: [[-1.9072821   0.68621707]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3581321  0.6312867  0.         0.01058115] \n",
      "\tDiff: tensor([-0.0248, -0.4646,  0.1667,  0.3228]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0496]])\n",
      "\n",
      ">>> Iter: 0065 (0.025s): loss = 00.244709, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.6691, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.06873351 0.9312665 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.7453736 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-1.9321449  0.6741638]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35781923 0.6315902  0.         0.01059053] \n",
      "\tDiff: tensor([-0.0245, -0.4649,  0.1667,  0.3227]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0245]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.6691, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.06873351 0.9312665 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.7453736 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-1.9321449  0.6741638]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35781923 0.6315902  0.         0.01059053] \n",
      "\tDiff: tensor([-0.0245, -0.4649,  0.1667,  0.3227]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0490]])\n",
      "\n",
      ">>> Iter: 0066 (0.024s): loss = 00.244705, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.6807, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.06795024 0.9320497 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.7329477 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-1.9560319  0.6625786]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3575225  0.6318781  0.         0.01059944] \n",
      "\tDiff: tensor([-0.0242, -0.4652,  0.1667,  0.3227]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0242]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.6807, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.06795024 0.9320497 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.7329477 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-1.9560319  0.6625786]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3575225  0.6318781  0.         0.01059944] \n",
      "\tDiff: tensor([-0.0242, -0.4652,  0.1667,  0.3227]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0484]])\n",
      "\n",
      ">>> Iter: 0067 (0.024s): loss = 00.244700, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.6918, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.06720527 0.9327947 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.721005  3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-1.9789987  0.6514349]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35724065 0.6321514  0.         0.01060791] \n",
      "\tDiff: tensor([-0.0239, -0.4655,  0.1667,  0.3227]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0239]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.6918, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.06720527 0.9327947 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.721005  3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-1.9789987  0.6514349]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35724065 0.6321514  0.         0.01060791] \n",
      "\tDiff: tensor([-0.0239, -0.4655,  0.1667,  0.3227]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0478]])\n",
      "\n",
      ">>> Iter: 0068 (0.025s): loss = 00.244696, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.7026, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.06649587 0.9335042 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.709518  3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-2.0010974  0.6407082]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35697255 0.63241154 0.         0.01061598] \n",
      "\tDiff: tensor([-0.0236, -0.4657,  0.1667,  0.3227]) \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0236]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.7026, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.06649587 0.9335042 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.709518  3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-2.0010974  0.6407082]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35697255 0.63241154 0.         0.01061598] \n",
      "\tDiff: tensor([-0.0236, -0.4657,  0.1667,  0.3227]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0473]])\n",
      "\n",
      ">>> Iter: 0069 (0.024s): loss = 00.244692, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.7129, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.06581955 0.9341805 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.6984605 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-2.0223777  0.6303749]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35671723 0.63265914 0.         0.01062367] \n",
      "\tDiff: tensor([-0.0234, -0.4660,  0.1667,  0.3227]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0234]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.7129, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.06581955 0.9341805 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.6984605 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-2.0223777  0.6303749]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35671723 0.63265914 0.         0.01062367] \n",
      "\tDiff: tensor([-0.0234, -0.4660,  0.1667,  0.3227]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0468]])\n",
      "\n",
      ">>> Iter: 0070 (0.025s): loss = 00.244688, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.7228, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.0651741  0.9348259 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.6878085 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-2.0428846   0.62041354]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35647392 0.63289505 0.         0.01063101] \n",
      "\tDiff: tensor([-0.0231, -0.4662,  0.1667,  0.3227]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0231]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.7228, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.0651741  0.9348259 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.6878085 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-2.0428846   0.62041354]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35647392 0.63289505 0.         0.01063101] \n",
      "\tDiff: tensor([-0.0231, -0.4662,  0.1667,  0.3227]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0463]])\n",
      "\n",
      ">>> Iter: 0071 (0.026s): loss = 00.244684, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.7325, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.06455742 0.93544257]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.6775391 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-2.0626612  0.6108036]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3562417  0.6331203  0.         0.01063802] \n",
      "\tDiff: tensor([-0.0229, -0.4665,  0.1667,  0.3227]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0229]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.7325, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.06455742 0.93544257]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.6775391 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-2.0626612  0.6108036]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3562417  0.6331203  0.         0.01063802] \n",
      "\tDiff: tensor([-0.0229, -0.4665,  0.1667,  0.3227]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0458]])\n",
      "\n",
      ">>> Iter: 0072 (0.023s): loss = 00.244681, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.7417, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.06396761 0.9360324 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.66763145 3.3779728  3.3779728  0.        ], \n",
      "\tQ: [[-2.081747    0.60152626]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35601982 0.6333355  0.         0.01064473] \n",
      "\tDiff: tensor([-0.0227, -0.4667,  0.1667,  0.3227]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0227]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.7417, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.06396761 0.9360324 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.66763145 3.3779728  3.3779728  0.        ], \n",
      "\tQ: [[-2.081747    0.60152626]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35601982 0.6333355  0.         0.01064473] \n",
      "\tDiff: tensor([-0.0227, -0.4667,  0.1667,  0.3227]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0454]])\n",
      "\n",
      ">>> Iter: 0073 (0.024s): loss = 00.244678, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.7507, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.06340291 0.93659705]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.65806574 3.3779728  3.3779728  0.        ], \n",
      "\tQ: [[-2.1001797  0.5925636]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35580763 0.6335412  0.         0.01065115] \n",
      "\tDiff: tensor([-0.0225, -0.4669,  0.1667,  0.3227]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0225]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.7507, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.06340291 0.93659705]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.65806574 3.3779728  3.3779728  0.        ], \n",
      "\tQ: [[-2.1001797  0.5925636]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35580763 0.6335412  0.         0.01065115] \n",
      "\tDiff: tensor([-0.0225, -0.4669,  0.1667,  0.3227]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0449]])\n",
      "\n",
      ">>> Iter: 0074 (0.024s): loss = 00.244674, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.7594, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.06286174 0.93713826]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.6488235 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-2.117994   0.583899 ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3556044  0.6337383  0.         0.01065731] \n",
      "\tDiff: tensor([-0.0223, -0.4671,  0.1667,  0.3227]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0223]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.7594, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.06286174 0.93713826]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.6488235 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-2.117994   0.583899 ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3556044  0.6337383  0.         0.01065731] \n",
      "\tDiff: tensor([-0.0223, -0.4671,  0.1667,  0.3227]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0445]])\n",
      "\n",
      ">>> Iter: 0075 (0.025s): loss = 00.244671, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.7677, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.06234259 0.9376574 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.63988733 3.3779728  3.3779728  0.        ], \n",
      "\tQ: [[-2.1352232  0.5755167]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35540974 0.63392705 0.         0.01066321] \n",
      "\tDiff: tensor([-0.0221, -0.4673,  0.1667,  0.3227]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0221]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.7677, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.06234259 0.9376574 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.63988733 3.3779728  3.3779728  0.        ], \n",
      "\tQ: [[-2.1352232  0.5755167]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35540974 0.63392705 0.         0.01066321] \n",
      "\tDiff: tensor([-0.0221, -0.4673,  0.1667,  0.3227]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0442]])\n",
      "\n",
      ">>> Iter: 0076 (0.028s): loss = 00.244668, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.7759, grad_fn=<SelectBackward>), tensor(-0.0025, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 91.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.06188332 0.9381167 ]\n",
      " [0.96404505 0.03595493]\n",
      " [0.96585125 0.03414869]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.5768941 3.3229449 3.377031  0.       ], \n",
      "\tQ: [[-2.2056105e+00  5.1301318e-01]\n",
      " [ 3.2863276e+00 -2.5440627e-03]\n",
      " [ 3.3422856e+00  0.0000000e+00]\n",
      " [ 0.0000000e+00  0.0000000e+00]]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35523763 0.6335191  0.         0.01124331] \n",
      "\tDiff: tensor([-0.0219, -0.4669,  0.1667,  0.3221]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000, -0.4669, -0.0219]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.7759, grad_fn=<SelectBackward>), tensor(-0.0025, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 91.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.06188332 0.9381167 ]\n",
      " [0.96404505 0.03595493]\n",
      " [0.96585125 0.03414869]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.5768941 3.3229449 3.377031  0.       ], \n",
      "\tQ: [[-2.2056105e+00  5.1301318e-01]\n",
      " [ 3.2863276e+00 -2.5440627e-03]\n",
      " [ 3.3422856e+00  0.0000000e+00]\n",
      " [ 0.0000000e+00  0.0000000e+00]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35523763 0.6335191  0.         0.01124331] \n",
      "\tDiff: tensor([-0.0219, -0.4669,  0.1667,  0.3221]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000, -0.9337, -0.0438]])\n",
      "\n",
      ">>> Iter: 0077 (0.030s): loss = 00.244378, likelihood = 0.0011\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.7837, grad_fn=<SelectBackward>), tensor(-0.0886, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 91.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.06216173 0.9378383 ]\n",
      " [0.89557177 0.10442826]\n",
      " [0.96585125 0.03414869]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [-0.5705797  2.170671   3.377031   0.       ], \n",
      "\tQ: [[-3.3485956  -0.6347574 ]\n",
      " [ 2.060378   -0.08858386]\n",
      " [ 3.3422856   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3553419  0.61201245 0.         0.03264561] \n",
      "\tDiff: tensor([-0.0220, -0.4453,  0.1667,  0.3007]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000, -0.4453, -0.0220]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.7837, grad_fn=<SelectBackward>), tensor(-0.0886, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 91.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.06216173 0.9378383 ]\n",
      " [0.89557177 0.10442826]\n",
      " [0.96585125 0.03414869]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [-0.5705797  2.170671   3.377031   0.       ], \n",
      "\tQ: [[-3.3485956  -0.6347574 ]\n",
      " [ 2.060378   -0.08858386]\n",
      " [ 3.3422856   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3553419  0.61201245 0.         0.03264561] \n",
      "\tDiff: tensor([-0.0220, -0.4453,  0.1667,  0.3007]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000, -0.8907, -0.0440]])\n",
      "\n",
      ">>> Iter: 0078 (0.035s): loss = 00.233677, likelihood = 0.0031\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.7914, grad_fn=<SelectBackward>), tensor(-0.2051, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 91.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.0621539  0.9378461 ]\n",
      " [0.8030724  0.19692768]\n",
      " [0.96585125 0.03414869]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [-1.3215802  1.4198062  3.377031   0.       ], \n",
      "\tQ: [[-4.099722  -1.3857496]\n",
      " [ 1.2004958 -0.2051124]\n",
      " [ 3.3422856  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35533902 0.5830984  0.         0.06156262] \n",
      "\tDiff: tensor([-0.0220, -0.4164,  0.1667,  0.2718]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000, -0.4164, -0.0220]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.7914, grad_fn=<SelectBackward>), tensor(-0.2051, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 91.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.0621539  0.9378461 ]\n",
      " [0.8030724  0.19692768]\n",
      " [0.96585125 0.03414869]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [-1.3215802  1.4198062  3.377031   0.       ], \n",
      "\tQ: [[-4.099722  -1.3857496]\n",
      " [ 1.2004958 -0.2051124]\n",
      " [ 3.3422856  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35533902 0.5830984  0.         0.06156262] \n",
      "\tDiff: tensor([-0.0220, -0.4164,  0.1667,  0.2718]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000, -0.8329, -0.0440]])\n",
      "\n",
      ">>> Iter: 0079 (0.033s): loss = 00.219219, likelihood = 0.0059\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.7988, grad_fn=<SelectBackward>), tensor(-0.3413, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 91.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.06202975 0.9379702 ]\n",
      " [0.7046176  0.29538244]\n",
      " [0.96585125 0.03414869]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [-1.8653734  0.8781659  3.377031   0.       ], \n",
      "\tQ: [[-4.6455145  -1.9294105 ]\n",
      " [ 0.52806586 -0.34131837]\n",
      " [ 3.3422856   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35529247 0.5523542  0.         0.09235331] \n",
      "\tDiff: tensor([-0.0220, -0.3857,  0.1667,  0.2410]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000, -0.3857, -0.0220]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.7988, grad_fn=<SelectBackward>), tensor(-0.3413, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 91.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.06202975 0.9379702 ]\n",
      " [0.7046176  0.29538244]\n",
      " [0.96585125 0.03414869]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [-1.8653734  0.8781659  3.377031   0.       ], \n",
      "\tQ: [[-4.6455145  -1.9294105 ]\n",
      " [ 0.52806586 -0.34131837]\n",
      " [ 3.3422856   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35529247 0.5523542  0.         0.09235331] \n",
      "\tDiff: tensor([-0.0220, -0.3857,  0.1667,  0.2410]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000, -0.7714, -0.0439]])\n",
      "\n",
      ">>> Iter: 0080 (0.028s): loss = 00.203823, likelihood = 0.0089\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.8060, grad_fn=<SelectBackward>), tensor(-0.4911, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 91.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.0618487  0.93815136]\n",
      " [0.6092064  0.39079368]\n",
      " [0.96585125 0.03414869]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [-2.2982252  0.448462   3.377031   0.       ], \n",
      "\tQ: [[-5.0812893  -2.3620691 ]\n",
      " [-0.04713618 -0.49111357]\n",
      " [ 3.3422856   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3552247  0.5225675  0.         0.12220787] \n",
      "\tDiff: tensor([-0.0219, -0.3559,  0.1667,  0.2111]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000, -0.3559, -0.0219]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.8060, grad_fn=<SelectBackward>), tensor(-0.4911, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 91.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.0618487  0.93815136]\n",
      " [0.6092064  0.39079368]\n",
      " [0.96585125 0.03414869]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [-2.2982252  0.448462   3.377031   0.       ], \n",
      "\tQ: [[-5.0812893  -2.3620691 ]\n",
      " [-0.04713618 -0.49111357]\n",
      " [ 3.3422856   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3552247  0.5225675  0.         0.12220787] \n",
      "\tDiff: tensor([-0.0219, -0.3559,  0.1667,  0.2111]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000, -0.7118, -0.0438]])\n",
      "\n",
      ">>> Iter: 0081 (0.033s): loss = 00.188896, likelihood = 0.0117\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.8131, grad_fn=<SelectBackward>), tensor(-0.6504, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 91.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.06163727 0.9383628 ]\n",
      " [0.52141094 0.47858906]\n",
      " [0.96585125 0.03414869]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [-2.6638117   0.08656186  3.377031    0.        ], \n",
      "\tQ: [[-5.4503    -2.7274303]\n",
      " [-0.5646549 -0.6503511]\n",
      " [ 3.3422856  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35514548 0.49515784 0.         0.14969672] \n",
      "\tDiff: tensor([-0.0218, -0.3285,  0.1667,  0.1836]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000, -0.3285, -0.0218]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.8131, grad_fn=<SelectBackward>), tensor(-0.6504, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 91.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.06163727 0.9383628 ]\n",
      " [0.52141094 0.47858906]\n",
      " [0.96585125 0.03414869]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [-2.6638117   0.08656186  3.377031    0.        ], \n",
      "\tQ: [[-5.4503    -2.7274303]\n",
      " [-0.5646549 -0.6503511]\n",
      " [ 3.3422856  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35514548 0.49515784 0.         0.14969672] \n",
      "\tDiff: tensor([-0.0218, -0.3285,  0.1667,  0.1836]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000, -0.6570, -0.0436]])\n",
      "\n",
      ">>> Iter: 0082 (0.031s): loss = 00.175152, likelihood = 0.0144\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.8200, grad_fn=<SelectBackward>), tensor(-0.8160, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 91.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.06140899 0.938591  ]\n",
      " [0.44323036 0.5567696 ]\n",
      " [0.96585125 0.03414869]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [-2.9847329  -0.23036554  3.377031    0.        ], \n",
      "\tQ: [[-5.774932  -3.0481083]\n",
      " [-1.0440311 -0.8159692]\n",
      " [ 3.3422856  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35506    0.470747   0.         0.17419298] \n",
      "\tDiff: tensor([-0.0217, -0.3041,  0.1667,  0.1591]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000, -0.3041, -0.0217]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.8200, grad_fn=<SelectBackward>), tensor(-0.8160, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 91.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.06140899 0.938591  ]\n",
      " [0.44323036 0.5567696 ]\n",
      " [0.96585125 0.03414869]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [-2.9847329  -0.23036554  3.377031    0.        ], \n",
      "\tQ: [[-5.774932  -3.0481083]\n",
      " [-1.0440311 -0.8159692]\n",
      " [ 3.3422856  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35506    0.470747   0.         0.17419298] \n",
      "\tDiff: tensor([-0.0217, -0.3041,  0.1667,  0.1591]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000, -0.6082, -0.0435]])\n",
      "\n",
      ">>> Iter: 0083 (0.026s): loss = 00.162904, likelihood = 0.0167\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.8268, grad_fn=<SelectBackward>), tensor(-0.9856, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 91.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.06117134 0.93882865]\n",
      " [0.37513465 0.6248653 ]\n",
      " [0.96585125 0.03414869]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [-3.2739449 -0.5154051  3.377031   0.       ], \n",
      "\tQ: [[-6.0680213  -3.3370671 ]\n",
      " [-1.4958754  -0.98562425]\n",
      " [ 3.3422856   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35497108 0.44948173 0.         0.19554715] \n",
      "\tDiff: tensor([-0.0216, -0.2828,  0.1667,  0.1378]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000, -0.2828, -0.0216]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.8268, grad_fn=<SelectBackward>), tensor(-0.9856, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 91.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.06117134 0.93882865]\n",
      " [0.37513465 0.6248653 ]\n",
      " [0.96585125 0.03414869]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [-3.2739449 -0.5154051  3.377031   0.       ], \n",
      "\tQ: [[-6.0680213  -3.3370671 ]\n",
      " [-1.4958754  -0.98562425]\n",
      " [ 3.3422856   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35497108 0.44948173 0.         0.19554715] \n",
      "\tDiff: tensor([-0.0216, -0.2828,  0.1667,  0.1378]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000, -0.5656, -0.0433]])\n",
      "\n",
      ">>> Iter: 0084 (0.025s): loss = 00.152226, likelihood = 0.0188\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.8334, grad_fn=<SelectBackward>), tensor(-1.1575, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 91.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.06092878 0.9390711 ]\n",
      " [0.31672207 0.6832779 ]\n",
      " [0.96585125 0.03414869]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [-3.539457  -0.7766436  3.377031   0.       ], \n",
      "\tQ: [[-6.337507  -3.6023211]\n",
      " [-1.9263743 -1.1574972]\n",
      " [ 3.3422856  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35488036 0.43123737 0.         0.2138822 ] \n",
      "\tDiff: tensor([-0.0215, -0.2646,  0.1667,  0.1195]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000, -0.2646, -0.0215]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.8334, grad_fn=<SelectBackward>), tensor(-1.1575, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 91.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.06092878 0.9390711 ]\n",
      " [0.31672207 0.6832779 ]\n",
      " [0.96585125 0.03414869]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [-3.539457  -0.7766436  3.377031   0.       ], \n",
      "\tQ: [[-6.337507  -3.6023211]\n",
      " [-1.9263743 -1.1574972]\n",
      " [ 3.3422856  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35488036 0.43123737 0.         0.2138822 ] \n",
      "\tDiff: tensor([-0.0215, -0.2646,  0.1667,  0.1195]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000, -0.5291, -0.0431]])\n",
      "\n",
      ">>> Iter: 0085 (0.026s): loss = 00.143059, likelihood = 0.0206\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.8399, grad_fn=<SelectBackward>), tensor(-1.3302, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 91.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.06068412 0.93931586]\n",
      " [0.26714042 0.7328596 ]\n",
      " [0.96585125 0.03414869]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [-3.7865145 -1.0193734  3.377031   0.       ], \n",
      "\tQ: [[-6.5885878 -3.849118 ]\n",
      " [-2.3393543 -1.3301746]\n",
      " [ 3.3422856  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3547889  0.41574886 0.         0.22946222] \n",
      "\tDiff: tensor([-0.0215, -0.2491,  0.1667,  0.1039]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000, -0.2491, -0.0215]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.8399, grad_fn=<SelectBackward>), tensor(-1.3302, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 91.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.06068412 0.93931586]\n",
      " [0.26714042 0.7328596 ]\n",
      " [0.96585125 0.03414869]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [-3.7865145 -1.0193734  3.377031   0.       ], \n",
      "\tQ: [[-6.5885878 -3.849118 ]\n",
      " [-2.3393543 -1.3301746]\n",
      " [ 3.3422856  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3547889  0.41574886 0.         0.22946222] \n",
      "\tDiff: tensor([-0.0215, -0.2491,  0.1667,  0.1039]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000, -0.4982, -0.0429]])\n",
      "\n",
      ">>> Iter: 0086 (0.025s): loss = 00.135269, likelihood = 0.0221\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.8463, grad_fn=<SelectBackward>), tensor(-1.5026, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 91.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.06043913 0.9395609 ]\n",
      " [0.22535162 0.7746485 ]\n",
      " [0.96585125 0.03414869]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [-4.0187106 -1.2472197  3.377031   0.       ], \n",
      "\tQ: [[-6.824829  -4.0810533]\n",
      " [-2.737313  -1.5025656]\n",
      " [ 3.3422856  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35469735 0.4026929  0.         0.2426098 ] \n",
      "\tDiff: tensor([-0.0214, -0.2360,  0.1667,  0.0907]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000, -0.2360, -0.0214]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.8463, grad_fn=<SelectBackward>), tensor(-1.5026, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 91.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.06043913 0.9395609 ]\n",
      " [0.22535162 0.7746485 ]\n",
      " [0.96585125 0.03414869]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [-4.0187106 -1.2472197  3.377031   0.       ], \n",
      "\tQ: [[-6.824829  -4.0810533]\n",
      " [-2.737313  -1.5025656]\n",
      " [ 3.3422856  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35469735 0.4026929  0.         0.2426098 ] \n",
      "\tDiff: tensor([-0.0214, -0.2360,  0.1667,  0.0907]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000, -0.4721, -0.0427]])\n",
      "\n",
      ">>> Iter: 0087 (0.023s): loss = 00.128695, likelihood = 0.0234\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.8526, grad_fn=<SelectBackward>), tensor(-1.6738, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 91.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.06019498 0.9398051 ]\n",
      " [0.19028892 0.80971104]\n",
      " [0.96585125 0.03414869]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [-4.238603  -1.4627615  3.377031   0.       ], \n",
      "\tQ: [[-7.0487695 -4.300686 ]\n",
      " [-3.1219733 -1.6738393]\n",
      " [ 3.3422856  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35460615 0.39173707 0.         0.25365683] \n",
      "\tDiff: tensor([-0.0213, -0.2251,  0.1667,  0.0797]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000, -0.2251, -0.0213]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.8526, grad_fn=<SelectBackward>), tensor(-1.6738, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 91.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.06019498 0.9398051 ]\n",
      " [0.19028892 0.80971104]\n",
      " [0.96585125 0.03414869]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [-4.238603  -1.4627615  3.377031   0.       ], \n",
      "\tQ: [[-7.0487695 -4.300686 ]\n",
      " [-3.1219733 -1.6738393]\n",
      " [ 3.3422856  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35460615 0.39173707 0.         0.25365683] \n",
      "\tDiff: tensor([-0.0213, -0.2251,  0.1667,  0.0797]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000, -0.4501, -0.0425]])\n",
      "\n",
      ">>> Iter: 0088 (0.024s): loss = 00.123172, likelihood = 0.0244\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.8587, grad_fn=<SelectBackward>), tensor(-1.8434, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 91.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05995249 0.94004756]\n",
      " [0.16094461 0.83905536]\n",
      " [0.96585125 0.03414869]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [-4.4480753 -1.6678953  3.377031   0.       ], \n",
      "\tQ: [[-7.262278  -4.5099   ]\n",
      " [-3.4945903 -1.8433739]\n",
      " [ 3.3422856  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35451558 0.38256714 0.         0.2629173 ] \n",
      "\tDiff: tensor([-0.0212, -0.2159,  0.1667,  0.0704]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000, -0.2159, -0.0212]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.8587, grad_fn=<SelectBackward>), tensor(-1.8434, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 91.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05995249 0.94004756]\n",
      " [0.16094461 0.83905536]\n",
      " [0.96585125 0.03414869]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [-4.4480753 -1.6678953  3.377031   0.       ], \n",
      "\tQ: [[-7.262278  -4.5099   ]\n",
      " [-3.4945903 -1.8433739]\n",
      " [ 3.3422856  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35451558 0.38256714 0.         0.2629173 ] \n",
      "\tDiff: tensor([-0.0212, -0.2159,  0.1667,  0.0704]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000, -0.4318, -0.0424]])\n",
      "\n",
      ">>> Iter: 0089 (0.028s): loss = 00.118541, likelihood = 0.0253\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.8647, grad_fn=<SelectBackward>), tensor(-2.0107, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 91.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05971218 0.9402879 ]\n",
      " [0.13641196 0.863588  ]\n",
      " [0.96585125 0.03414869]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [-4.648552 -1.864057  3.377031  0.      ], \n",
      "\tQ: [[-7.466771  -4.710121 ]\n",
      " [-3.8561327 -2.0107164]\n",
      " [ 3.3422856  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3544259  0.37490034 0.         0.27067378] \n",
      "\tDiff: tensor([-0.0211, -0.2082,  0.1667,  0.0627]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000, -0.2082, -0.0211]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.8647, grad_fn=<SelectBackward>), tensor(-2.0107, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 91.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05971218 0.9402879 ]\n",
      " [0.13641196 0.863588  ]\n",
      " [0.96585125 0.03414869]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [-4.648552 -1.864057  3.377031  0.      ], \n",
      "\tQ: [[-7.466771  -4.710121 ]\n",
      " [-3.8561327 -2.0107164]\n",
      " [ 3.3422856  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3544259  0.37490034 0.         0.27067378] \n",
      "\tDiff: tensor([-0.0211, -0.2082,  0.1667,  0.0627]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000, -0.4165, -0.0422]])\n",
      "\n",
      ">>> Iter: 0090 (0.029s): loss = 00.114663, likelihood = 0.0261\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.8706, grad_fn=<SelectBackward>), tensor(-2.1755, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 91.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.0594744  0.94052553]\n",
      " [0.11590026 0.8840997 ]\n",
      " [0.96585125 0.03414869]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [-4.841144  -2.0523636  3.377031   0.       ], \n",
      "\tQ: [[-7.6633534 -4.9024606]\n",
      " [-4.207389  -2.175549 ]\n",
      " [ 3.3422856  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3543372  0.36848998 0.         0.27717277] \n",
      "\tDiff: tensor([-0.0210, -0.2018,  0.1667,  0.0562]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000, -0.2018, -0.0210]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.8706, grad_fn=<SelectBackward>), tensor(-2.1755, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Backward Pass  [ ... ] Backward pass converged @ 91.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.0594744  0.94052553]\n",
      " [0.11590026 0.8840997 ]\n",
      " [0.96585125 0.03414869]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [-4.841144  -2.0523636  3.377031   0.       ], \n",
      "\tQ: [[-7.6633534 -4.9024606]\n",
      " [-4.207389  -2.175549 ]\n",
      " [ 3.3422856  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3543372  0.36848998 0.         0.27717277] \n",
      "\tDiff: tensor([-0.0210, -0.2018,  0.1667,  0.0562]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000, -0.4036, -0.0420]])\n",
      "\n",
      ">>> Iter: 0091 (0.033s): loss = 00.111414, likelihood = 0.0267\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.8764, grad_fn=<SelectBackward>), tensor(-2.3377, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 91.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05923942 0.94076055]\n",
      " [0.09873421 0.9012658 ]\n",
      " [0.96585125 0.03414869]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [-5.0267377 -2.233706   3.377031   0.       ], \n",
      "\tQ: [[-7.8529058 -5.0878043]\n",
      " [-4.54903   -2.337661 ]\n",
      " [ 3.3422856  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35424957 0.36312532 0.         0.2826251 ] \n",
      "\tDiff: tensor([-0.0209, -0.1965,  0.1667,  0.0507]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000, -0.1965, -0.0209]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.8764, grad_fn=<SelectBackward>), tensor(-2.3377, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 91.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05923942 0.94076055]\n",
      " [0.09873421 0.9012658 ]\n",
      " [0.96585125 0.03414869]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [-5.0267377 -2.233706   3.377031   0.       ], \n",
      "\tQ: [[-7.8529058 -5.0878043]\n",
      " [-4.54903   -2.337661 ]\n",
      " [ 3.3422856  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35424957 0.36312532 0.         0.2826251 ] \n",
      "\tDiff: tensor([-0.0209, -0.1965,  0.1667,  0.0507]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000, -0.3929, -0.0418]])\n",
      "\n",
      ">>> Iter: 0092 (0.030s): loss = 00.108687, likelihood = 0.0272\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.8822, grad_fn=<SelectBackward>), tensor(-2.4969, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 91.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05900743 0.94099253]\n",
      " [0.08434509 0.91565496]\n",
      " [0.96585125 0.03414869]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [-5.206055  -2.4088109  3.377031   0.       ], \n",
      "\tQ: [[-8.036147  -5.2668753]\n",
      " [-4.8816495 -2.4969265]\n",
      " [ 3.3422856  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35416308 0.35862875 0.         0.28720817] \n",
      "\tDiff: tensor([-0.0208, -0.1920,  0.1667,  0.0461]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000, -0.1920, -0.0208]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.8822, grad_fn=<SelectBackward>), tensor(-2.4969, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 91.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05900743 0.94099253]\n",
      " [0.08434509 0.91565496]\n",
      " [0.96585125 0.03414869]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [-5.206055  -2.4088109  3.377031   0.       ], \n",
      "\tQ: [[-8.036147  -5.2668753]\n",
      " [-4.8816495 -2.4969265]\n",
      " [ 3.3422856  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35416308 0.35862875 0.         0.28720817] \n",
      "\tDiff: tensor([-0.0208, -0.1920,  0.1667,  0.0461]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000, -0.3839, -0.0417]])\n",
      "\n",
      ">>> Iter: 0093 (0.029s): loss = 00.106396, likelihood = 0.0277\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.8878, grad_fn=<SelectBackward>), tensor(-2.6533, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 91.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05877856 0.94122136]\n",
      " [0.07225871 0.9277412 ]\n",
      " [0.96585125 0.03414869]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [-5.3796983 -2.5782828  3.377031   0.       ], \n",
      "\tQ: [[-8.213676  -5.440275 ]\n",
      " [-5.2057853 -2.6532853]\n",
      " [ 3.3422856  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35407782 0.35485217 0.         0.29106995] \n",
      "\tDiff: tensor([-0.0207, -0.1882,  0.1667,  0.0423]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000, -0.1882, -0.0207]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.8878, grad_fn=<SelectBackward>), tensor(-2.6533, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 91.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05877856 0.94122136]\n",
      " [0.07225871 0.9277412 ]\n",
      " [0.96585125 0.03414869]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [-5.3796983 -2.5782828  3.377031   0.       ], \n",
      "\tQ: [[-8.213676  -5.440275 ]\n",
      " [-5.2057853 -2.6532853]\n",
      " [ 3.3422856  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35407782 0.35485217 0.         0.29106995] \n",
      "\tDiff: tensor([-0.0207, -0.1882,  0.1667,  0.0423]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000, -0.3764, -0.0415]])\n",
      "\n",
      ">>> Iter: 0094 (0.029s): loss = 00.104465, likelihood = 0.0281\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.8933, grad_fn=<SelectBackward>), tensor(-2.8067, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 91.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05855286 0.94144714]\n",
      " [0.06208183 0.9379181 ]\n",
      " [0.96585125 0.03414869]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [-5.548179  -2.7426355  3.377031   0.       ], \n",
      "\tQ: [[-8.386004  -5.608516 ]\n",
      " [-5.5219374 -2.8067281]\n",
      " [ 3.3422856  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35399377 0.35167277 0.         0.29433343] \n",
      "\tDiff: tensor([-0.0207, -0.1850,  0.1667,  0.0390]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000, -0.1850, -0.0207]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.8933, grad_fn=<SelectBackward>), tensor(-2.8067, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 91.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05855286 0.94144714]\n",
      " [0.06208183 0.9379181 ]\n",
      " [0.96585125 0.03414869]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [-5.548179  -2.7426355  3.377031   0.       ], \n",
      "\tQ: [[-8.386004  -5.608516 ]\n",
      " [-5.5219374 -2.8067281]\n",
      " [ 3.3422856  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35399377 0.35167277 0.         0.29433343] \n",
      "\tDiff: tensor([-0.0207, -0.1850,  0.1667,  0.0390]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000, -0.3700, -0.0413]])\n",
      "\n",
      ">>> Iter: 0095 (0.033s): loss = 00.102833, likelihood = 0.0284\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.8988, grad_fn=<SelectBackward>), tensor(-2.9573, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 91.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.0583304  0.9416694 ]\n",
      " [0.05348996 0.94651014]\n",
      " [0.96585125 0.03414869]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [-5.711938  -2.9023108  3.377031   0.       ], \n",
      "\tQ: [[-8.55357   -5.772039 ]\n",
      " [-5.830572  -2.9572845]\n",
      " [ 3.3422856  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35391095 0.3489891  0.         0.2970999 ] \n",
      "\tDiff: tensor([-0.0206, -0.1823,  0.1667,  0.0362]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000, -0.1823, -0.0206]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.8988, grad_fn=<SelectBackward>), tensor(-2.9573, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 91.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.0583304  0.9416694 ]\n",
      " [0.05348996 0.94651014]\n",
      " [0.96585125 0.03414869]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [-5.711938  -2.9023108  3.377031   0.       ], \n",
      "\tQ: [[-8.55357   -5.772039 ]\n",
      " [-5.830572  -2.9572845]\n",
      " [ 3.3422856  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35391095 0.3489891  0.         0.2970999 ] \n",
      "\tDiff: tensor([-0.0206, -0.1823,  0.1667,  0.0362]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000, -0.3646, -0.0412]])\n",
      "\n",
      ">>> Iter: 0096 (0.028s): loss = 00.101450, likelihood = 0.0287\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.9041, grad_fn=<SelectBackward>), tensor(-3.1050, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 91.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05811125 0.941889  ]\n",
      " [0.04621571 0.9537843 ]\n",
      " [0.96585125 0.03414869]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [-5.87136   -3.0576947  3.377031   0.       ], \n",
      "\tQ: [[-8.716756  -5.9312277]\n",
      " [-6.13213   -3.1050124]\n",
      " [ 3.3422856  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35382938 0.3467178  0.         0.299453  ] \n",
      "\tDiff: tensor([-0.0205, -0.1801,  0.1667,  0.0339]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000, -0.1801, -0.0205]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.9041, grad_fn=<SelectBackward>), tensor(-3.1050, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 91.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05811125 0.941889  ]\n",
      " [0.04621571 0.9537843 ]\n",
      " [0.96585125 0.03414869]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [-5.87136   -3.0576947  3.377031   0.       ], \n",
      "\tQ: [[-8.716756  -5.9312277]\n",
      " [-6.13213   -3.1050124]\n",
      " [ 3.3422856  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35382938 0.3467178  0.         0.299453  ] \n",
      "\tDiff: tensor([-0.0205, -0.1801,  0.1667,  0.0339]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000, -0.3601, -0.0410]])\n",
      "\n",
      ">>> Iter: 0097 (0.025s): loss = 00.100274, likelihood = 0.0289\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.9094, grad_fn=<SelectBackward>), tensor(-3.2500, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 91.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05789535 0.9421046 ]\n",
      " [0.04003905 0.95996106]\n",
      " [0.96585125 0.03414869]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [-6.026784  -3.2091286  3.377031   0.       ], \n",
      "\tQ: [[-8.875902  -6.086423 ]\n",
      " [-6.4270287 -3.2499912]\n",
      " [ 3.3422856  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35374907 0.34478965 0.         0.30146125] \n",
      "\tDiff: tensor([-0.0204, -0.1781,  0.1667,  0.0319]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000, -0.1781, -0.0204]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.9094, grad_fn=<SelectBackward>), tensor(-3.2500, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Backward Pass  [ ... ] Backward pass converged @ 91.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05789535 0.9421046 ]\n",
      " [0.04003905 0.95996106]\n",
      " [0.96585125 0.03414869]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [-6.026784  -3.2091286  3.377031   0.       ], \n",
      "\tQ: [[-8.875902  -6.086423 ]\n",
      " [-6.4270287 -3.2499912]\n",
      " [ 3.3422856  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35374907 0.34478965 0.         0.30146125] \n",
      "\tDiff: tensor([-0.0204, -0.1781,  0.1667,  0.0319]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000, -0.3562, -0.0408]])\n",
      "\n",
      ">>> Iter: 0098 (0.031s): loss = 00.099269, likelihood = 0.0291\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.9146, grad_fn=<SelectBackward>), tensor(-3.3923, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 91.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05768281 0.9423171 ]\n",
      " [0.03477885 0.9652209 ]\n",
      " [0.96585125 0.03414869]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [-6.178515  -3.3569167  3.377031   0.       ], \n",
      "\tQ: [[-9.031311  -6.2379284]\n",
      " [-6.7156625 -3.392315 ]\n",
      " [ 3.3422856  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35367    0.34314844 0.         0.3031814 ] \n",
      "\tDiff: tensor([-0.0203, -0.1765,  0.1667,  0.0302]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000, -0.1765, -0.0203]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.9146, grad_fn=<SelectBackward>), tensor(-3.3923, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 91.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05768281 0.9423171 ]\n",
      " [0.03477885 0.9652209 ]\n",
      " [0.96585125 0.03414869]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [-6.178515  -3.3569167  3.377031   0.       ], \n",
      "\tQ: [[-9.031311  -6.2379284]\n",
      " [-6.7156625 -3.392315 ]\n",
      " [ 3.3422856  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35367    0.34314844 0.         0.3031814 ] \n",
      "\tDiff: tensor([-0.0203, -0.1765,  0.1667,  0.0302]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000, -0.3530, -0.0407]])\n",
      "\n",
      ">>> Iter: 0099 (0.025s): loss = 00.098409, likelihood = 0.0293\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "maxent_loss_history, learned_policies, _ , _ = MaxEntIRL(trajectory_list, lambda _: S, lambda _: T, A, phi, R_model, R_optimizer, gamma, \n",
    "                                                  verbose=True, debug=True, n_iters=100, boltzmann_temp=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,0,'Iterations')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(maxent_loss_history)\n",
    "plt.title(\"MaxEnt\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Iterations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With Error Fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "R_model = Models.LinearRewardModel_ReLU(phi_dim, 0)\n",
    "R_model.apply(weights_init)\n",
    "\n",
    "# Optimizer\n",
    "lr, weight_decay = 0.1, 1e-2\n",
    "optimizer_fn=lambda params, lr, weight_decay: optim.Adam(params, lr=lr, weight_decay=weight_decay)\n",
    "R_optimizer = optimizer_fn(R_model.parameters(), lr, weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaxEntIRL params \n",
      "-----\n",
      "\t Domains: 2, sizes: [4, 4],\n",
      "\t Action dim: 2, \n",
      "\t Feature dim: 4,\n",
      "\t Iterations: 100, \n",
      "\t Max likelihood: 0.99,\n",
      "\t VI iterations: 100, \n",
      "\t VI convergence eps: 0.001,\n",
      "\t Gamma (discount factor): 0.99,\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass didn't converge by 100.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.93902075 0.06097929]\n",
      " [0.9660942  0.03390587]\n",
      " [0.9660942  0.03390587]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [6.146894 3.384167 3.384167 0.      ], \n",
      "\tQ: [[6.0839763 3.349673 ]\n",
      " [3.349673  0.       ]\n",
      " [3.349673  0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [9.4026023e-01 5.9050620e-02 0.0000000e+00 6.8918540e-04] \n",
      "\tDiff: tensor([-0.6069,  0.1076,  0.1667,  0.3326]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[0., 0., 0., 0.]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass didn't converge by 100.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.93902075 0.06097929]\n",
      " [0.9660942  0.03390587]\n",
      " [0.9660942  0.03390587]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [6.146894 3.384167 3.384167 0.      ], \n",
      "\tQ: [[6.0839763 3.349673 ]\n",
      " [3.349673  0.       ]\n",
      " [3.349673  0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [9.4026023e-01 5.9050620e-02 0.0000000e+00 6.8918540e-04] \n",
      "\tDiff: tensor([-0.6069,  0.1076,  0.1667,  0.3326]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[0., 0., 0., 0.]])\n",
      "\n",
      ">>> Iter: 0000 (0.039s): loss = 00.303463, likelihood = 0.0000\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass didn't converge by 100.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.93902075 0.06097929]\n",
      " [0.9660942  0.03390587]\n",
      " [0.9660942  0.03390587]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [6.146894 3.384167 3.384167 0.      ], \n",
      "\tQ: [[6.0839763 3.349673 ]\n",
      " [3.349673  0.       ]\n",
      " [3.349673  0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [9.4026023e-01 5.9050620e-02 0.0000000e+00 6.8918540e-04] \n",
      "\tDiff: tensor([-0.6069,  0.1076,  0.1667,  0.3326]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[0., 0., 0., 0.]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass didn't converge by 100.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.93902075 0.06097929]\n",
      " [0.9660942  0.03390587]\n",
      " [0.9660942  0.03390587]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [6.146894 3.384167 3.384167 0.      ], \n",
      "\tQ: [[6.0839763 3.349673 ]\n",
      " [3.349673  0.       ]\n",
      " [3.349673  0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [9.4026023e-01 5.9050620e-02 0.0000000e+00 6.8918540e-04] \n",
      "\tDiff: tensor([-0.6069,  0.1076,  0.1667,  0.3326]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[0., 0., 0., 0.]])\n",
      "\n",
      ">>> Iter: 0001 (0.024s): loss = 00.303463, likelihood = 0.0000\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass didn't converge by 100.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.93902075 0.06097929]\n",
      " [0.9660942  0.03390587]\n",
      " [0.9660942  0.03390587]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [6.146894 3.384167 3.384167 0.      ], \n",
      "\tQ: [[6.0839763 3.349673 ]\n",
      " [3.349673  0.       ]\n",
      " [3.349673  0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [9.4026023e-01 5.9050620e-02 0.0000000e+00 6.8918540e-04] \n",
      "\tDiff: tensor([-0.6069,  0.1076,  0.1667,  0.3326]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[0., 0., 0., 0.]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass didn't converge by 100.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.93902075 0.06097929]\n",
      " [0.9660942  0.03390587]\n",
      " [0.9660942  0.03390587]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [6.146894 3.384167 3.384167 0.      ], \n",
      "\tQ: [[6.0839763 3.349673 ]\n",
      " [3.349673  0.       ]\n",
      " [3.349673  0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [9.4026023e-01 5.9050620e-02 0.0000000e+00 6.8918540e-04] \n",
      "\tDiff: tensor([-0.6069,  0.1076,  0.1667,  0.3326]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[0., 0., 0., 0.]])\n",
      "\n",
      ">>> Iter: 0002 (0.026s): loss = 00.303463, likelihood = 0.0000\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass didn't converge by 100.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.93902075 0.06097929]\n",
      " [0.9660942  0.03390587]\n",
      " [0.9660942  0.03390587]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [6.146894 3.384167 3.384167 0.      ], \n",
      "\tQ: [[6.0839763 3.349673 ]\n",
      " [3.349673  0.       ]\n",
      " [3.349673  0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [9.4026023e-01 5.9050620e-02 0.0000000e+00 6.8918540e-04] \n",
      "\tDiff: tensor([-0.6069,  0.1076,  0.1667,  0.3326]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[0., 0., 0., 0.]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass didn't converge by 100.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.93902075 0.06097929]\n",
      " [0.9660942  0.03390587]\n",
      " [0.9660942  0.03390587]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [6.146894 3.384167 3.384167 0.      ], \n",
      "\tQ: [[6.0839763 3.349673 ]\n",
      " [3.349673  0.       ]\n",
      " [3.349673  0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [9.4026023e-01 5.9050620e-02 0.0000000e+00 6.8918540e-04] \n",
      "\tDiff: tensor([-0.6069,  0.1076,  0.1667,  0.3326]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[0., 0., 0., 0.]])\n",
      "\n",
      ">>> Iter: 0003 (0.026s): loss = 00.303463, likelihood = 0.0000\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass didn't converge by 100.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.93902075 0.06097929]\n",
      " [0.9660942  0.03390587]\n",
      " [0.9660942  0.03390587]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [6.146894 3.384167 3.384167 0.      ], \n",
      "\tQ: [[6.0839763 3.349673 ]\n",
      " [3.349673  0.       ]\n",
      " [3.349673  0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [9.4026023e-01 5.9050620e-02 0.0000000e+00 6.8918540e-04] \n",
      "\tDiff: tensor([-0.6069,  0.1076,  0.1667,  0.3326]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[0., 0., 0., 0.]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass didn't converge by 100.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.93902075 0.06097929]\n",
      " [0.9660942  0.03390587]\n",
      " [0.9660942  0.03390587]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [6.146894 3.384167 3.384167 0.      ], \n",
      "\tQ: [[6.0839763 3.349673 ]\n",
      " [3.349673  0.       ]\n",
      " [3.349673  0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [9.4026023e-01 5.9050620e-02 0.0000000e+00 6.8918540e-04] \n",
      "\tDiff: tensor([-0.6069,  0.1076,  0.1667,  0.3326]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[0., 0., 0., 0.]])\n",
      "\n",
      ">>> Iter: 0004 (0.026s): loss = 00.303463, likelihood = 0.0000\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass didn't converge by 100.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.93902075 0.06097929]\n",
      " [0.9660942  0.03390587]\n",
      " [0.9660942  0.03390587]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [6.146894 3.384167 3.384167 0.      ], \n",
      "\tQ: [[6.0839763 3.349673 ]\n",
      " [3.349673  0.       ]\n",
      " [3.349673  0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [9.4026023e-01 5.9050620e-02 0.0000000e+00 6.8918540e-04] \n",
      "\tDiff: tensor([-0.6069,  0.1076,  0.1667,  0.3326]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[0., 0., 0., 0.]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass didn't converge by 100.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.93902075 0.06097929]\n",
      " [0.9660942  0.03390587]\n",
      " [0.9660942  0.03390587]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [6.146894 3.384167 3.384167 0.      ], \n",
      "\tQ: [[6.0839763 3.349673 ]\n",
      " [3.349673  0.       ]\n",
      " [3.349673  0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [9.4026023e-01 5.9050620e-02 0.0000000e+00 6.8918540e-04] \n",
      "\tDiff: tensor([-0.6069,  0.1076,  0.1667,  0.3326]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[0., 0., 0., 0.]])\n",
      "\n",
      ">>> Iter: 0005 (0.025s): loss = 00.303463, likelihood = 0.0000\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass didn't converge by 100.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.93902075 0.06097929]\n",
      " [0.9660942  0.03390587]\n",
      " [0.9660942  0.03390587]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [6.146894 3.384167 3.384167 0.      ], \n",
      "\tQ: [[6.0839763 3.349673 ]\n",
      " [3.349673  0.       ]\n",
      " [3.349673  0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [9.4026023e-01 5.9050620e-02 0.0000000e+00 6.8918540e-04] \n",
      "\tDiff: tensor([-0.6069,  0.1076,  0.1667,  0.3326]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[0., 0., 0., 0.]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass didn't converge by 100.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.93902075 0.06097929]\n",
      " [0.9660942  0.03390587]\n",
      " [0.9660942  0.03390587]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [6.146894 3.384167 3.384167 0.      ], \n",
      "\tQ: [[6.0839763 3.349673 ]\n",
      " [3.349673  0.       ]\n",
      " [3.349673  0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [9.4026023e-01 5.9050620e-02 0.0000000e+00 6.8918540e-04] \n",
      "\tDiff: tensor([-0.6069,  0.1076,  0.1667,  0.3326]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[0., 0., 0., 0.]])\n",
      "\n",
      ">>> Iter: 0006 (0.027s): loss = 00.303463, likelihood = 0.0000\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass didn't converge by 100.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.93902075 0.06097929]\n",
      " [0.9660942  0.03390587]\n",
      " [0.9660942  0.03390587]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [6.146894 3.384167 3.384167 0.      ], \n",
      "\tQ: [[6.0839763 3.349673 ]\n",
      " [3.349673  0.       ]\n",
      " [3.349673  0.       ]\n",
      " [0.        0.       ]]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [9.4026023e-01 5.9050620e-02 0.0000000e+00 6.8918540e-04] \n",
      "\tDiff: tensor([-0.6069,  0.1076,  0.1667,  0.3326]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[0., 0., 0., 0.]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass didn't converge by 100.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.93902075 0.06097929]\n",
      " [0.9660942  0.03390587]\n",
      " [0.9660942  0.03390587]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [6.146894 3.384167 3.384167 0.      ], \n",
      "\tQ: [[6.0839763 3.349673 ]\n",
      " [3.349673  0.       ]\n",
      " [3.349673  0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [9.4026023e-01 5.9050620e-02 0.0000000e+00 6.8918540e-04] \n",
      "\tDiff: tensor([-0.6069,  0.1076,  0.1667,  0.3326]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[0., 0., 0., 0.]])\n",
      "\n",
      ">>> Iter: 0007 (0.026s): loss = 00.303463, likelihood = 0.0000\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass didn't converge by 100.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.93902075 0.06097929]\n",
      " [0.9660942  0.03390587]\n",
      " [0.9660942  0.03390587]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [6.146894 3.384167 3.384167 0.      ], \n",
      "\tQ: [[6.0839763 3.349673 ]\n",
      " [3.349673  0.       ]\n",
      " [3.349673  0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [9.4026023e-01 5.9050620e-02 0.0000000e+00 6.8918540e-04] \n",
      "\tDiff: tensor([-0.6069,  0.1076,  0.1667,  0.3326]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[0., 0., 0., 0.]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass didn't converge by 100.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.93902075 0.06097929]\n",
      " [0.9660942  0.03390587]\n",
      " [0.9660942  0.03390587]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [6.146894 3.384167 3.384167 0.      ], \n",
      "\tQ: [[6.0839763 3.349673 ]\n",
      " [3.349673  0.       ]\n",
      " [3.349673  0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [9.4026023e-01 5.9050620e-02 0.0000000e+00 6.8918540e-04] \n",
      "\tDiff: tensor([-0.6069,  0.1076,  0.1667,  0.3326]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[0., 0., 0., 0.]])\n",
      "\n",
      ">>> Iter: 0008 (0.026s): loss = 00.303463, likelihood = 0.0000\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass didn't converge by 100.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.93902075 0.06097929]\n",
      " [0.9660942  0.03390587]\n",
      " [0.9660942  0.03390587]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [6.146894 3.384167 3.384167 0.      ], \n",
      "\tQ: [[6.0839763 3.349673 ]\n",
      " [3.349673  0.       ]\n",
      " [3.349673  0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [9.4026023e-01 5.9050620e-02 0.0000000e+00 6.8918540e-04] \n",
      "\tDiff: tensor([-0.6069,  0.1076,  0.1667,  0.3326]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[0., 0., 0., 0.]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass didn't converge by 100.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.93902075 0.06097929]\n",
      " [0.9660942  0.03390587]\n",
      " [0.9660942  0.03390587]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [6.146894 3.384167 3.384167 0.      ], \n",
      "\tQ: [[6.0839763 3.349673 ]\n",
      " [3.349673  0.       ]\n",
      " [3.349673  0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [9.4026023e-01 5.9050620e-02 0.0000000e+00 6.8918540e-04] \n",
      "\tDiff: tensor([-0.6069,  0.1076,  0.1667,  0.3326]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[0., 0., 0., 0.]])\n",
      "\n",
      ">>> Iter: 0009 (0.027s): loss = 00.303463, likelihood = 0.0000\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass didn't converge by 100.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.93902075 0.06097929]\n",
      " [0.9660942  0.03390587]\n",
      " [0.9660942  0.03390587]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [6.146894 3.384167 3.384167 0.      ], \n",
      "\tQ: [[6.0839763 3.349673 ]\n",
      " [3.349673  0.       ]\n",
      " [3.349673  0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [9.4026023e-01 5.9050620e-02 0.0000000e+00 6.8918540e-04] \n",
      "\tDiff: tensor([-0.6069,  0.1076,  0.1667,  0.3326]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[0., 0., 0., 0.]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass didn't converge by 100.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.93902075 0.06097929]\n",
      " [0.9660942  0.03390587]\n",
      " [0.9660942  0.03390587]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [6.146894 3.384167 3.384167 0.      ], \n",
      "\tQ: [[6.0839763 3.349673 ]\n",
      " [3.349673  0.       ]\n",
      " [3.349673  0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [9.4026023e-01 5.9050620e-02 0.0000000e+00 6.8918540e-04] \n",
      "\tDiff: tensor([-0.6069,  0.1076,  0.1667,  0.3326]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[0., 0., 0., 0.]])\n",
      "\n",
      ">>> Iter: 0010 (0.027s): loss = 00.303463, likelihood = 0.0000\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(0., grad_fn=<SelectBackward>), tensor(-0.0018, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass didn't converge by 100.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.9394189  0.06058128]\n",
      " [0.96477807 0.03522193]\n",
      " [0.9660942  0.03390587]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [6.1139984 3.3442526 3.384167  0.       ], \n",
      "\tQ: [[ 6.0515046e+00  3.3102291e+00]\n",
      " [ 3.3083954e+00 -1.8338710e-03]\n",
      " [ 3.3496730e+00  0.0000000e+00]\n",
      " [ 0.0000000e+00  0.0000000e+00]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [9.4064230e-01 5.8646653e-02 0.0000000e+00 7.1126310e-04] \n",
      "\tDiff: tensor([-0.6073,  0.1080,  0.1667,  0.3326]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[0.0000, 0.0000, 0.1080, 0.0000]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(0., grad_fn=<SelectBackward>), tensor(-0.0018, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass didn't converge by 100.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.9394189  0.06058128]\n",
      " [0.96477807 0.03522193]\n",
      " [0.9660942  0.03390587]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [6.1139984 3.3442526 3.384167  0.       ], \n",
      "\tQ: [[ 6.0515046e+00  3.3102291e+00]\n",
      " [ 3.3083954e+00 -1.8338710e-03]\n",
      " [ 3.3496730e+00  0.0000000e+00]\n",
      " [ 0.0000000e+00  0.0000000e+00]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [9.4064230e-01 5.8646653e-02 0.0000000e+00 7.1126310e-04] \n",
      "\tDiff: tensor([-0.6073,  0.1080,  0.1667,  0.3326]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[0.0000, 0.0000, 0.2160, 0.0000]])\n",
      "\n",
      ">>> Iter: 0011 (0.026s): loss = 00.303654, likelihood = 0.0000\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-0.0525, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(-0.0594, grad_fn=<SelectBackward>), tensor(-0.0607, grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass didn't converge by 100.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.8966404  0.10335956]\n",
      " [0.9660942  0.03390587]\n",
      " [0.91944826 0.0805517 ]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [5.5667577 3.384167  2.4594781 0.       ], \n",
      "\tQ: [[ 5.4576573   3.2972162 ]\n",
      " [ 3.349673    0.        ]\n",
      " [ 2.3754966  -0.05937798]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.9002015  0.09863033 0.         0.00116817] \n",
      "\tDiff: tensor([-0.5669,  0.0680,  0.1667,  0.3322]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.3322,  0.1667,  0.0000, -0.5669]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-0.0525, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(-0.0594, grad_fn=<SelectBackward>), tensor(-0.0607, grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass didn't converge by 100.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.8966404  0.10335956]\n",
      " [0.9660942  0.03390587]\n",
      " [0.91944826 0.0805517 ]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [5.5667577 3.384167  2.4594781 0.       ], \n",
      "\tQ: [[ 5.4576573   3.2972162 ]\n",
      " [ 3.349673    0.        ]\n",
      " [ 2.3754966  -0.05937798]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.9002015  0.09863033 0.         0.00116817] \n",
      "\tDiff: tensor([-0.5669,  0.0680,  0.1667,  0.3322]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.6643,  0.3333,  0.0000, -1.1337]])\n",
      "\n",
      ">>> Iter: 0012 (0.027s): loss = 00.283434, likelihood = 0.0000\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-0.1017, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(-0.0147, grad_fn=<SelectBackward>), tensor(-0.0142, grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 97.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.856801   0.14319913]\n",
      " [0.966024   0.03397606]\n",
      " [0.95517653 0.0448235 ]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [5.1893754 3.3820992 3.0903604 0.       ], \n",
      "\tQ: [[ 5.034826    3.2458563 ]\n",
      " [ 3.3475325   0.        ]\n",
      " [ 3.0445013  -0.01466234]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.8636363  0.13474202 0.         0.00162178] \n",
      "\tDiff: tensor([-0.5303,  0.0319,  0.1667,  0.3317]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.3317,  0.1667,  0.0000, -0.5303]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-0.1017, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(-0.0147, grad_fn=<SelectBackward>), tensor(-0.0142, grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 97.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.856801   0.14319913]\n",
      " [0.966024   0.03397606]\n",
      " [0.95517653 0.0448235 ]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [5.1893754 3.3820992 3.0903604 0.       ], \n",
      "\tQ: [[ 5.034826    3.2458563 ]\n",
      " [ 3.3475325   0.        ]\n",
      " [ 3.0445013  -0.01466234]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.8636363  0.13474202 0.         0.00162178] \n",
      "\tDiff: tensor([-0.5303,  0.0319,  0.1667,  0.3317]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.6634,  0.3333,  0.0000, -1.0606]])\n",
      "\n",
      ">>> Iter: 0013 (0.028s): loss = 00.265151, likelihood = 0.0000\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-0.1672, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 95.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.8054675  0.19453228]\n",
      " [0.96597147 0.03402844]\n",
      " [0.96597147 0.03402844]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [4.815911  3.3805585 3.3805585 0.       ], \n",
      "\tQ: [[4.5995784 3.1787536]\n",
      " [3.3459375 0.       ]\n",
      " [3.3459375 0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.8180818  0.17971145 0.         0.00220654] \n",
      "\tDiff: tensor([-0.4847, -0.0130,  0.1667,  0.3311]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.4847]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-0.1672, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 95.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.8054675  0.19453228]\n",
      " [0.96597147 0.03402844]\n",
      " [0.96597147 0.03402844]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [4.815911  3.3805585 3.3805585 0.       ], \n",
      "\tQ: [[4.5995784 3.1787536]\n",
      " [3.3459375 0.       ]\n",
      " [3.3459375 0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.8180818  0.17971145 0.         0.00220654] \n",
      "\tDiff: tensor([-0.4847, -0.0130,  0.1667,  0.3311]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.9695]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Iter: 0014 (0.026s): loss = 00.248897, likelihood = 0.0000\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-0.2432, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 94.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.74898434 0.25101545]\n",
      " [0.9659436  0.03405649]\n",
      " [0.9659436  0.03405649]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [4.4841166 3.3797348 3.3797348 0.       ], \n",
      "\tQ: [[4.1950793 3.1018758]\n",
      " [3.345085  0.       ]\n",
      " [3.345085  0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.7699873  0.22716296 0.         0.00284957] \n",
      "\tDiff: tensor([-0.4367, -0.0605,  0.1667,  0.3305]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.4367]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-0.2432, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 94.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.74898434 0.25101545]\n",
      " [0.9659436  0.03405649]\n",
      " [0.9659436  0.03405649]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [4.4841166 3.3797348 3.3797348 0.       ], \n",
      "\tQ: [[4.1950793 3.1018758]\n",
      " [3.345085  0.       ]\n",
      " [3.345085  0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.7699873  0.22716296 0.         0.00284957] \n",
      "\tDiff: tensor([-0.4367, -0.0605,  0.1667,  0.3305]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.8733]])\n",
      "\n",
      ">>> Iter: 0015 (0.025s): loss = 00.248575, likelihood = 0.0001\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-0.3265, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 94.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.69118613 0.3088137 ]\n",
      " [0.9659436  0.03405649]\n",
      " [0.9659436  0.03405649]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [4.193651  3.3797348 3.3797348 0.       ], \n",
      "\tQ: [[3.824305 3.018634]\n",
      " [3.345085 0.      ]\n",
      " [3.345085 0.      ]\n",
      " [0.       0.      ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.7229748  0.27351934 0.         0.0035057 ] \n",
      "\tDiff: tensor([-0.3896, -0.1069,  0.1667,  0.3298]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.3896]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-0.3265, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 94.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.69118613 0.3088137 ]\n",
      " [0.9659436  0.03405649]\n",
      " [0.9659436  0.03405649]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [4.193651  3.3797348 3.3797348 0.       ], \n",
      "\tQ: [[3.824305 3.018634]\n",
      " [3.345085 0.      ]\n",
      " [3.345085 0.      ]\n",
      " [0.       0.      ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.7229748  0.27351934 0.         0.0035057 ] \n",
      "\tDiff: tensor([-0.3896, -0.1069,  0.1667,  0.3298]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.7793]])\n",
      "\n",
      ">>> Iter: 0016 (0.025s): loss = 00.248247, likelihood = 0.0001\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-0.4146, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 93.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.6344722  0.36552778]\n",
      " [0.96591413 0.03408583]\n",
      " [0.96591413 0.03408583]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [3.9359877 3.3788736 3.3788736 0.       ], \n",
      "\tQ: [[3.481026  2.9295747]\n",
      " [3.3441932 0.       ]\n",
      " [3.3441932 0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.679009   0.31683782 0.         0.00415311] \n",
      "\tDiff: tensor([-0.3457, -0.1502,  0.1667,  0.3292]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.3457]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-0.4146, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 93.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.6344722  0.36552778]\n",
      " [0.96591413 0.03408583]\n",
      " [0.96591413 0.03408583]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [3.9359877 3.3788736 3.3788736 0.       ], \n",
      "\tQ: [[3.481026  2.9295747]\n",
      " [3.3441932 0.       ]\n",
      " [3.3441932 0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.679009   0.31683782 0.         0.00415311] \n",
      "\tDiff: tensor([-0.3457, -0.1502,  0.1667,  0.3292]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.6914]])\n",
      "\n",
      ">>> Iter: 0017 (0.023s): loss = 00.247923, likelihood = 0.0002\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-0.5060, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 93.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.5804159  0.41958416]\n",
      " [0.96591413 0.03408583]\n",
      " [0.96591413 0.03408583]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [3.7067115 3.3788736 3.3788736 0.       ], \n",
      "\tQ: [[3.1627011 2.8382204]\n",
      " [3.3441932 0.       ]\n",
      " [3.3441932 0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.63909954 0.35613322 0.         0.00476729] \n",
      "\tDiff: tensor([-0.3058, -0.1895,  0.1667,  0.3286]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.3058]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-0.5060, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 93.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.5804159  0.41958416]\n",
      " [0.96591413 0.03408583]\n",
      " [0.96591413 0.03408583]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [3.7067115 3.3788736 3.3788736 0.       ], \n",
      "\tQ: [[3.1627011 2.8382204]\n",
      " [3.3441932 0.       ]\n",
      " [3.3441932 0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.63909954 0.35613322 0.         0.00476729] \n",
      "\tDiff: tensor([-0.3058, -0.1895,  0.1667,  0.3286]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.6115]])\n",
      "\n",
      ">>> Iter: 0018 (0.023s): loss = 00.247616, likelihood = 0.0002\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-0.5991, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 93.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.5298857  0.47011435]\n",
      " [0.96591413 0.03408583]\n",
      " [0.96591413 0.03408583]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [3.4998384 3.3788736 3.3788736 0.       ], \n",
      "\tQ: [[2.8647444 2.745059 ]\n",
      " [3.3441932 0.       ]\n",
      " [3.3441932 0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.6035549  0.39110374 0.         0.00534141] \n",
      "\tDiff: tensor([-0.2702, -0.2244,  0.1667,  0.3280]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.2702]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-0.5991, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 93.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.5298857  0.47011435]\n",
      " [0.96591413 0.03408583]\n",
      " [0.96591413 0.03408583]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [3.4998384 3.3788736 3.3788736 0.       ], \n",
      "\tQ: [[2.8647444 2.745059 ]\n",
      " [3.3441932 0.       ]\n",
      " [3.3441932 0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.6035549  0.39110374 0.         0.00534141] \n",
      "\tDiff: tensor([-0.2702, -0.2244,  0.1667,  0.3280]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.5404]])\n",
      "\n",
      ">>> Iter: 0019 (0.023s): loss = 00.247329, likelihood = 0.0003\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-0.6930, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 93.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.48332992 0.51667017]\n",
      " [0.96591413 0.03408583]\n",
      " [0.96591413 0.03408583]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [3.311558  3.3788736 3.3788736 0.       ], \n",
      "\tQ: [[2.5845022 2.6512074]\n",
      " [3.3441932 0.       ]\n",
      " [3.3441932 0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.57231253 0.4218171  0.         0.00587038] \n",
      "\tDiff: tensor([-0.2390, -0.2552,  0.1667,  0.3275]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.2390]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-0.6930, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 93.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.48332992 0.51667017]\n",
      " [0.96591413 0.03408583]\n",
      " [0.96591413 0.03408583]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [3.311558  3.3788736 3.3788736 0.       ], \n",
      "\tQ: [[2.5845022 2.6512074]\n",
      " [3.3441932 0.       ]\n",
      " [3.3441932 0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.57231253 0.4218171  0.         0.00587038] \n",
      "\tDiff: tensor([-0.2390, -0.2552,  0.1667,  0.3275]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.4780]])\n",
      "\n",
      ">>> Iter: 0020 (0.023s): loss = 00.247065, likelihood = 0.0003\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-0.7866, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 93.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.4408907  0.5591092 ]\n",
      " [0.96591413 0.03408583]\n",
      " [0.96591413 0.03408583]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [3.1389847 3.3788736 3.3788736 0.       ], \n",
      "\tQ: [[2.3200264 2.5575743]\n",
      " [3.3441932 0.       ]\n",
      " [3.3441932 0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.54509175 0.4485556  0.         0.00635257] \n",
      "\tDiff: tensor([-0.2118, -0.2819,  0.1667,  0.3270]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.2118]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-0.7866, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 93.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.4408907  0.5591092 ]\n",
      " [0.96591413 0.03408583]\n",
      " [0.96591413 0.03408583]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [3.1389847 3.3788736 3.3788736 0.       ], \n",
      "\tQ: [[2.3200264 2.5575743]\n",
      " [3.3441932 0.       ]\n",
      " [3.3441932 0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.54509175 0.4485556  0.         0.00635257] \n",
      "\tDiff: tensor([-0.2118, -0.2819,  0.1667,  0.3270]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.4235]])\n",
      "\n",
      ">>> Iter: 0021 (0.025s): loss = 00.246824, likelihood = 0.0004\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-0.8793, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.40249395 0.5975059 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [2.9789553 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[2.06888   2.4639642]\n",
      " [3.3432608 0.       ]\n",
      " [3.3432608 0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.52149844 0.47170654 0.         0.00679495] \n",
      "\tDiff: tensor([-0.1882, -0.3050,  0.1667,  0.3265]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.1882]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-0.8793, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.40249395 0.5975059 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [2.9789553 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[2.06888   2.4639642]\n",
      " [3.3432608 0.       ]\n",
      " [3.3432608 0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.52149844 0.47170654 0.         0.00679495] \n",
      "\tDiff: tensor([-0.1882, -0.3050,  0.1667,  0.3265]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.3763]])\n",
      "\n",
      ">>> Iter: 0022 (0.023s): loss = 00.246603, likelihood = 0.0004\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-0.9704, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.36797893 0.63202107]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [2.831666  3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[1.8319364 2.3728335]\n",
      " [3.3432608 0.       ]\n",
      " [3.3432608 0.       ]\n",
      " [0.        0.       ]]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.50112915 0.4916834  0.         0.00718746] \n",
      "\tDiff: tensor([-0.1678, -0.3250,  0.1667,  0.3261]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.1678]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-0.9704, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.36797893 0.63202107]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [2.831666  3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[1.8319364 2.3728335]\n",
      " [3.3432608 0.       ]\n",
      " [3.3432608 0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.50112915 0.4916834  0.         0.00718746] \n",
      "\tDiff: tensor([-0.1678, -0.3250,  0.1667,  0.3261]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.3356]])\n",
      "\n",
      ">>> Iter: 0023 (0.024s): loss = 00.246406, likelihood = 0.0005\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-1.0595, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.33706665 0.66293335]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [2.6947982 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[1.6073236 2.2837174]\n",
      " [3.3432608 0.       ]\n",
      " [3.3432608 0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.4835602 0.5089008 0.        0.007539 ] \n",
      "\tDiff: tensor([-0.1502, -0.3422,  0.1667,  0.3258]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.1502]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-1.0595, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.33706665 0.66293335]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [2.6947982 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[1.6073236 2.2837174]\n",
      " [3.3432608 0.       ]\n",
      " [3.3432608 0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.4835602 0.5089008 0.        0.007539 ] \n",
      "\tDiff: tensor([-0.1502, -0.3422,  0.1667,  0.3258]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.3005]])\n",
      "\n",
      ">>> Iter: 0024 (0.025s): loss = 00.246231, likelihood = 0.0005\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-1.1463, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.3094569  0.69054306]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [2.5672548 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[1.3943183 2.1969779]\n",
      " [3.3432608 0.       ]\n",
      " [3.3432608 0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.46840683 0.5237402  0.         0.00785298] \n",
      "\tDiff: tensor([-0.1351, -0.3571,  0.1667,  0.3255]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.1351]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-1.1463, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.3094569  0.69054306]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [2.5672548 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[1.3943183 2.1969779]\n",
      " [3.3432608 0.       ]\n",
      " [3.3432608 0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.46840683 0.5237402  0.         0.00785298] \n",
      "\tDiff: tensor([-0.1351, -0.3571,  0.1667,  0.3255]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.2701]])\n",
      "\n",
      ">>> Iter: 0025 (0.023s): loss = 00.246074, likelihood = 0.0006\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-1.2304, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.28483835 0.71516156]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [2.4481347 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[1.1923012 2.1128879]\n",
      " [3.3432608 0.       ]\n",
      " [3.3432608 0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.45532373 0.53654325 0.         0.00813295] \n",
      "\tDiff: tensor([-0.1220, -0.3699,  0.1667,  0.3252]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.1220]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-1.2304, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.28483835 0.71516156]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [2.4481347 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[1.1923012 2.1128879]\n",
      " [3.3432608 0.       ]\n",
      " [3.3432608 0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.45532373 0.53654325 0.         0.00813295] \n",
      "\tDiff: tensor([-0.1220, -0.3699,  0.1667,  0.3252]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.2440]])\n",
      "\n",
      ">>> Iter: 0026 (0.024s): loss = 00.245934, likelihood = 0.0006\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-1.3116, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.26290545 0.7370947 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [2.3366833 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[1.0007224 2.0316443]\n",
      " [3.3432608 0.       ]\n",
      " [3.3432608 0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.44400826 0.5476095  0.         0.00838237] \n",
      "\tDiff: tensor([-0.1107, -0.3809,  0.1667,  0.3250]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.1107]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-1.3116, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.26290545 0.7370947 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [2.3366833 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[1.0007224 2.0316443]\n",
      " [3.3432608 0.       ]\n",
      " [3.3432608 0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.44400826 0.5476095  0.         0.00838237] \n",
      "\tDiff: tensor([-0.1107, -0.3809,  0.1667,  0.3250]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.2213]])\n",
      "\n",
      ">>> Iter: 0027 (0.023s): loss = 00.245809, likelihood = 0.0006\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-1.3899, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.24336843 0.7566315 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [2.23226   3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[0.8190812 1.9533811]\n",
      " [3.3432608 0.       ]\n",
      " [3.3432608 0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.43419886 0.55719656 0.         0.00860455] \n",
      "\tDiff: tensor([-0.1009, -0.3905,  0.1667,  0.3247]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.1009]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-1.3899, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.24336843 0.7566315 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [2.23226   3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[0.8190812 1.9533811]\n",
      " [3.3432608 0.       ]\n",
      " [3.3432608 0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.43419886 0.55719656 0.         0.00860455] \n",
      "\tDiff: tensor([-0.1009, -0.3905,  0.1667,  0.3247]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.2017]])\n",
      "\n",
      ">>> Iter: 0028 (0.022s): loss = 00.245698, likelihood = 0.0007\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-1.4651, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.22595944 0.77404064]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [2.1343102 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[0.64691037 1.8781793 ]\n",
      " [3.3432608  0.        ]\n",
      " [3.3432608  0.        ]\n",
      " [0.         0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.42567238 0.5655252  0.         0.00880253] \n",
      "\tDiff: tensor([-0.0923, -0.3989,  0.1667,  0.3245]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0923]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-1.4651, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.22595944 0.77404064]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [2.1343102 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[0.64691037 1.8781793 ]\n",
      " [3.3432608  0.        ]\n",
      " [3.3432608  0.        ]\n",
      " [0.         0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.42567238 0.5655252  0.         0.00880253] \n",
      "\tDiff: tensor([-0.0923, -0.3989,  0.1667,  0.3245]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.1847]])\n",
      "\n",
      ">>> Iter: 0029 (0.023s): loss = 00.245599, likelihood = 0.0007\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-1.5372, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.21043435 0.7895658 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [2.042349  3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[0.4837675 1.806077 ]\n",
      " [3.3432608 0.       ]\n",
      " [3.3432608 0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.41823897 0.57278204 0.         0.00897908] \n",
      "\tDiff: tensor([-0.0849, -0.4061,  0.1667,  0.3244]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0849]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-1.5372, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.21043435 0.7895658 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [2.042349  3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[0.4837675 1.806077 ]\n",
      " [3.3432608 0.       ]\n",
      " [3.3432608 0.       ]\n",
      " [0.        0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.41823897 0.57278204 0.         0.00897908] \n",
      "\tDiff: tensor([-0.0849, -0.4061,  0.1667,  0.3244]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.1698]])\n",
      "\n",
      ">>> Iter: 0030 (0.024s): loss = 00.245510, likelihood = 0.0007\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-1.6062, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.19657382 0.80342615]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.9559468 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[0.32922956 1.7370768 ]\n",
      " [3.3432608  0.        ]\n",
      " [3.3432608  0.        ]\n",
      " [0.         0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.41173837 0.5791249  0.         0.00913671] \n",
      "\tDiff: tensor([-0.0784, -0.4125,  0.1667,  0.3242]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0784]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-1.6062, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.19657382 0.80342615]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.9559468 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[0.32922956 1.7370768 ]\n",
      " [3.3432608  0.        ]\n",
      " [3.3432608  0.        ]\n",
      " [0.         0.        ]]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.41173837 0.5791249  0.         0.00913671] \n",
      "\tDiff: tensor([-0.0784, -0.4125,  0.1667,  0.3242]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.1568]])\n",
      "\n",
      ">>> Iter: 0031 (0.024s): loss = 00.245432, likelihood = 0.0008\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-1.6721, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.18418247 0.81581753]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.8747171 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[0.18288872 1.6711526 ]\n",
      " [3.3432608  0.        ]\n",
      " [3.3432608  0.        ]\n",
      " [0.         0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.40603518 0.5846872  0.         0.00927763] \n",
      "\tDiff: tensor([-0.0727, -0.4180,  0.1667,  0.3241]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0727]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-1.6721, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.18418247 0.81581753]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.8747171 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[0.18288872 1.6711526 ]\n",
      " [3.3432608  0.        ]\n",
      " [3.3432608  0.        ]\n",
      " [0.         0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.40603518 0.5846872  0.         0.00927763] \n",
      "\tDiff: tensor([-0.0727, -0.4180,  0.1667,  0.3241]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.1454]])\n",
      "\n",
      ">>> Iter: 0032 (0.025s): loss = 00.245361, likelihood = 0.0008\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-1.7350, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.17308706 0.8269129 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.7983118 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[0.04435123 1.6082559 ]\n",
      " [3.3432608  0.        ]\n",
      " [3.3432608  0.        ]\n",
      " [0.         0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.4010154 0.5895808 0.        0.0094038] \n",
      "\tDiff: tensor([-0.0677, -0.4229,  0.1667,  0.3239]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0677]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-1.7350, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.17308706 0.8269129 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.7983118 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[0.04435123 1.6082559 ]\n",
      " [3.3432608  0.        ]\n",
      " [3.3432608  0.        ]\n",
      " [0.         0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.4010154 0.5895808 0.        0.0094038] \n",
      "\tDiff: tensor([-0.0677, -0.4229,  0.1667,  0.3239]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.1354]])\n",
      "\n",
      ">>> Iter: 0033 (0.024s): loss = 00.245298, likelihood = 0.0008\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-1.7949, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.16313505 0.836865  ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.7264125 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-0.08676429  1.54832   ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3965827  0.5939004  0.         0.00951698] \n",
      "\tDiff: tensor([-0.0632, -0.4272,  0.1667,  0.3238]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0632]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-1.7949, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.16313505 0.836865  ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.7264125 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-0.08676429  1.54832   ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3965827  0.5939004  0.         0.00951698] \n",
      "\tDiff: tensor([-0.0632, -0.4272,  0.1667,  0.3238]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.1265]])\n",
      "\n",
      ">>> Iter: 0034 (0.024s): loss = 00.245242, likelihood = 0.0008\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-1.8520, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.15419228 0.8458077 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.6587281 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-0.21082674  1.4912648 ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.39265585 0.59772545 0.         0.00961868] \n",
      "\tDiff: tensor([-0.0593, -0.4311,  0.1667,  0.3237]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0593]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-1.8520, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.15419228 0.8458077 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.6587281 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-0.21082674  1.4912648 ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.39265585 0.59772545 0.         0.00961868] \n",
      "\tDiff: tensor([-0.0593, -0.4311,  0.1667,  0.3237]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.1186]])\n",
      "\n",
      ">>> Iter: 0035 (0.024s): loss = 00.245191, likelihood = 0.0008\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-1.9063, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.14614116 0.8538588 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.594989  3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-0.3281933  1.4369996]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.38916612 0.60112363 0.         0.00971024] \n",
      "\tDiff: tensor([-0.0558, -0.4345,  0.1667,  0.3236]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0558]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-1.9063, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.14614116 0.8538588 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.594989  3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-0.3281933  1.4369996]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.38916612 0.60112363 0.         0.00971024] \n",
      "\tDiff: tensor([-0.0558, -0.4345,  0.1667,  0.3236]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.1117]])\n",
      "\n",
      ">>> Iter: 0036 (0.024s): loss = 00.245145, likelihood = 0.0008\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-1.9578, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.13887864 0.8611214 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.534946  3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-0.4392089  1.3854262]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.38605526 0.6041519  0.         0.00979283] \n",
      "\tDiff: tensor([-0.0527, -0.4375,  0.1667,  0.3235]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0527]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-1.9578, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.13887864 0.8611214 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.534946  3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-0.4392089  1.3854262]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.38605526 0.6041519  0.         0.00979283] \n",
      "\tDiff: tensor([-0.0527, -0.4375,  0.1667,  0.3235]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.1054]])\n",
      "\n",
      ">>> Iter: 0037 (0.024s): loss = 00.245104, likelihood = 0.0009\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.0068, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.13231449 0.8676855 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.4783672 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-0.5442065  1.3364413]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.38327384 0.6068587  0.         0.00986748] \n",
      "\tDiff: tensor([-0.0499, -0.4402,  0.1667,  0.3235]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0499]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.0068, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.13231449 0.8676855 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.4783672 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-0.5442065  1.3364413]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.38327384 0.6068587  0.         0.00986748] \n",
      "\tDiff: tensor([-0.0499, -0.4402,  0.1667,  0.3235]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0999]])\n",
      "\n",
      ">>> Iter: 0038 (0.023s): loss = 00.245066, likelihood = 0.0009\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.0533, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.1263698  0.8736302 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.4250368 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-0.6435059  1.2899387]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3807797  0.60928524 0.         0.00993508] \n",
      "\tDiff: tensor([-0.0474, -0.4426,  0.1667,  0.3234]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0474]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.0533, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.1263698  0.8736302 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.4250368 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-0.6435059  1.2899387]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3807797  0.60928524 0.         0.00993508] \n",
      "\tDiff: tensor([-0.0474, -0.4426,  0.1667,  0.3234]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0949]])\n",
      "\n",
      ">>> Iter: 0039 (0.023s): loss = 00.245032, likelihood = 0.0009\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.0975, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.1209754  0.8790246 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.3747526 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-0.7374154  1.2458103]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.37853682 0.6114668  0.         0.00999643] \n",
      "\tDiff: tensor([-0.0452, -0.4448,  0.1667,  0.3233]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0452]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.0975, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.1209754  0.8790246 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.3747526 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-0.7374154  1.2458103]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.37853682 0.6114668  0.         0.00999643] \n",
      "\tDiff: tensor([-0.0452, -0.4448,  0.1667,  0.3233]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0904]])\n",
      "\n",
      ">>> Iter: 0040 (0.024s): loss = 00.245002, likelihood = 0.0009\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.1393, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.11607069 0.8839293 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.3273259 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-0.82623005  1.2039478 ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3765144 0.6134334 0.        0.0100522] \n",
      "\tDiff: tensor([-0.0432, -0.4468,  0.1667,  0.3233]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0432]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.1393, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.11607069 0.8839293 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.3273259 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-0.82623005  1.2039478 ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3765144 0.6134334 0.        0.0100522] \n",
      "\tDiff: tensor([-0.0432, -0.4468,  0.1667,  0.3233]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0864]])\n",
      "\n",
      ">>> Iter: 0041 (0.025s): loss = 00.244974, likelihood = 0.0009\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.1790, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.11160243 0.8883975 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.2825797 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-0.91023284  1.1642437 ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.37468585 0.61521107 0.         0.01010302] \n",
      "\tDiff: tensor([-0.0414, -0.4485,  0.1667,  0.3232]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0414]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.1790, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.11160243 0.8883975 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.2825797 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-0.91023284  1.1642437 ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.37468585 0.61521107 0.         0.01010302] \n",
      "\tDiff: tensor([-0.0414, -0.4485,  0.1667,  0.3232]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0827]])\n",
      "\n",
      ">>> Iter: 0042 (0.023s): loss = 00.244948, likelihood = 0.0009\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.2167, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.10752399 0.89247596]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.2403481 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-0.98969316  1.1265924 ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3730285  0.61682206 0.         0.0101494 ] \n",
      "\tDiff: tensor([-0.0397, -0.4502,  0.1667,  0.3232]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0397]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.2167, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.10752399 0.89247596]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.2403481 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-0.98969316  1.1265924 ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3730285  0.61682206 0.         0.0101494 ] \n",
      "\tDiff: tensor([-0.0397, -0.4502,  0.1667,  0.3232]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0794]])\n",
      "\n",
      ">>> Iter: 0043 (0.024s): loss = 00.244925, likelihood = 0.0009\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.2524, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.10379423 0.89620584]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.2004751 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-1.0648698  1.0908899]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.37152252 0.6182857  0.         0.01019182] \n",
      "\tDiff: tensor([-0.0382, -0.4516,  0.1667,  0.3231]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0382]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.2524, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.10379423 0.89620584]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.2004751 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-1.0648698  1.0908899]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.37152252 0.6182857  0.         0.01019182] \n",
      "\tDiff: tensor([-0.0382, -0.4516,  0.1667,  0.3231]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0764]])\n",
      "\n",
      ">>> Iter: 0044 (0.024s): loss = 00.244904, likelihood = 0.0009\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.2862, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.10037698 0.899623  ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.1628152 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-1.1360073  1.0570357]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.37015083 0.6196185  0.         0.01023068] \n",
      "\tDiff: tensor([-0.0368, -0.4530,  0.1667,  0.3231]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0368]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.2862, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.10037698 0.899623  ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.1628152 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-1.1360073  1.0570357]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.37015083 0.6196185  0.         0.01023068] \n",
      "\tDiff: tensor([-0.0368, -0.4530,  0.1667,  0.3231]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0736]])\n",
      "\n",
      ">>> Iter: 0045 (0.022s): loss = 00.244885, likelihood = 0.0009\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.3183, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.09724031 0.90275973]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.127231  3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-1.203339   1.0249321]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.36889866 0.620835   0.         0.01026635] \n",
      "\tDiff: tensor([-0.0356, -0.4542,  0.1667,  0.3231]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0356]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.3183, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.09724031 0.90275973]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.127231  3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-1.203339   1.0249321]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.36889866 0.620835   0.         0.01026635] \n",
      "\tDiff: tensor([-0.0356, -0.4542,  0.1667,  0.3231]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0711]])\n",
      "\n",
      ">>> Iter: 0046 (0.023s): loss = 00.244867, likelihood = 0.0009\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.3488, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.09435599 0.90564406]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.0935938 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-1.2670867  0.9944849]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.367753   0.6219479  0.         0.01029915] \n",
      "\tDiff: tensor([-0.0344, -0.4553,  0.1667,  0.3230]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0344]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.3488, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.09435599 0.90564406]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.0935938 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-1.2670867  0.9944849]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.367753   0.6219479  0.         0.01029915] \n",
      "\tDiff: tensor([-0.0344, -0.4553,  0.1667,  0.3230]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0688]])\n",
      "\n",
      ">>> Iter: 0047 (0.026s): loss = 00.244850, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.3777, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.09169897 0.908301  ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.0617828 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-1.3274612   0.96560335]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.36670256 0.6229681  0.         0.01032936] \n",
      "\tDiff: tensor([-0.0334, -0.4563,  0.1667,  0.3230]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0334]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.3777, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.09169897 0.908301  ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.0617828 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-1.3274612   0.96560335]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.36670256 0.6229681  0.         0.01032936] \n",
      "\tDiff: tensor([-0.0334, -0.4563,  0.1667,  0.3230]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0667]])\n",
      "\n",
      ">>> Iter: 0048 (0.025s): loss = 00.244835, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.4051, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.08924719 0.9107528 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.0316848 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-1.3846606   0.93820095]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.36573744 0.62390536 0.         0.01035725] \n",
      "\tDiff: tensor([-0.0324, -0.4572,  0.1667,  0.3230]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0324]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.4051, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.08924719 0.9107528 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.0316848 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-1.3846606   0.93820095]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.36573744 0.62390536 0.         0.01035725] \n",
      "\tDiff: tensor([-0.0324, -0.4572,  0.1667,  0.3230]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0648]])\n",
      "\n",
      ">>> Iter: 0049 (0.024s): loss = 00.244821, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.4311, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.08698089 0.91301906]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.003193  3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-1.4388739  0.9121945]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.36484885 0.6247681  0.         0.01038302] \n",
      "\tDiff: tensor([-0.0315, -0.4581,  0.1667,  0.3230]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0315]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.4311, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.08698089 0.91301906]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [1.003193  3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-1.4388739  0.9121945]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.36484885 0.6247681  0.         0.01038302] \n",
      "\tDiff: tensor([-0.0315, -0.4581,  0.1667,  0.3230]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0630]])\n",
      "\n",
      ">>> Iter: 0050 (0.024s): loss = 00.244808, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.4558, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.08488257 0.9151174 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.97620773 3.3779728  3.3779728  0.        ], \n",
      "\tQ: [[-1.4902788  0.8875048]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3640292  0.6255639  0.         0.01040688] \n",
      "\tDiff: tensor([-0.0307, -0.4589,  0.1667,  0.3229]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0307]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.4558, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.08488257 0.9151174 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.97620773 3.3779728  3.3779728  0.        ], \n",
      "\tQ: [[-1.4902788  0.8875048]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3640292  0.6255639  0.         0.01040688] \n",
      "\tDiff: tensor([-0.0307, -0.4589,  0.1667,  0.3229]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0614]])\n",
      "\n",
      ">>> Iter: 0051 (0.024s): loss = 00.244797, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.4792, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.0829366  0.9170634 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.95063525 3.3779728  3.3779728  0.        ], \n",
      "\tQ: [[-1.5390437  0.8640566]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3632717  0.6262993  0.         0.01042901] \n",
      "\tDiff: tensor([-0.0299, -0.4596,  0.1667,  0.3229]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0299]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.4792, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.0829366  0.9170634 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.95063525 3.3779728  3.3779728  0.        ], \n",
      "\tQ: [[-1.5390437  0.8640566]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3632717  0.6262993  0.         0.01042901] \n",
      "\tDiff: tensor([-0.0299, -0.4596,  0.1667,  0.3229]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0599]])\n",
      "\n",
      ">>> Iter: 0052 (0.024s): loss = 00.244786, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.5015, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.08112901 0.918871  ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.92638785 3.3779728  3.3779728  0.        ], \n",
      "\tQ: [[-1.5853269  0.8417783]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3625703  0.6269801  0.         0.01044957] \n",
      "\tDiff: tensor([-0.0292, -0.4603,  0.1667,  0.3229]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0292]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.5015, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.08112901 0.918871  ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.92638785 3.3779728  3.3779728  0.        ], \n",
      "\tQ: [[-1.5853269  0.8417783]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3625703  0.6269801  0.         0.01044957] \n",
      "\tDiff: tensor([-0.0292, -0.4603,  0.1667,  0.3229]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0585]])\n",
      "\n",
      ">>> Iter: 0053 (0.024s): loss = 00.244775, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.5227, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.07944735 0.9205527 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.90338326 3.3779728  3.3779728  0.        ], \n",
      "\tQ: [[-1.6292775  0.8206022]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.36191976 0.6276116  0.         0.01046869] \n",
      "\tDiff: tensor([-0.0286, -0.4609,  0.1667,  0.3229]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0286]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.5227, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.07944735 0.9205527 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.90338326 3.3779728  3.3779728  0.        ], \n",
      "\tQ: [[-1.6292775  0.8206022]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.36191976 0.6276116  0.         0.01046869] \n",
      "\tDiff: tensor([-0.0286, -0.4609,  0.1667,  0.3229]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0572]])\n",
      "\n",
      ">>> Iter: 0054 (0.023s): loss = 00.244766, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.5428, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.07788043 0.9221195 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.88154435 3.3779728  3.3779728  0.        ], \n",
      "\tQ: [[-1.6710362  0.8004639]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.36131525 0.6281982  0.         0.01048651] \n",
      "\tDiff: tensor([-0.0280, -0.4615,  0.1667,  0.3228]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0280]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.5428, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.07788043 0.9221195 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.88154435 3.3779728  3.3779728  0.        ], \n",
      "\tQ: [[-1.6710362  0.8004639]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.36131525 0.6281982  0.         0.01048651] \n",
      "\tDiff: tensor([-0.0280, -0.4615,  0.1667,  0.3228]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0560]])\n",
      "\n",
      ">>> Iter: 0055 (0.022s): loss = 00.244757, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.5620, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.07641822 0.9235818 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.8607986 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-1.7107356  0.7813027]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3607526  0.6287442  0.         0.01050314] \n",
      "\tDiff: tensor([-0.0274, -0.4621,  0.1667,  0.3228]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0274]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.5620, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.07641822 0.9235818 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.8607986 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-1.7107356  0.7813027]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3607526  0.6287442  0.         0.01050314] \n",
      "\tDiff: tensor([-0.0274, -0.4621,  0.1667,  0.3228]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0548]])\n",
      "\n",
      ">>> Iter: 0056 (0.023s): loss = 00.244748, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.5802, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.07505167 0.92494833]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.8410787 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-1.7484998  0.7630613]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.36022815 0.6292532  0.         0.01051868] \n",
      "\tDiff: tensor([-0.0269, -0.4626,  0.1667,  0.3228]) \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0269]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.5802, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.07505167 0.92494833]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.8410787 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-1.7484998  0.7630613]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.36022815 0.6292532  0.         0.01051868] \n",
      "\tDiff: tensor([-0.0269, -0.4626,  0.1667,  0.3228]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0538]])\n",
      "\n",
      ">>> Iter: 0057 (0.026s): loss = 00.244741, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.5976, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.07377271 0.92622733]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.8223212 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-1.7844453  0.7456856]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35973835 0.62972844 0.         0.01053322] \n",
      "\tDiff: tensor([-0.0264, -0.4631,  0.1667,  0.3228]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0264]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.5976, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.07377271 0.92622733]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.8223212 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-1.7844453  0.7456856]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35973835 0.62972844 0.         0.01053322] \n",
      "\tDiff: tensor([-0.0264, -0.4631,  0.1667,  0.3228]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0528]])\n",
      "\n",
      ">>> Iter: 0058 (0.026s): loss = 00.244733, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.6141, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.07257397 0.92742604]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.8044668 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-1.8186821   0.72912455]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35928035 0.63017285 0.         0.01054686] \n",
      "\tDiff: tensor([-0.0259, -0.4635,  0.1667,  0.3228]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0259]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.6141, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.07257397 0.92742604]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.8044668 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-1.8186821   0.72912455]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35928035 0.63017285 0.         0.01054686] \n",
      "\tDiff: tensor([-0.0259, -0.4635,  0.1667,  0.3228]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0519]])\n",
      "\n",
      ">>> Iter: 0059 (0.023s): loss = 00.244727, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.6299, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.07144886 0.92855114]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.78745985 3.3779728  3.3779728  0.        ], \n",
      "\tQ: [[-1.8513135   0.71333003]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35885122 0.63058907 0.         0.01055965] \n",
      "\tDiff: tensor([-0.0255, -0.4639,  0.1667,  0.3228]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0255]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.6299, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.07144886 0.92855114]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.78745985 3.3779728  3.3779728  0.        ], \n",
      "\tQ: [[-1.8513135   0.71333003]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35885122 0.63058907 0.         0.01055965] \n",
      "\tDiff: tensor([-0.0255, -0.4639,  0.1667,  0.3228]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0510]])\n",
      "\n",
      ">>> Iter: 0060 (0.024s): loss = 00.244720, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.6450, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.0703914  0.9296085 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.77124894 3.3779728  3.3779728  0.        ], \n",
      "\tQ: [[-1.8824351  0.6982572]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35844877 0.6309795  0.         0.01057168] \n",
      "\tDiff: tensor([-0.0251, -0.4643,  0.1667,  0.3228]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0251]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.6450, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.0703914  0.9296085 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.77124894 3.3779728  3.3779728  0.        ], \n",
      "\tQ: [[-1.8824351  0.6982572]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35844877 0.6309795  0.         0.01057168] \n",
      "\tDiff: tensor([-0.0251, -0.4643,  0.1667,  0.3228]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0502]])\n",
      "\n",
      ">>> Iter: 0061 (0.026s): loss = 00.244714, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.6594, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.06939621 0.9306038 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.75578505 3.3779728  3.3779728  0.        ], \n",
      "\tQ: [[-1.912138   0.6838634]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3580707  0.63134634 0.         0.01058299] \n",
      "\tDiff: tensor([-0.0247, -0.4647,  0.1667,  0.3228]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0247]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.6594, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.06939621 0.9306038 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.75578505 3.3779728  3.3779728  0.        ], \n",
      "\tQ: [[-1.912138   0.6838634]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3580707  0.63134634 0.         0.01058299] \n",
      "\tDiff: tensor([-0.0247, -0.4647,  0.1667,  0.3228]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0495]])\n",
      "\n",
      ">>> Iter: 0062 (0.024s): loss = 00.244709, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.6732, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.0684583  0.9315416 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.74102294 3.3779728  3.3779728  0.        ], \n",
      "\tQ: [[-1.9405074   0.67010856]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35771498 0.6316914  0.         0.01059366] \n",
      "\tDiff: tensor([-0.0244, -0.4650,  0.1667,  0.3227]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0244]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.6732, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.0684583  0.9315416 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.74102294 3.3779728  3.3779728  0.        ], \n",
      "\tQ: [[-1.9405074   0.67010856]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35771498 0.6316914  0.         0.01059366] \n",
      "\tDiff: tensor([-0.0244, -0.4650,  0.1667,  0.3227]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0488]])\n",
      "\n",
      ">>> Iter: 0063 (0.023s): loss = 00.244703, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.6863, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.06757331 0.93242675]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.7269202 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-1.967622   0.6569555]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35737982 0.6320165  0.         0.01060373] \n",
      "\tDiff: tensor([-0.0240, -0.4653,  0.1667,  0.3227]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0240]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.6863, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.06757331 0.93242675]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.7269202 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-1.967622   0.6569555]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35737982 0.6320165  0.         0.01060373] \n",
      "\tDiff: tensor([-0.0240, -0.4653,  0.1667,  0.3227]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0481]])\n",
      "\n",
      ">>> Iter: 0064 (0.023s): loss = 00.244698, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.6989, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.06673712 0.9332629 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.71343726 3.3779728  3.3779728  0.        ], \n",
      "\tQ: [[-1.9935567  0.6443689]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35706368 0.6323231  0.         0.01061323] \n",
      "\tDiff: tensor([-0.0237, -0.4657,  0.1667,  0.3227]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0237]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.6989, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.06673712 0.9332629 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.71343726 3.3779728  3.3779728  0.        ], \n",
      "\tQ: [[-1.9935567  0.6443689]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35706368 0.6323231  0.         0.01061323] \n",
      "\tDiff: tensor([-0.0237, -0.4657,  0.1667,  0.3227]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0475]])\n",
      "\n",
      ">>> Iter: 0065 (0.026s): loss = 00.244693, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.7109, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.06594606 0.9340539 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.700537  3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-2.018381   0.6323159]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.356765   0.63261276 0.         0.01062223] \n",
      "\tDiff: tensor([-0.0234, -0.4659,  0.1667,  0.3227]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0234]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.7109, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.06594606 0.9340539 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.700537  3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-2.018381   0.6323159]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.356765   0.63261276 0.         0.01062223] \n",
      "\tDiff: tensor([-0.0234, -0.4659,  0.1667,  0.3227]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0469]])\n",
      "\n",
      ">>> Iter: 0066 (0.025s): loss = 00.244689, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.7225, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.06519682 0.9348032 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.688185  3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-2.0421596  0.6207657]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3564825  0.6328868  0.         0.01063075] \n",
      "\tDiff: tensor([-0.0231, -0.4662,  0.1667,  0.3227]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0231]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.7225, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.06519682 0.9348032 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.688185  3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-2.0421596  0.6207657]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3564825  0.6328868  0.         0.01063075] \n",
      "\tDiff: tensor([-0.0231, -0.4662,  0.1667,  0.3227]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0463]])\n",
      "\n",
      ">>> Iter: 0067 (0.025s): loss = 00.244685, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.7336, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.06448629 0.9355137 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.67634875 3.3779728  3.3779728  0.        ], \n",
      "\tQ: [[-2.064954    0.60968924]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3562149  0.6331462  0.         0.01063883] \n",
      "\tDiff: tensor([-0.0229, -0.4665,  0.1667,  0.3227]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0229]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.7336, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.06448629 0.9355137 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.67634875 3.3779728  3.3779728  0.        ], \n",
      "\tQ: [[-2.064954    0.60968924]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3562149  0.6331462  0.         0.01063883] \n",
      "\tDiff: tensor([-0.0229, -0.4665,  0.1667,  0.3227]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0458]])\n",
      "\n",
      ">>> Iter: 0068 (0.023s): loss = 00.244681, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.7442, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.06381167 0.9361883 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.664998  3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-2.086821    0.59905934]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3559612 0.6333923 0.        0.0106465] \n",
      "\tDiff: tensor([-0.0226, -0.4667,  0.1667,  0.3227]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0226]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.7442, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.06381167 0.9361883 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.664998  3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-2.086821    0.59905934]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3559612 0.6333923 0.        0.0106465] \n",
      "\tDiff: tensor([-0.0226, -0.4667,  0.1667,  0.3227]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0453]])\n",
      "\n",
      ">>> Iter: 0069 (0.023s): loss = 00.244677, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.7544, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.06317044 0.93682957]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.65410465 3.3779728  3.3779728  0.        ], \n",
      "\tQ: [[-2.107814    0.58885074]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3557203  0.6336259  0.         0.01065379] \n",
      "\tDiff: tensor([-0.0224, -0.4670,  0.1667,  0.3227]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0224]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.7544, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.06317044 0.93682957]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.65410465 3.3779728  3.3779728  0.        ], \n",
      "\tQ: [[-2.107814    0.58885074]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3557203  0.6336259  0.         0.01065379] \n",
      "\tDiff: tensor([-0.0224, -0.4670,  0.1667,  0.3227]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0448]])\n",
      "\n",
      ">>> Iter: 0070 (0.023s): loss = 00.244673, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.7642, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.06256025 0.93743974]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.64364237 3.3779728  3.3779728  0.        ], \n",
      "\tQ: [[-2.1279829  0.5790396]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3554913  0.63384795 0.         0.01066073] \n",
      "\tDiff: tensor([-0.0222, -0.4672,  0.1667,  0.3227]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0222]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.7642, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.06256025 0.93743974]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.64364237 3.3779728  3.3779728  0.        ], \n",
      "\tQ: [[-2.1279829  0.5790396]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3554913  0.63384795 0.         0.01066073] \n",
      "\tDiff: tensor([-0.0222, -0.4672,  0.1667,  0.3227]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0443]])\n",
      "\n",
      ">>> Iter: 0071 (0.023s): loss = 00.244670, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.7737, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.06197894 0.93802106]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.6335866 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-2.147374   0.5696037]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35527346 0.63405925 0.         0.01066735] \n",
      "\tDiff: tensor([-0.0219, -0.4674,  0.1667,  0.3227]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0219]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.7737, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.06197894 0.93802106]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.6335866 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-2.147374   0.5696037]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35527346 0.63405925 0.         0.01066735] \n",
      "\tDiff: tensor([-0.0219, -0.4674,  0.1667,  0.3227]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0439]])\n",
      "\n",
      ">>> Iter: 0072 (0.024s): loss = 00.244666, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.7827, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.06142455 0.93857545]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.6239141 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-2.1660314  0.5605221]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35506582 0.6342605  0.         0.01067365] \n",
      "\tDiff: tensor([-0.0217, -0.4676,  0.1667,  0.3227]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0217]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.7827, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.06142455 0.93857545]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.6239141 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-2.1660314  0.5605221]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35506582 0.6342605  0.         0.01067365] \n",
      "\tDiff: tensor([-0.0217, -0.4676,  0.1667,  0.3227]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0435]])\n",
      "\n",
      ">>> Iter: 0073 (0.024s): loss = 00.244663, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.7915, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.06089533 0.9391047 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.61460376 3.3779728  3.3779728  0.        ], \n",
      "\tQ: [[-2.183995    0.55177546]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35486785 0.6344525  0.         0.01067967] \n",
      "\tDiff: tensor([-0.0215, -0.4678,  0.1667,  0.3227]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0215]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.7915, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.06089533 0.9391047 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.61460376 3.3779728  3.3779728  0.        ], \n",
      "\tQ: [[-2.183995    0.55177546]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35486785 0.6344525  0.         0.01067967] \n",
      "\tDiff: tensor([-0.0215, -0.4678,  0.1667,  0.3227]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0431]])\n",
      "\n",
      ">>> Iter: 0074 (0.025s): loss = 00.244660, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.7999, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.06038957 0.9396105 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.6056356 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-2.2013032  0.5433457]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35467878 0.6346358  0.         0.01068542] \n",
      "\tDiff: tensor([-0.0213, -0.4680,  0.1667,  0.3226]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0213]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.7999, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.06038957 0.9396105 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.6056356 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-2.2013032  0.5433457]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35467878 0.6346358  0.         0.01068542] \n",
      "\tDiff: tensor([-0.0213, -0.4680,  0.1667,  0.3226]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0427]])\n",
      "\n",
      ">>> Iter: 0075 (0.023s): loss = 00.244657, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.8080, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05990576 0.9400942 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.5969908 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-2.2179918  0.5352156]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35449815 0.63481086 0.         0.01069092] \n",
      "\tDiff: tensor([-0.0212, -0.4681,  0.1667,  0.3226]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0212]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.8080, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05990576 0.9400942 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.5969908 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-2.2179918  0.5352156]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35449815 0.63481086 0.         0.01069092] \n",
      "\tDiff: tensor([-0.0212, -0.4681,  0.1667,  0.3226]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0423]])\n",
      "\n",
      ">>> Iter: 0076 (0.027s): loss = 00.244655, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.8159, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05944253 0.9405575 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.5886518 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-2.2340937   0.52736926]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3543253  0.63497853 0.         0.01069619] \n",
      "\tDiff: tensor([-0.0210, -0.4683,  0.1667,  0.3226]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0210]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.8159, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05944253 0.9405575 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.5886518 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-2.2340937   0.52736926]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3543253  0.63497853 0.         0.01069619] \n",
      "\tDiff: tensor([-0.0210, -0.4683,  0.1667,  0.3226]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0420]])\n",
      "\n",
      ">>> Iter: 0077 (0.024s): loss = 00.244652, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.8235, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05899854 0.9410015 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.58060217 3.3779728  3.3779728  0.        ], \n",
      "\tQ: [[-2.2496405  0.5197916]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3541598  0.635139   0.         0.01070124] \n",
      "\tDiff: tensor([-0.0208, -0.4685,  0.1667,  0.3226]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0208]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.8235, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05899854 0.9410015 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.58060217 3.3779728  3.3779728  0.        ], \n",
      "\tQ: [[-2.2496405  0.5197916]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3541598  0.635139   0.         0.01070124] \n",
      "\tDiff: tensor([-0.0208, -0.4685,  0.1667,  0.3226]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0417]])\n",
      "\n",
      ">>> Iter: 0078 (0.024s): loss = 00.244649, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.8308, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05857265 0.9414274 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.5728271 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-2.2646604   0.51246905]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35400113 0.6352928  0.         0.01070608] \n",
      "\tDiff: tensor([-0.0207, -0.4686,  0.1667,  0.3226]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0207]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.8308, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05857265 0.9414274 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.5728271 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-2.2646604   0.51246905]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35400113 0.6352928  0.         0.01070608] \n",
      "\tDiff: tensor([-0.0207, -0.4686,  0.1667,  0.3226]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0413]])\n",
      "\n",
      ">>> Iter: 0079 (0.023s): loss = 00.244647, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.8379, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05816374 0.9418363 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.5653121 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-2.279181    0.50538826]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35384893 0.6354404  0.         0.01071073] \n",
      "\tDiff: tensor([-0.0205, -0.4688,  0.1667,  0.3226]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0205]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.8379, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05816374 0.9418363 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.5653121 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-2.279181    0.50538826]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35384893 0.6354404  0.         0.01071073] \n",
      "\tDiff: tensor([-0.0205, -0.4688,  0.1667,  0.3226]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0410]])\n",
      "\n",
      ">>> Iter: 0080 (0.024s): loss = 00.244645, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.8447, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05777083 0.9422292 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.5580438 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-2.2932277   0.49853706]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35370278 0.63558203 0.         0.0107152 ] \n",
      "\tDiff: tensor([-0.0204, -0.4689,  0.1667,  0.3226]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0204]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.8447, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05777083 0.9422292 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.5580438 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-2.2932277   0.49853706]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35370278 0.63558203 0.         0.0107152 ] \n",
      "\tDiff: tensor([-0.0204, -0.4689,  0.1667,  0.3226]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0407]])\n",
      "\n",
      ">>> Iter: 0081 (0.024s): loss = 00.244642, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.8514, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05739295 0.94260716]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.55100995 3.3779728  3.3779728  0.        ], \n",
      "\tQ: [[-2.306824    0.49190426]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35356233 0.6357183  0.         0.0107195 ] \n",
      "\tDiff: tensor([-0.0202, -0.4691,  0.1667,  0.3226]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0202]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.8514, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05739295 0.94260716]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.55100995 3.3779728  3.3779728  0.        ], \n",
      "\tQ: [[-2.306824    0.49190426]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35356233 0.6357183  0.         0.0107195 ] \n",
      "\tDiff: tensor([-0.0202, -0.4691,  0.1667,  0.3226]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0405]])\n",
      "\n",
      ">>> Iter: 0082 (0.024s): loss = 00.244640, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.8578, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05702923 0.9429707 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.54419893 3.3779728  3.3779728  0.        ], \n",
      "\tQ: [[-2.3199923   0.48547888]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3534272  0.6358492  0.         0.01072363] \n",
      "\tDiff: tensor([-0.0201, -0.4692,  0.1667,  0.3226]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0201]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.8578, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05702923 0.9429707 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.54419893 3.3779728  3.3779728  0.        ], \n",
      "\tQ: [[-2.3199923   0.48547888]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3534272  0.6358492  0.         0.01072363] \n",
      "\tDiff: tensor([-0.0201, -0.4692,  0.1667,  0.3226]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0402]])\n",
      "\n",
      ">>> Iter: 0083 (0.025s): loss = 00.244638, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.8640, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05667892 0.94332105]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.53759974 3.3779728  3.3779728  0.        ], \n",
      "\tQ: [[-2.3327532   0.47925115]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3532971  0.63597524 0.         0.01072762] \n",
      "\tDiff: tensor([-0.0200, -0.4693,  0.1667,  0.3226]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0200]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.8640, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05667892 0.94332105]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.53759974 3.3779728  3.3779728  0.        ], \n",
      "\tQ: [[-2.3327532   0.47925115]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3532971  0.63597524 0.         0.01072762] \n",
      "\tDiff: tensor([-0.0200, -0.4693,  0.1667,  0.3226]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0399]])\n",
      "\n",
      ">>> Iter: 0084 (0.024s): loss = 00.244636, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.8700, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05634124 0.94365877]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.5312022 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-2.3451264   0.47321153]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35317186 0.6360967  0.         0.01073146] \n",
      "\tDiff: tensor([-0.0198, -0.4694,  0.1667,  0.3226]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0198]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.8700, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05634124 0.94365877]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.5312022 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-2.3451264   0.47321153]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35317186 0.6360967  0.         0.01073146] \n",
      "\tDiff: tensor([-0.0198, -0.4694,  0.1667,  0.3226]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0397]])\n",
      "\n",
      ">>> Iter: 0085 (0.023s): loss = 00.244634, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.8759, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05601551 0.94398445]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.524997  3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-2.3571296   0.46735144]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35305107 0.6362137  0.         0.01073516] \n",
      "\tDiff: tensor([-0.0197, -0.4695,  0.1667,  0.3226]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0197]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.8759, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05601551 0.94398445]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.524997  3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-2.3571296   0.46735144]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35305107 0.6362137  0.         0.01073516] \n",
      "\tDiff: tensor([-0.0197, -0.4695,  0.1667,  0.3226]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0394]])\n",
      "\n",
      ">>> Iter: 0086 (0.024s): loss = 00.244632, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.8816, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.0557011  0.9442989 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.518975  3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-2.3687804   0.46166253]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3529346  0.63632673 0.         0.01073874] \n",
      "\tDiff: tensor([-0.0196, -0.4697,  0.1667,  0.3226]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0196]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.8816, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.0557011  0.9442989 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.518975  3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-2.3687804   0.46166253]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3529346  0.63632673 0.         0.01073874] \n",
      "\tDiff: tensor([-0.0196, -0.4697,  0.1667,  0.3226]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0392]])\n",
      "\n",
      ">>> Iter: 0087 (0.023s): loss = 00.244631, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.8871, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05539742 0.9446026 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.51312816 3.3779728  3.3779728  0.        ], \n",
      "\tQ: [[-2.380094    0.45613718]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3528221  0.63643575 0.         0.01074219] \n",
      "\tDiff: tensor([-0.0195, -0.4698,  0.1667,  0.3226]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0195]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.8871, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05539742 0.9446026 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.51312816 3.3779728  3.3779728  0.        ], \n",
      "\tQ: [[-2.380094    0.45613718]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3528221  0.63643575 0.         0.01074219] \n",
      "\tDiff: tensor([-0.0195, -0.4698,  0.1667,  0.3226]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0390]])\n",
      "\n",
      ">>> Iter: 0088 (0.022s): loss = 00.244629, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.8925, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05510393 0.9448961 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.50744855 3.3779728  3.3779728  0.        ], \n",
      "\tQ: [[-2.3910856   0.45076823]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35271347 0.63654107 0.         0.01074553] \n",
      "\tDiff: tensor([-0.0194, -0.4699,  0.1667,  0.3226]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0194]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.8925, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05510393 0.9448961 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.50744855 3.3779728  3.3779728  0.        ], \n",
      "\tQ: [[-2.3910856   0.45076823]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35271347 0.63654107 0.         0.01074553] \n",
      "\tDiff: tensor([-0.0194, -0.4699,  0.1667,  0.3226]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0388]])\n",
      "\n",
      ">>> Iter: 0089 (0.023s): loss = 00.244627, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.8977, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05482009 0.94517994]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.50192875 3.3779728  3.3779728  0.        ], \n",
      "\tQ: [[-2.4017696   0.44554877]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35260844 0.6366429  0.         0.01074876] \n",
      "\tDiff: tensor([-0.0193, -0.4700,  0.1667,  0.3226]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0193]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.8977, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05482009 0.94517994]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.50192875 3.3779728  3.3779728  0.        ], \n",
      "\tQ: [[-2.4017696   0.44554877]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35260844 0.6366429  0.         0.01074876] \n",
      "\tDiff: tensor([-0.0193, -0.4700,  0.1667,  0.3226]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0386]])\n",
      "\n",
      ">>> Iter: 0090 (0.023s): loss = 00.244626, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.9028, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05454545 0.9454546 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.496562  3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-2.412159   0.4404726]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35250688 0.6367413  0.         0.01075188] \n",
      "\tDiff: tensor([-0.0192, -0.4701,  0.1667,  0.3226]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0192]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.9028, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05454545 0.9454546 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.496562  3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-2.412159   0.4404726]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35250688 0.6367413  0.         0.01075188] \n",
      "\tDiff: tensor([-0.0192, -0.4701,  0.1667,  0.3226]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0383]])\n",
      "\n",
      ">>> Iter: 0091 (0.026s): loss = 00.244624, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.9077, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05427956 0.94572043]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.49134207 3.3779728  3.3779728  0.        ], \n",
      "\tQ: [[-2.4222655   0.43553376]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35240862 0.63683647 0.         0.0107549 ] \n",
      "\tDiff: tensor([-0.0191, -0.4702,  0.1667,  0.3226]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0191]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.9077, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05427956 0.94572043]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.49134207 3.3779728  3.3779728  0.        ], \n",
      "\tQ: [[-2.4222655   0.43553376]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35240862 0.63683647 0.         0.0107549 ] \n",
      "\tDiff: tensor([-0.0191, -0.4702,  0.1667,  0.3226]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0382]])\n",
      "\n",
      ">>> Iter: 0092 (0.025s): loss = 00.244623, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.9125, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05402201 0.94597805]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.4862627 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-2.432101    0.43072677]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35231343 0.63692874 0.         0.01075783] \n",
      "\tDiff: tensor([-0.0190, -0.4703,  0.1667,  0.3226]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0190]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.9125, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05402201 0.94597805]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.4862627 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-2.432101    0.43072677]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35231343 0.63692874 0.         0.01075783] \n",
      "\tDiff: tensor([-0.0190, -0.4703,  0.1667,  0.3226]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0380]])\n",
      "\n",
      ">>> Iter: 0093 (0.025s): loss = 00.244621, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.9172, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05377239 0.9462276 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.4813183 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-2.4416769   0.42604613]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35222128 0.637018   0.         0.01076067] \n",
      "\tDiff: tensor([-0.0189, -0.4704,  0.1667,  0.3226]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0189]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.9172, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05377239 0.9462276 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.4813183 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-2.4416769   0.42604613]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35222128 0.637018   0.         0.01076067] \n",
      "\tDiff: tensor([-0.0189, -0.4704,  0.1667,  0.3226]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0378]])\n",
      "\n",
      ">>> Iter: 0094 (0.023s): loss = 00.244620, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.9218, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05353037 0.9464696 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.47650352 3.3779728  3.3779728  0.        ], \n",
      "\tQ: [[-2.4510026  0.4214871]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35213196 0.6371046  0.         0.01076342] \n",
      "\tDiff: tensor([-0.0188, -0.4704,  0.1667,  0.3226]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0188]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.9218, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05353037 0.9464696 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.47650352 3.3779728  3.3779728  0.        ], \n",
      "\tQ: [[-2.4510026  0.4214871]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35213196 0.6371046  0.         0.01076342] \n",
      "\tDiff: tensor([-0.0188, -0.4704,  0.1667,  0.3226]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0376]])\n",
      "\n",
      ">>> Iter: 0095 (0.023s): loss = 00.244618, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.9262, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05329563 0.9467044 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.47181353 3.3779728  3.3779728  0.        ], \n",
      "\tQ: [[-2.4600875   0.41704512]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35204533 0.6371886  0.         0.01076609] \n",
      "\tDiff: tensor([-0.0187, -0.4705,  0.1667,  0.3226]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0187]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.9262, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05329563 0.9467044 ]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.47181353 3.3779728  3.3779728  0.        ], \n",
      "\tQ: [[-2.4600875   0.41704512]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35204533 0.6371886  0.         0.01076609] \n",
      "\tDiff: tensor([-0.0187, -0.4705,  0.1667,  0.3226]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0374]])\n",
      "\n",
      ">>> Iter: 0096 (0.022s): loss = 00.244617, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.9305, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05306781 0.94693214]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.4672435 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-2.4689412   0.41271567]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35196134 0.63727    0.         0.01076868] \n",
      "\tDiff: tensor([-0.0186, -0.4706,  0.1667,  0.3226]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0186]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.9305, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05306781 0.94693214]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.4672435 3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-2.4689412   0.41271567]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 3.3432608   0.        ]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35196134 0.63727    0.         0.01076868] \n",
      "\tDiff: tensor([-0.0186, -0.4706,  0.1667,  0.3226]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0373]])\n",
      "\n",
      ">>> Iter: 0097 (0.022s): loss = 00.244616, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.9348, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05284663 0.94715333]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.462789  3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-2.4775722  0.4084947]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3518798  0.63734895 0.         0.0107712 ] \n",
      "\tDiff: tensor([-0.0185, -0.4707,  0.1667,  0.3226]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0185]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.9348, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 92.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05284663 0.94715333]\n",
      " [0.9658835  0.03411654]\n",
      " [0.9658835  0.03411654]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.462789  3.3779728 3.3779728 0.       ], \n",
      "\tQ: [[-2.4775722  0.4084947]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 3.3432608  0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.3518798  0.63734895 0.         0.0107712 ] \n",
      "\tDiff: tensor([-0.0185, -0.4707,  0.1667,  0.3226]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000,  0.0000, -0.0371]])\n",
      "\n",
      ">>> Iter: 0098 (0.024s): loss = 00.244614, likelihood = 0.0010\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.9389, grad_fn=<SelectBackward>), tensor(-0.0007, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 91.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05263947 0.9473606 ]\n",
      " [0.9653781  0.03462182]\n",
      " [0.96585125 0.03414869]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.443229  3.3626006 3.377031  0.       ], \n",
      "\tQ: [[-2.5010600e+00  3.8915351e-01]\n",
      " [ 3.3273652e+00 -6.7058532e-04]\n",
      " [ 3.3422856e+00  0.0000000e+00]\n",
      " [ 0.0000000e+00  0.0000000e+00]]\n",
      "\n",
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35180345 0.6372635  0.         0.01093312] \n",
      "\tDiff: tensor([-0.0185, -0.4706,  0.1667,  0.3224]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000, -0.4706, -0.0185]])\n",
      "--------------------------------------------------------------------------------\n",
      "Backward Pass I/P: \n",
      "\tS: [s: (1), s: (2), s: (3), s: (4)], \n",
      "\tA: ['stay', 'move'], \n",
      "\tR: [tensor(-2.9389, grad_fn=<SelectBackward>), tensor(-0.0007, grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>), tensor(0., grad_fn=<SelectBackward>)], \n",
      "\tT: <function T at 0x130f3a8c8>\n",
      "\n",
      "Running Backward Pass  [ ... ] Backward pass converged @ 91.\n",
      "Backward Pass Results: \n",
      "\tPolicy: [[0.05263947 0.9473606 ]\n",
      " [0.9653781  0.03462182]\n",
      " [0.96585125 0.03414869]\n",
      " [0.5        0.5       ]], \n",
      "\tV: [0.443229  3.3626006 3.377031  0.       ], \n",
      "\tQ: [[-2.5010600e+00  3.8915351e-01]\n",
      " [ 3.3273652e+00 -6.7058532e-04]\n",
      " [ 3.3422856e+00  0.0000000e+00]\n",
      " [ 0.0000000e+00  0.0000000e+00]]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: \n",
      "\tExpert SVF: [0.33333333 0.16666667 0.16666667 0.33333333], \n",
      "\tLearner SVF: [0.35180345 0.6372635  0.         0.01093312] \n",
      "\tDiff: tensor([-0.0185, -0.4706,  0.1667,  0.3224]) \n",
      "\n",
      "Grads: \n",
      "\tParam: 0, grad: tensor([[ 0.0000,  0.0000, -0.9412, -0.0369]])\n",
      "\n",
      ">>> Iter: 0099 (0.024s): loss = 00.244533, likelihood = 0.0011\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "maxent_loss_history, learned_policies, _, _ = MaxEntIRL(trajectory_list, lambda _: S, lambda _: T, A, phi, R_model, R_optimizer, gamma, \n",
    "                                                  verbose=True, debug=True, n_iters=100, boltzmann_temp=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,0,'Iterations')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(maxent_loss_history)\n",
    "plt.title(\"MaxEnt\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Iterations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "irl_python3",
   "language": "python",
   "name": "irl_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
